{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5bc7a5",
   "metadata": {},
   "source": [
    "## <a name=\"index\"></a> Indice\n",
    "\n",
    "[¿Que es un Data Warehouse?](#mark_01)\n",
    "\n",
    "[Bases de datos columnares y arquitectura orientada a optimización de consultas](#mark_02)\n",
    "\n",
    "[¿Cómo funciona AWS Redshift?](#mark_03)\n",
    "\n",
    "[Creando nuestro entorno de trabajo en AWS](#mark_04)\n",
    "\n",
    "[Cómo conectarnos a Redshift](#mark_05)\n",
    "\n",
    "[Creación de tablas en Redshift](#mark_06)\n",
    "\n",
    "[Carga de datos en S3](#mark_07)\n",
    "\n",
    "[Ingesta S3 --> Redshift](#mark_08)\n",
    "\n",
    "[Algoritmos de compresión con Redshift 1° Parte.](#mark_09)\n",
    "\n",
    "   - [RAW - Sin compresión](#mark_09.0)\n",
    "   \n",
    "   - [AZ64](#mark_09.1)\n",
    "   \n",
    "   - [Codificación por diccionarios de bytes](#mark_09.2)\n",
    "   \n",
    "   - [Codificacion Delta](#mark_09.3)\n",
    "   \n",
    "   - [LZO](#mark_09.4)\n",
    "\n",
    "[Algoritmos de compresión con Redshift 2° Parte.](#mark_10)\n",
    "\n",
    "   - [Codificación Mostly](#mark_10.0)\n",
    "   \n",
    "   - [Codificación de run length](#mark_10.1)\n",
    "   \n",
    "   - [Codificaciones Text255 y Text32k](#mark_10.2)\n",
    "   \n",
    "   - [Codificación ZSTD](#mark_10.3)\n",
    "   \n",
    "\n",
    "[Codificando/Compresión de Columnas](#mark_11)\n",
    "\n",
    "[Análisis de desempeño con diferentes tipos de compresión](#mark_12)\n",
    "\n",
    "   - [STV_TBL_PERM --> STV = System Table View](#mark_12.0)\n",
    "\n",
    "   - [Visualización de los estilos de distribución (EVEN, KEY, ALL)](#mark_12.1)\n",
    "\n",
    "   - [Utilizamos \"STV_TBL_PERM\" pero para una tabla en especial \"encoding_venue\"](#mark_12.2)\n",
    "\n",
    "   - [Continuamos con la sentencia \"STV_BLOCKLIST\" --> cantidad de bloques de 1 MB](#mark_12.3)\n",
    "\n",
    "   - [Comparando codificación Raw vs. ZSTD](#mark_12.4)\n",
    "\n",
    "   - [ANALYZE COMPRESSION, recomienda que tipo de compresión usar.](#mark_12.5)\n",
    "   \n",
    "<a name=\"index_01\"></a>\n",
    "\n",
    "[Estilos de distribución con Redshift](#mark_13)\n",
    "\n",
    "   - [Distribución AUTO](#mark_13.0)\n",
    "\n",
    "   - [Distribución EVEN](#mark_13.1)\n",
    "\n",
    "   - [Distribución KEY](#mark_13.2)\n",
    "\n",
    "   - [Distribución ALL](#mark_13.3)\n",
    "\n",
    "[Evaluando los estilos de distribución](#mark_14)\n",
    "\n",
    "   - [SVV_TABLE_INFO, información resumen de tablas en base de datos](#mark_14.0)\n",
    "   \n",
    "   - [SVV_TABLE](#mark_14.0.0)\n",
    "\n",
    "   - [PG_TABLE_DEF, Almacena información acerca de las columnas de la tabla](#mark_14.1)\n",
    "\n",
    "   - [SVV_DISKUSAGE, combinación de las tablas STV_TBL_PERM y STV_BLOCKLIST](#mark_14.2)\n",
    "\n",
    "[sortkey, Llaves de ordenamiento para optimizar nuestras consultas](#mark_15)\n",
    "\n",
    "   - [Clave de ordenación compuesta (COMPOUND)](#mark_15.0)\n",
    "\n",
    "   - [Clave de ordenación intercalada (INTERLEAVED)](#mark_15.1)\n",
    "\n",
    "[Evaluando algoritmos de ordenamiento](#mark_16)\n",
    "\n",
    "   - [Primer prueba](#test_01)\n",
    "\n",
    "   - [Segunda prueba](#test_02)\n",
    "\n",
    "   - [Tercer prueba](#test_03)\n",
    "\n",
    "   - [Conclusiones](#conclusion_01)\n",
    "\n",
    "[Buenas prácticas para diseñar tablas en Redshift](#mark_17)\n",
    "\n",
    "[Tipos de datos en AWS Redshift](#mark_18)\n",
    "\n",
    "[Reto: mejora el desempeño de tu base de datos](#mark_19)\n",
    "\n",
    "<a name=\"index_02\"></a>\n",
    "\n",
    "[Olvídate de los insert, el copy llego para quedarse](#mark_20)\n",
    "\n",
    "[Cargando archivos tipo JSON](#mark_21)\n",
    "\n",
    "[El comando copy a fondo](#mark_22)\n",
    "\n",
    "   - [STL_LOAD_ERRORS](#mark_22.0)\n",
    "   - [STL_LOAD_COMMITS](#mark_22.1)\n",
    "\n",
    "[Manifiestos y uso de COMPUPDATE para carga con compresión automática](#mark_23)\n",
    "\n",
    "   - [Carga con archivo de manifiesto](#mark_23.0)\n",
    "\n",
    "   - [Comando de compresión COMPUPDATE](#mark_23.1)\n",
    "\n",
    "[Métodos de carga alternativos al comando copy](#mark_24)\n",
    "\n",
    "   - [INSERT por lotes](#mark_24.0)\n",
    "\n",
    "   - [bulk insert, datos por lotes](#mark_24.1)\n",
    "\n",
    "   - [deep copy](#mark_24.2)\n",
    "\n",
    "[¿Cómo ejecutar sentencias UPDATE y DELETE?](#mark_25)\n",
    "\n",
    "[¿Cómo mantener el desempeño de tu base de datos?](#mark_26)\n",
    "\n",
    "   - [Analyze](#mark_26.0)\n",
    "\n",
    "   - [Vacuum (Limpieza)](#mark_26.1)\n",
    "\n",
    "[Estadísticas y limpieza de las tablas](#mark_27)\n",
    "\n",
    "   - [PG_STATISTIC_INDICATOR](#mark_27.0)\n",
    "\n",
    "   - [STL_ANALYZE](#mark_27.1)\n",
    "\n",
    "   - [Vacuum](#mark_27.2)\n",
    "   \n",
    "<a name=\"index_03\"></a>\n",
    "\n",
    "[Agrupamiento, ordenamiento y subqueries](#mark_28)\n",
    "\n",
    "[¿Qué es y cómo interpretar un explain plan?](#mark_29)\n",
    "\n",
    "   - [Alerta sobre el performance de las queries con stl_alert_event_log](#mark_29.0)\n",
    "   \n",
    "   - [STL_QUERY](#mark_29.1)\n",
    "   \n",
    "   - [SVL_QLOG](#mark_29.2)\n",
    "\n",
    "[¿Cómo descargar datos eficientemente con UNLOAD?](#mark_30)\n",
    "\n",
    "[Otras tablas útiles de Redshift para entender el comportamiento de nuestros datos](#mark_31)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)\n",
    "\n",
    "[](#mark_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55f681",
   "metadata": {},
   "source": [
    "### <a name=\"mark_01\"></a>¿Que es un Data Warehouse?\n",
    "    \n",
    "[Index](#index)\n",
    "\n",
    "![](img_01.png)\n",
    "\n",
    "- Es una \" gran base de datos\" que recibe información de muchas fuentes de datos, su objetivo está definido por una estructura analítica, no mantiene operaciones constantes de por ejemplo I/O sino que está orientado a la analítica de datos.\n",
    "    \n",
    "Tomamos los objetos de diferentes fuentes y en diferentes formatos (csv, json, xml, etc) y los llevamos a un solo repositorio \"Redshift\"\n",
    "    \n",
    "![](img_02.png)\n",
    "\n",
    "Se usa el proceso de ETL (extract, transform, load) para alimentar el Data Warehouse.\n",
    "    \n",
    "![](img_03.png)\n",
    "\n",
    "Extraer: Obtener los datos de las distintas bases de datos\n",
    "Transformar: Realizar una limpieza y modificación de los datos, creando una buena estructura analítica.\n",
    "Cargar: Luego de transformar los datos, se cargar al DW\n",
    "    \n",
    "### ¿Que es una estructura analítica? \n",
    "    \n",
    "Depende de la organización y sus objetivos, pero el estandar es el modelo dimensional, que posee:\n",
    "    \n",
    "![](img_04.png)\n",
    "    \n",
    "Modelo estrella, tengo el QUE? en el centro y los CÓMO? alrededor.\n",
    "    \n",
    "![](img_05.png)\n",
    "\n",
    "- Tabla de hechos: Que quiero medir\n",
    "    \n",
    "- Tabla de dimensiones: Como medirlo, es decir, que variables son importante para generar los análisis\n",
    "    \n",
    "En el curso trabajaremos con el siguiente modelo de AWS Redshift.\n",
    "    \n",
    "![](img_06.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecda51",
   "metadata": {},
   "source": [
    "### <a name=\"mark_02\"></a>Bases de datos columnares y arquitectura orientada a optimización de consultas\n",
    "    \n",
    "[Index](#index)\n",
    "\n",
    "![](img_07.png)\n",
    "    \n",
    "### ¿Qué es una base de datos columnar?\n",
    "    \n",
    "Redshift es una de ellas. Es una BD optimizada para lograr una recuperación rápida de columnas de datos, normalmente en aplicaciones y con un fin analítico, esto permite procesar queries complejos con cantidades enormes de datos de una manera óptima\n",
    "\n",
    "### Base de datos basadas en filas (Postgres, MySQL, Oracle)\n",
    "    \n",
    "Tienen una arquitectura y un proposito distinto a las bases de datos columnares, están enfocadas en la transaccionalidad y en la lectura y escritura rápida de filas únicas en una db.\n",
    "\n",
    "![](img_08.png)\n",
    "\n",
    "Hay una diferencia a nivel de arquitectura en ambas, \"EL BLOQUE DE DATOS\".\n",
    "    \n",
    "![](img_09.png)\n",
    "    \n",
    "Cada una de las filas va un bloque de datos guardado en el disco duro, soportando operaciones del tipo transaccional OLTP.\n",
    "    \n",
    "![](img_10.png)\n",
    "    \n",
    "### Qué pasa si quiero hacer analítica con estas DB:\n",
    "    \n",
    "Los bloques de datos de estas db usualmente pesan por defecto 32kb, con lo cual si nuestra data pesa menos, de igual forma el bloque pesará 32kb, en el caso de que nuestra data pese más, ocupará otro bloque de 32kb.\n",
    "    \n",
    "Otro punto, si quiero hacer analítica de una tabla de 10 columnas pero solo quiero la información de 2 columnas, de forma inevitable voy a tener 8 columnas que no voy a utilizar pero que sí tengo que consultar por bloque de datos.\n",
    "    \n",
    "    \n",
    "### Base de datos columnar:\n",
    "    \n",
    "![](img_11.png)\n",
    "    \n",
    "Los registros se guardan columna a columna en un Bloque de Datos, como vemos en el ejemplo anterior la columna SSN serial Number completa se encuentra en un bloque de datos.\n",
    "    \n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)\n",
    "\n",
    "![](img_14.png)\n",
    "    \n",
    "### Nota:\n",
    "    \n",
    "Hay algunas bd columnares que son accesibles a travéz de un solo cliente, con lo cual debes aprender a manejar ese cliente en especifico, en Redshift no sucede eso, si manejas DBeaver que es una aplicación de software cliente de SQL y una herramienta de administración de bases de datos, o SQL Workbench, simplemente puedes conectar por medio de HOST, USER, y PASSWORD a la db en Redshift.\n",
    "\n",
    "### Lecturas recomendadas\n",
    "    \n",
    "https://aws.amazon.com/es/nosql/columnar/\n",
    "    \n",
    "https://aws.amazon.com/es/redshift/features/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575b8ab",
   "metadata": {},
   "source": [
    "### <a name=\"mark_03\"></a>¿Cómo funciona AWS Redshift?\n",
    "    \n",
    "### [Index](#index)\n",
    "\n",
    "![](img_15.png)\n",
    "\n",
    "### La clave de Redshift --> Repartir datos:\n",
    "\n",
    "- La clave de Redshift es \"repartir los datos\", no voy a instalar Redshift en un Servidor, sino en un cluster siendo este cluster un arreglo de varios servidores conectados y a estos servidores lo voy a llamar Nodos (de datos), cuando me conecte a la db a travéz de los drivers JDBC o ODBC me conectaré a un nodo lider quien orquestará los procesos (repartirá el trabajo en los otros nodos y lo hará en paralelo), luego que los otros nodos hayan computado las tareas el organiza la data y la devuelve al cliente.\n",
    "\n",
    "### Los Nodos o Servidores:\n",
    "- Cada nodo es un servidor teniendo su propia memoría RAM, su propio espacio en disco, su propio procesador y está diseñado para procesar muchos datos. Cada nodo tiene unos \"Servidores Virtuales, Segmentos, o Slices\", entonces cada slice tendrá repartida la data que se le asignó al nodo, un nodo puede tener 2, 4, u 8 slices esto es para que también sea trabajada en paralelo. Hadoop también funciona por cluster y con trabajo en paralelo sobre un File System que para el caso de Hadoop es el HDFS.\n",
    "\n",
    "### Postgres y Redshift, limitaciones:\n",
    "\n",
    "- La arquitectura de Postgres 8.02 funciona de base para la creación de Redshift, pero hay algunas cosas que puedo hacer en Postgres que no puedo hacer en Redshift.\n",
    "\n",
    "Por ejemplo en Redshif no puedo usar:\n",
    "\n",
    "    - Triggers.\n",
    "    - Espacios almacenados.\n",
    "    - Table Spaces.\n",
    "    - Indices\n",
    "\n",
    "Pero si analizamos su necesidad para el caso de Redshift no sería muy útiles debido a que estamos pensando en Analítica y esas características son necesarias para cuestiones Transaccionales.\n",
    "\n",
    "En la siguiete imagen veremos que para una tabla en especifico (tbl que es el id de la tabla), de una columna especifica (col 9) voy a tener la data dividida en 2 slices (0 y 1, estoy trabajando 1 solo nodo), mi bloque de datos en el slice 0 tiene para esa tabla/esa columna tiene 130.994 registros, al igual que el slice 1, podemos ver como se reparte la data en el bloque 0 (blocknum), también podemos ver cual es el valor mín y máx en el slice.\n",
    "\n",
    "En la imagen de Data Distribuida, podemos ver que para las distintas tablas los slice no manejan exactamente la misma cantidad de filas, lo ideal es repartir la data lo más equitativamente posible para lograr un mejor trabajo en paralelo.\n",
    "\n",
    "![](img_16.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d2b3c",
   "metadata": {},
   "source": [
    "### <a name=\"mark_04\"></a> Creando nuestro entorno de trabajo en AWS\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "### Creando el Role para Redshift:\n",
    "\n",
    "Como buena práctica iniciamos creando/limitando un rol para el uso de Redshift.\n",
    "\n",
    "En la consola de AWS --> IAM\n",
    "\n",
    "![](img_17.png)\n",
    "\n",
    "En las opciones por default buscamos \"Redshift\"\n",
    "\n",
    "![](img_18.png)\n",
    "\n",
    "Para estos permisos necesito que Redshift se pueda comunicar con S3.\n",
    "\n",
    "![](img_19.png)\n",
    "\n",
    "Colocamos etiquetas si las necesitamos.\n",
    "\n",
    "![](img_20.png)\n",
    "\n",
    "![](img_21.png)\n",
    "\n",
    "\n",
    "### Creando el bucket S3:\n",
    "\n",
    "Consola de AWS --> S3 --> Crear bucket.\n",
    "\n",
    "![](img_22.png)\n",
    "\n",
    "### Configura el cluster de Redshift:\n",
    "\n",
    "Consola de AWS --> Amazon Redshift\n",
    "\n",
    "![](img_23.png)\n",
    "\n",
    "![](img_24.png)\n",
    "\n",
    "![](img_25.png)\n",
    "\n",
    "![](img_26.png)\n",
    "\n",
    "Nombre de la db y un puerto, ambos opcionales.\n",
    "\n",
    "Master user name --> un nombre de conexión a mi instancia de Redshift y un password\n",
    "![](img_27.png)\n",
    "\n",
    "Agregamos el rol creado previamente y luego click en Crear Cluster.\n",
    "\n",
    "![](img_28.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61100b2d",
   "metadata": {},
   "source": [
    "### <a name=\"mark_05\"></a> Cómo conectarnos a Redshift\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "Primero necesitamos realizar algunas configuraciones para permiter la conexión.\n",
    "\n",
    "Ingresamos a nuestro cluster.\n",
    "\n",
    "![](img_29.png)\n",
    "\n",
    "![](img_30.png)\n",
    "\n",
    "En la parte final vemos si tenemos acceso público, luego ingresamos al \"VPC Security Group\".\n",
    "\n",
    "![](img_31.png)\n",
    "\n",
    "![](img_32.png)\n",
    "\n",
    "![](img_33.png)\n",
    "\n",
    "Agregamos la nueva regla y guardamos los cambios.\n",
    "\n",
    "![](img_34.png)\n",
    "\n",
    "Nuevamente en la consola de Redshift, buscamos \"View all connection details\", donde podremos encontrar los drivers JDBC, ODBC, y la posibilidad de crear un tunel SSH. Elegimos ODBC para este ejemplo.\n",
    "\n",
    "![](img_35.png)\n",
    "\n",
    "Para este ejemplo utilizaremos el Cliente DBeaver, es open source y gratuito.\n",
    "\n",
    "![](img_36.png)\n",
    "\n",
    "![](img_37.png)\n",
    "\n",
    "Recordar que para el host --> utilizamos \"Server\", User --> UID, PWD --> Password\n",
    "\n",
    "![](img_38.png)\n",
    "\n",
    "![](img_39.png)\n",
    "\n",
    "![](img_40.png)\n",
    "\n",
    "Una vez que testeamos la conexión damos click en Finalizar.\n",
    "\n",
    "Abrimos un editor para nuestra nueva conexión.\n",
    "\n",
    "![](img_41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2893014",
   "metadata": {},
   "source": [
    "### <a name=\"mark_06\"></a>Creación de tablas en Redshift\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "En el repo de GitHub tenemos \"create_tables_tickit.sql\" con el detalle que se puede ver a continuación. En esta creación de tablas existen nuevos \"Constrains\" para bd columnares `distkey sortkey`.\n",
    "\n",
    "    - `distkey` es una llave de distribución, distribuye los datos en el custer, en los nodos a travez de los datos que se contienen en la columna o atributo.\n",
    "    \n",
    "    -`sortkey` es la llave de ordenamiento facilita la consulta a travéz de los datos que se contienen en la columna o atributo.\n",
    "\n",
    "Procedemos a crear nuestras tablas en Redshift.\n",
    "\n",
    "```sql\n",
    "create table users(\n",
    "\tuserid integer not null distkey sortkey,\n",
    "\tusername char(8),\n",
    "\tfirstname varchar(30),\n",
    "\tlastname varchar(30),\n",
    "\tcity varchar(30),\n",
    "\tstate char(2),\n",
    "\temail varchar(100),\n",
    "\tphone char(14),\n",
    "\tlikesports boolean,\n",
    "\tliketheatre boolean,\n",
    "\tlikeconcerts boolean,\n",
    "\tlikejazz boolean,\n",
    "\tlikeclassical boolean,\n",
    "\tlikeopera boolean,\n",
    "\tlikerock boolean,\n",
    "\tlikevegas boolean,\n",
    "\tlikebroadway boolean,\n",
    "\tlikemusicals boolean);\n",
    "\n",
    "create table venue(\n",
    "\tvenueid smallint not null distkey sortkey,\n",
    "\tvenuename varchar(100),\n",
    "\tvenuecity varchar(30),\n",
    "\tvenuestate char(2),\n",
    "\tvenueseats integer);\n",
    "\n",
    "create table category(\n",
    "\tcatid smallint not null distkey sortkey,\n",
    "\tcatgroup varchar(10),\n",
    "\tcatname varchar(10),\n",
    "\tcatdesc varchar(50));\n",
    "\n",
    "create table date(\n",
    "\tdateid smallint not null distkey sortkey,\n",
    "\tcaldate date not null,\n",
    "\tday character(3) not null,\n",
    "\tweek smallint not null,\n",
    "\tmonth character(5) not null,\n",
    "\tqtr character(5) not null,\n",
    "\tyear smallint not null,\n",
    "\tholiday boolean default('N'));\n",
    "\n",
    "create table event(\n",
    "\teventid integer not null distkey,\n",
    "\tvenueid smallint not null,\n",
    "\tcatid smallint not null,\n",
    "\tdateid smallint not null sortkey,\n",
    "\teventname varchar(200),\n",
    "\tstarttime timestamp);\n",
    "\n",
    "create table listing(\n",
    "\tlistid integer not null distkey,\n",
    "\tsellerid integer not null,\n",
    "\teventid integer not null,\n",
    "\tdateid smallint not null  sortkey,\n",
    "\tnumtickets smallint not null,\n",
    "\tpriceperticket decimal(8,2),\n",
    "\ttotalprice decimal(8,2),\n",
    "\tlisttime timestamp);\n",
    "\n",
    "create table sales(\n",
    "\tsalesid integer not null,\n",
    "\tlistid integer not null distkey,\n",
    "\tsellerid integer not null,\n",
    "\tbuyerid integer not null,\n",
    "\teventid integer not null,\n",
    "\tdateid smallint not null sortkey,\n",
    "\tqtysold smallint not null,\n",
    "\tpricepaid decimal(8,2),\n",
    "\tcommission decimal(8,2),\n",
    "\tsaletime timestamp);\n",
    "```\n",
    "\n",
    "Las tablas están vacías, necesitamos realizar la ingesta en Redshift.\n",
    "\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_sampledb.html\n",
    "\n",
    "https://github.com/alarcon7a/redshift_course/tree/master/Tickit_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99df238",
   "metadata": {},
   "source": [
    "### <a name=\"mark_07\"></a>Carga de datos en S3\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "Para esto tendremos que cargar los archivos planos tickitdb.zip (descomprimirlos) en un bucket S3.\n",
    "\n",
    "![](img_42.png)\n",
    "\n",
    "Dentro de nuestro S3 cargamos o arrastramos y soltamos (drag and drop), una vez realizado ya tenemos los archivos listos para la ingesta.\n",
    "\n",
    "![](img_43.png)\n",
    "\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_sampledb.html\n",
    "\n",
    "https://github.com/alarcon7a/redshift_course/tree/master/Tickit_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc42da",
   "metadata": {},
   "source": [
    "### <a name=\"mark_08\"></a> Ingesta S3 --> Redshift\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "Desde el archivo de github ejecutamos \"copy_s3_to_redshift_tickit.sql\" en nuestro cliente (que ya está conectado a Redshift), para realizar la ingesta.\n",
    "\n",
    "### Importante:\n",
    "\n",
    "`copy`, lleva data desde un repositorio a las tablas.\n",
    "\n",
    "Recordar reemplazar `<Reemplazar_con_la_ruta_del_bucket>` con la ruta de tu bucket.\n",
    "\n",
    "Recordar reemplazar `<Reemplazar_con_tu_iam_role_arn` con el ARN del rol (buscar en IAM el rol creado anteriormente), Redshift tiene que tener el mismo rol.\n",
    "\n",
    "Recordar reemplazar `<Reemplazar_con_tu_aws_region>` con tu region en AWS.\n",
    "\n",
    "Archivos **\"copy_s3_to_redshift_tickit.sql\"** :\n",
    "\n",
    "```sql\n",
    "copy users from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/allusers_pipe.txt' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' \n",
    "delimiter '|' region '<Reemplazar_con_tu_aws_region>';\n",
    "\n",
    "copy venue from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/venue_pipe.txt' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' \n",
    "delimiter '|' region '<Reemplazar_con_tu_aws_region>';\n",
    "\n",
    "copy category from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/category_pipe.txt' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' \n",
    "delimiter '|' region '<Reemplazar_con_tu_aws_region>';\n",
    "\n",
    "copy date from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/date2008_pipe.txt' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' \n",
    "delimiter '|' region '<Reemplazar_con_tu_aws_region>';\n",
    "\n",
    "copy event from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/allevents_pipe.txt' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' \n",
    "delimiter '|' timeformat 'YYYY-MM-DD HH:MI:SS' region '<Reemplazar_con_tu_aws_region>';\n",
    "\n",
    "copy listing from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/listings_pipe.txt' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>' \n",
    "delimiter '|' region '<Reemplazar_con_tu_aws_region>';\n",
    "\n",
    "copy sales from 's3://<Reemplazar_con_la_ruta_del_bucket>/tickit/sales_tab.txt'\n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>'\n",
    "delimiter '\\t' timeformat 'MM/DD/YYYY HH:MI:SS' region '<Reemplazar_con_tu_aws_region>';\n",
    "```\n",
    "Ejecutamos y las tablas comenzaran a poblar/populate.\n",
    "\n",
    "### Lecturas recomendadas¶\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_sampledb.html\n",
    "\n",
    "https://github.com/alarcon7a/redshift_course/tree/master/Tickit_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0e1cc",
   "metadata": {},
   "source": [
    "### <a name=\"mark_09\"></a> Algoritmos de compresión con Redshift 1° Parte.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_45.png)\n",
    "\n",
    "Supongamos que tenemos una tabla que utiliza 500 bloques de datos = 500Mb, si logro comprimirla a 100 bloques de datos, esto se traducirá en mayor velocidad/mejor performance al momento de realizar las lecturas en disco que son transacciones I/O.\n",
    "\n",
    "Entonces al momento de crear una tabla, no después, debo ejecutar las siguientes sentencias, que basicamente dice que la columna \"name\" tendrá un ENCODE del tipo TEXT255 (tipo de compresión).\n",
    "\n",
    "![](img_46.png)\n",
    "\n",
    "## Algoritmos y tipos de compresión:\n",
    "\n",
    "### <a name=\"mark_09.0\"></a>RAW - Sin compresión:\n",
    "[Index](#index)\n",
    "\n",
    "![](img_47.png)\n",
    "\n",
    "Raw --> Ninguna compresión, es la data cruda como viene.\n",
    "\n",
    "### <a name=\"mark_09.1\"></a>AZ64:\n",
    "[Index](#index)\n",
    "\n",
    "![](img_48.png)\n",
    "\n",
    "Como trabaja SIMD.\n",
    "\n",
    "![](img_49.png)\n",
    "\n",
    "### <a name=\"mark_09.2\"></a>Codificación por diccionarios de bytes\n",
    "[Index](#index)\n",
    "\n",
    "![](img_50.png)\n",
    "\n",
    "En la codificación por diccionarios de bytes, se crea un diccionario independiente de valores únicos para cada bloque de los valores de columna del disco. (Un bloque de disco de Amazon Redshift ocupa 1 MB). El diccionario tiene hasta 256 valores de un byte que se almacenan como índices de los valores de datos originales. Si se almacenan más de 256 valores en un mismo bloque, los valores adicionales se graban en un bloque descomprimido sin formato. El proceso se repite para cada bloque del disco.\n",
    "\n",
    "Esta codificación es muy eficaz en columnas de cadenas de cardinalidad baja. Esta codificación es una solución óptima cuando el dominio de los datos de una columna es menor que 256 valores únicos.\n",
    "\n",
    "En las columnas cuyo tipo de datos de cadena (CHAR y VARCHAR) está codificado con BYTEDICT, Amazon Redshift realiza exámenes vectorizados y evaluaciones de predicados que actúan directamente en los datos comprimidos. Estos exámenes utilizan instrucciones de instrucción única y datos múltiples (SIMD) específicas del hardware para el procesamiento paralelo. Esto acelera considerablemente el examen de las columnas de cadenas. La codificación por diccionario de bytes es muy eficaz en cuanto a espacio si una columna CHAR/VARCHAR almacena cadenas largas de caracteres.\n",
    "\n",
    "Supongamos que una tabla tiene una columna COUNTRY con un tipo de datos CHAR(30). A medida que se cargan datos, Amazon Redshift crea un diccionario y rellena la columna COUNTRY con el valor índice. El diccionario tiene los valores únicos indexados y la tabla en sí tiene solo los subscripts de un byte de los valores correspondientes.\n",
    "\n",
    "**nota**\n",
    "\n",
    "Los espacios a la derecha se almacenan en columnas de caracteres de longitud fija. Por lo tanto, en una columna CHAR (30), cada valor comprimido ahorra 29 bytes de almacenamiento cuando utiliza la codificación por diccionario de bytes.\n",
    "\n",
    "En la siguiente tabla, se representa el diccionario para la columna COUNTRY.\n",
    "\n",
    "![](img_53.png)\n",
    "\n",
    "En la siguiente tabla, se representan los valores de la columna COUNTRY.\n",
    "\n",
    "|Valor original del dato|\tTamaño original (longitud fija, 30 bytes por valor)\t|Valor comprimido (índice)\t|Tamaño nuevo (bytes)|\n",
    "|---|---|---|---|\n",
    "|England|30|0|1|\n",
    "|England|30|0|1|\n",
    "|United States of America|30|1|1|\n",
    "|United States of America|30|1|1|\n",
    "|Venezuela|30|2|1|\n",
    "|Sri Lanka|30|3|1|\n",
    "|Argentina|30|4|1|\n",
    "|Japan|30|5|1|\n",
    "|Sri Lanka|30|3|1|\n",
    "|Argentina|30|4|1|\n",
    "|Total|300||10|\n",
    "\n",
    "El tamaño total comprimido en este ejemplo se calcula de la siguiente manera: hay 6 entradas diferentes almacenadas en el diccionario (6 * 30 = 180) y la tabla tiene 10 valores comprimidos de 1 byte para un total de 190 bytes.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En el contexto de Redshift, la codificación por diccionarios de bytes es una técnica de compresión de datos que se utiliza para reducir el tamaño de las columnas de datos que contienen un número limitado de valores únicos.\n",
    "\n",
    "La codificación por diccionarios de bytes funciona creando un diccionario de los valores únicos que se encuentran en la columna. Luego, cada valor de la columna se reemplaza por su índice en el diccionario. Por ejemplo, si la columna `color` contiene los valores \"rojo\", \"verde\", \"azul\" y \"amarillo\", el diccionario se vería así:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"rojo\": 0,\n",
    "  \"verde\": 1,\n",
    "  \"azul\": 2,\n",
    "  \"amarillo\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "Luego, los valores de la columna se pueden almacenar como índices. Por ejemplo, el valor \"rojo\" se almacenaría como 0, el valor \"verde\" se almacenaría como 1, etc.\n",
    "\n",
    "La codificación por diccionarios de bytes puede proporcionar una reducción de tamaño significativa para columnas con un número limitado de valores únicos. Por ejemplo, si la columna `color` contiene 1000 registros, con un promedio de 2 valores únicos por registro, la codificación por diccionarios de bytes puede reducir el tamaño de la columna de 2000 bytes a 1000 bytes.\n",
    "\n",
    "Para utilizar la codificación por diccionarios de bytes en Redshift, debe habilitar la opción `data_compression` para la columna. Esto se puede hacer mediante la sentencia `ALTER TABLE`. Por ejemplo, para habilitar la codificación por diccionarios de bytes para la columna `color`, se puede utilizar el siguiente comando:\n",
    "\n",
    "```sql\n",
    "ALTER TABLE my_table ALTER COLUMN color SET data_compression = 'dict';\n",
    "```\n",
    "\n",
    "Una vez que la opción `data_compression` está habilitada, Redshift automáticamente comprimirá la columna utilizando la codificación por diccionarios de bytes.\n",
    "\n",
    "La codificación por diccionarios de bytes tiene algunas limitaciones. No se puede utilizar para columnas que contienen valores NULL o valores que cambian con frecuencia. Además, la codificación por diccionarios de bytes puede reducir la velocidad de las consultas que utilizan la columna.\n",
    "\n",
    "En general, la codificación por diccionarios de bytes es una técnica eficaz para reducir el tamaño de las columnas de datos que contienen un número limitado de valores únicos. Sin embargo, es importante tener en cuenta las limitaciones de la técnica antes de utilizarla.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### <a name=\"mark_09.3\"></a>Codificacion Delta:\n",
    "[Index](#index)\n",
    "\n",
    "![](img_51.png)\n",
    "\n",
    "Se basa en medir la distancia entre un dato y el siguiente.\n",
    "\n",
    "![](img_52.png)\n",
    "\n",
    "Si el delta supera número de dígitos posibles que puedo guardar en 1 byte, que es desde -127 hasta 127 no se podrá codificar, de igual forma para el de 2 bytes, será desde -32.000 hasta 32.000.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "La codificación delta es una técnica de compresión de datos que se utiliza para almacenar las diferencias entre dos versiones de un archivo. Esto permite almacenar solo los cambios que se han realizado en el archivo, en lugar de almacenar todo el archivo nuevamente.\n",
    "\n",
    "La codificación delta funciona comparando las dos versiones del archivo y generando un archivo que contiene las diferencias entre ellas. Estas diferencias se pueden almacenar de varias maneras, como:\n",
    "\n",
    "* **Diferencias de bytes:** Esta es la forma más simple de codificación delta. Simplemente se almacenan los bytes que han cambiado entre las dos versiones del archivo.\n",
    "* **Diferencias de registros:** Esta forma de codificación delta almacena las diferencias entre los registros de las dos versiones del archivo.\n",
    "* **Diferencias de estructuras de datos:** Esta forma de codificación delta almacena las diferencias entre las estructuras de datos de las dos versiones del archivo.\n",
    "\n",
    "La codificación delta puede proporcionar una reducción de tamaño significativa para archivos que cambian con frecuencia. Por ejemplo, si un archivo se modifica agregando un nuevo registro al final, la codificación delta puede reducir el tamaño del archivo en un 99%.\n",
    "\n",
    "La codificación delta se utiliza en una variedad de aplicaciones, como:\n",
    "\n",
    "* **Versionado de archivos:** La codificación delta se puede utilizar para almacenar las versiones anteriores de un archivo. Esto permite restaurar una versión anterior del archivo si es necesario.\n",
    "* **Copia de seguridad incremental:** La codificación delta se puede utilizar para realizar copias de seguridad incrementales de un archivo. Esto reduce el tiempo y el espacio de almacenamiento necesarios para realizar copias de seguridad.\n",
    "* **Carga de datos:** La codificación delta se puede utilizar para cargar datos de forma incremental en una base de datos. Esto reduce el tiempo necesario para cargar los datos.\n",
    "\n",
    "En el contexto de Amazon Redshift, la codificación delta se utiliza para almacenar las diferencias entre dos versiones de un conjunto de datos. Esto permite a Redshift cargar los datos incrementalmente, lo que reduce el tiempo necesario para cargar los datos.\n",
    "\n",
    "Para utilizar la codificación delta en Redshift, debe habilitar la opción `data_compression` para el conjunto de datos. Esto se puede hacer mediante la sentencia `ALTER TABLE`. Por ejemplo, para habilitar la codificación delta para el conjunto de datos `my_data`, se puede utilizar el siguiente comando:\n",
    "\n",
    "```sql\n",
    "ALTER TABLE my_data ALTER COLUMN data SET data_compression = 'delta';\n",
    "```\n",
    "\n",
    "Una vez que la opción `data_compression` está habilitada, Redshift automáticamente comprimirá el conjunto de datos utilizando la codificación delta.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### <a name=\"mark_09.4\"></a>Codificación LZO\n",
    "[Index](#index)\n",
    "\n",
    "![](img_54.png)\n",
    "\n",
    "Cadenas de texto libre --> descripciones.\n",
    "\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html\n",
    "\n",
    "https://aws.amazon.com/es/about-aws/whats-new/2019/10/amazon-redshift-introduces-az64-a-new-compression-encoding-for-optimized-storage-and-high-query-performance/\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_Byte_dictionary_encoding.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f5b13",
   "metadata": {},
   "source": [
    "### <a name=\"mark_10\"></a> Algoritmos de compresión con Redshift 2° Parte.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "\n",
    "### <a name=\"mark_10.0\"></a>Codificación Mostly.\n",
    "[Index](#index)\n",
    "\n",
    "Las codificaciones mostly son útiles cuando el tipo de datos de una columna es mayor que lo que requieren la mayoría de los valores almacenados. Al especificar una codificación mostly para este tipo de columnas, puede comprimir la mayoría de los valores de la columna a un tamaño de almacenamiento estándar más pequeño. Los valores restantes que no pueden comprimirse se almacenan sin formato. Por ejemplo, puede comprimir una columna de 16 bits, como una columna IT2, a un almacenamiento de 8 bits.\n",
    "\n",
    "Por lo general, las codificaciones mostly funcionan con los siguientes tipos de datos:\n",
    "\n",
    "SMALLINT/INT2 (16 bits)\n",
    "\n",
    "INTEGER/INT (32 bits)\n",
    "\n",
    "BIGINT/INT8 (64 bits)\n",
    "\n",
    "DECIMAL/NUMERIC (64 bits)\n",
    "\n",
    "Seleccione la variación adecuada de codificación mostly en función del tamaño de los tipos de datos de la columna. Por ejemplo, aplique la codificación MOSTLY8 a una columna definida para valores enteros de 16 bits. No está permitido aplicar MOSTLY16 a una columna con tipos de datos de 16 bits o MOSTLY32 a una columna con tipos de datos de 32 bits.\n",
    "\n",
    "La mayoría de las codificaciones pueden ser menos eficaces que la ausencia de compresión cuando no se puede comprimir un número relativamente elevado de los valores de la columna. Antes de aplicar una de estas codificaciones a una columna, realice la comprobación correspondiente. La mayoría de los valores que se vayan a cargar ahora (y que es probable que se carguen más adelante) deberían ajustarse a los rangos que se muestran en la siguiente tabla.\n",
    "\n",
    "![](img_55.png)\n",
    "\n",
    "#### nota\n",
    "\n",
    "`Para los valores decimales, omita la coma decimal para determinar si el valor se ajusta al rango. Por ejemplo, 1 234,56 es tratado como 123 456 y se puede comprimir en una columna MOSTLY32.`\n",
    "\n",
    "Por ejemplo, la columna VENUEID de la tabla VENUE está definida como una columna de valores enteros sin formato, lo que significa que su valor consume 4 bytes de almacenamiento. No obstante, el rango actual de valores de la columna es de 0 a 309. Por lo tanto, si se vuelve a crear y a cargar esta tabla con la codificación MOSTLY16 para VENUEID, se reduciría el almacenamiento de cada valor en esa columna a 2 bytes.\n",
    "\n",
    "Si la mayoría de los valores de VENUEID a los que se hace referencia en otra tabla estuvieran en el rango de 0 a 127, podría tener sentido codificar esa columna de clave externa como MOSTLY8. Antes de tomar una decisión, ejecute algunas consultas en los datos de la tabla de referencia para averiguar si la mayoría de los valores están comprendidos en el intervalo de 8 bits, 16 bits o 32 bits.\n",
    "\n",
    "En la siguiente tabla, se muestran los tamaños comprimidos para valores numéricos específicos cuando se utilizan codificaciones MOSTLY8, MOSTLY16 y MOSTLY32:\n",
    "\n",
    "![](img_56.png)\n",
    "\n",
    "### <a name=\"mark_10.1\"></a>Codificación de run length\n",
    "[Index](#index)\n",
    "\n",
    "La codificación de run length reemplaza un valor que se repite de manera consecutiva por un token que consiste en el valor y un recuento de la cantidad de ocurrencias consecutivas (la longitud de la ejecución). Se crea un diccionario independiente de valores únicos para cada bloque de los valores de columna del disco. (Un bloque de disco de Amazon Redshift ocupa 1 MB). Esta codificación es una opción ideal para una tabla en la que los valores de datos suelen repetirse de manera consecutiva; por ejemplo, cuando la tabla está ordenada según esos valores.\n",
    "\n",
    "Por ejemplo, imagine que una columna de una tabla de grandes dimensiones presenta un dominio previsiblemente pequeño, como una columna COLOR con menos de 10 valores posibles. Es probable que estos valores queden dispuestos en secuencias de gran longitud en toda la tabla, incluso si los datos no están ordenados.\n",
    "\n",
    "No recomendamos aplicar la codificación de run length en ninguna columna que esté designada como clave de ordenación. Los exámenes de rango restringido funcionan mejor cuando los bloques tienen una cantidad similar de filas. Si las columnas de clave de ordenación se comprimen mucho más que otras columnas en la misma consulta, los exámenes de rango restringido podrían tener un rendimiento deficiente.\n",
    "\n",
    "En la siguiente tabla se utiliza el ejemplo de la columna COLOR para mostrar cómo funciona la codificación de run length.\n",
    "\n",
    "![](img_57.png)\n",
    "\n",
    "### <a name=\"mark_10.2\"></a>Codificaciones Text255 y Text32k\n",
    "[Index](#index)\n",
    "\n",
    "Las codificaciones text255 y text32k son útiles para comprimir columnas VARCHAR en las que se repiten con frecuencia las mismas palabras. Se crea un diccionario independiente de palabras únicas para cada bloque de los valores de columna del disco. (Un bloque de disco de Amazon Redshift ocupa 1 MB). El diccionario tiene las primeras 245 palabras únicas de la columna. Estas palabras se reemplazan en el disco por un valor índice de un byte, lo que representa uno de 245 valores, y todas las palabras que no están representadas en el diccionario se almacenan sin comprimir. El proceso se repite para cada bloque de 1 MB del disco. Si las palabras indexadas se repiten con frecuencia en la columna, esta arrojará una alta relación de compresión.\n",
    "\n",
    "Para la codificación text32k, se aplica el mismo principio, pero el diccionario para cada bloque no captura una cantidad específica de palabras. En lugar de ello, el diccionario indexa cada palabra única que encuentra hasta que las entradas combinadas alcancen una longitud de 32K, menos alguna sobrecarga. Los valores índices se almacenan en dos bytes.\n",
    "\n",
    "Por ejemplo, veamos el caso de la columna VENUENAME en la tabla VENUE. Palabras como Arena, Center y Theatre son recurrentes en esta columna y es posible que estén dentro de las primeras 245 palabras que se encuentran en cada bloque si se aplica la compresión text255. En tal caso, esta columna se beneficia de la compresión. El motivo de ello es que cada vez que aparezcan esas palabras, ocuparán solo 1 byte de almacenamiento (en lugar de 5, 6 o 7 bytes, respectivamente).\n",
    "\n",
    "### <a name=\"mark_10.3\"></a>Codificación ZSTD\n",
    "[Index](#index)\n",
    "\n",
    "La codificación zstandard (ZSTD) proporciona una relación de compresión muy alta con un muy buen rendimiento en diferentes conjuntos de datos. La codificación ZSTD funciona especialmente bien para las columnas CHAR y VARCHAR que almacenan una gran variedad de cadenas de caracteres largas y cortas, como descripciones de productos, comentarios de usuarios, registros o cadenas JSON. Mientras que algunos algoritmos, como la codificación Delta o la codificación Mostly pueden, potencialmente, usar más espacios de almacenamiento que si se dejan los datos sin comprimir, es muy poco probable que ZSTD aumente el uso del disco.\n",
    "\n",
    "ZSTD admite los tipos de datos SMALLINT, INTEGER, BIGINT, DECIMAL, REAL, DOUBLE PRECISION, BOOLEAN, CHAR, VARCHAR, DATE, TIMESTAMP y TIMESTAMPTZ.\n",
    "\n",
    "### Conclusión:\n",
    "\n",
    "![](img_58.png)\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/redshift/latest/dg/c_Compression_encodings.html\n",
    "\n",
    "https://aws.amazon.com/es/about-aws/whats-new/2019/10/amazon-redshift-introduces-az64-a-new-compression-encoding-for-optimized-storage-and-high-query-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0a45",
   "metadata": {},
   "source": [
    "### <a name=\"mark_11\"></a> Codificado/Compresión de Columnas.\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "En el siguiente ejemplo tenemos una tabla con 38 millones de registros, en la columna \"venuename\" tendremos nombres de personas, la prueba realizada es crear una nueva tabla \"encoding_venue\" con 7 diferentes tipos de codificación para los datos de la columna \"venuename\".\n",
    "\n",
    "```sql\n",
    "CREATE table encoding_venue(\n",
    "nameraw varchar(100) encode raw,\n",
    "namebytedict varchar(100) encode bytedict,\n",
    "namelzo varchar(100) encode lzo,\n",
    "namerunlength varchar(100) encode runlength,\n",
    "nametext255 varchar(100) encode text255,\n",
    "nametext32k varchar(100) encode text32k,\n",
    "namezstd varchar(100) encode zstd,\n",
    "\n",
    ");\n",
    "```\n",
    "\n",
    "Populando la tabla \"encoding_venue\" con todos los valores de los nombres, el siguiente ejemplo inserta dentro de \"encoding_venue\", a travéz de un SELECT 7 columnas de valores \"venuename\" que vienen desde la tabla \"cartesian_venue\", de esta forma inserta 38 millones de filas repetidas en 7 columnas pero cada columna con su propio tipo de codificación.\n",
    "\n",
    "```sql\n",
    "\n",
    "INSERT INTO encoding_venue\n",
    "(\n",
    "SELECT venuename,venuename,venuename,venuename,venuename,venuename,venuename\n",
    "FROM cartesian_venue\n",
    ");\n",
    "\n",
    "```\n",
    "\n",
    "### Observación:\n",
    "\n",
    "Una codificación de compresión especifica el tipo de compresión que se aplica a una columna de valores de datos a medida que se añaden filas a una tabla.\n",
    "\n",
    "Si no se especifica una compresión en la instrucción CREATE TABLE o ALTER TABLE, Amazon Redshift asigna automáticamente la codificación de compresión de la siguiente manera:\n",
    "\n",
    "A las columnas que están definidas como claves de ordenación se les asigna una compresión RAW.\n",
    "\n",
    "A las columnas que están definidas como tipos de datos BOOLEAN, REAL o DOUBLE PRECISION se les asigna una compresión RAW.\n",
    "\n",
    "Las columnas que se definen como tipos de datos SMALLINT, INTEGER, BIGINT, DECIMAL, CHAR, VARCHAR, DATE, TIMESTAMP o TIMESTAMPTZ tienen asignada la compresión AZ64.\n",
    "\n",
    "Las columnas que se definen como tipos de datos CHAR o VARCHAR tienen asignada la compresión LZO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642d9d5",
   "metadata": {},
   "source": [
    "### <a name=\"mark_12\"></a> Análisis de desempeño con diferentes tipos de compresión\n",
    "\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_59.png)\n",
    "\n",
    "### <a name=\"mark_12.0\"></a>STV_TBL_PERM --> STV = System Table View\n",
    "### [Index](#index)\n",
    "\n",
    "La tabla STV_TBL_PERM tiene información acerca de las tablas permanentes en Amazon Redshift, incluidas las tablas temporales creadas por un usuario para esta sesión. STV_TBL_PERM tiene información de todas las tablas en todas las bases de datos.\n",
    "\n",
    "Esta tabla se diferencia de STV_TBL_TRANS, la cual tiene información relacionada con las tablas transitorias de bases de datos que el sistema crea durante el procesamiento de consultas.\n",
    "\n",
    "Solo los superusuarios pueden ver STV_TBL_PERM. Para obtener más información, consulte Visibilidad de datos en las tablas y vistas de sistema.\n",
    "\n",
    "Columnas de la tabla:\n",
    "\n",
    "|Nombre de la columna | Tipo de datos | Descripción |\n",
    "|:---|:---:|:---|\n",
    "|slice|entero|Sector del nodo asignado a la tabla.|\n",
    "|id|entero|ID de la tabla.|\n",
    "|nombre|character (72)|\tNombre de la tabla.|\n",
    "|rows|bigint|Cantidad de fila de datos en el sector/slice.|\n",
    "|sorted_rows|bigint|Cantidad de filas en el sector que ya están ordenadas en el disco. Si esta cantidad no coincide con la cantidad indicada en ROWS, limpie la tabla para volver a ordenar las filas.|\n",
    "|temp|entero|Indica si es una tabla temporal o no: 0 = false; 1 = true.|\n",
    "|db_id|entero|ID de la base de datos donde se crea la tabla.|\n",
    "|insert_pristine|entero|Para uso interno.|\n",
    "|delete_pristine|entero|Para uso interno.|\n",
    "|backup|entero|Valor que indica si la tabla se incluye en las instantáneas del clúster: 0 = no; 1 = sí. Para obtener más información, consulte el parámetro BACKUP del comando CREATE TABLE.|\n",
    "|dist_style|entero|Estilo de distribución de la tabla a la que pertenece el segmento. Para obtener más información acerca de los valores, consulte Visualización de los estilos de distribución.|\n",
    "|block_count|entero|Número de bloques que utiliza el segmento. El valor es -1 cuando no se puede calcular el número de bloques.|\n",
    "\n",
    "### <a name=\"mark_12.1\"></a> Visualización de los estilos de distribución (EVEN, KEY, ALL)\n",
    "### [Index](#index)\n",
    "\n",
    "La columna RELEFFECTIVEDISTSTYLE en PG_CLASS_INFO indica el estilo de distribución actual de la tabla. Si la tabla usa una distribución automática, RELEFFECTIVEDISTSTYLE es 10, 11 o 12, lo que indica que el estilo de distribución efectivo es AUTO (ALL) o AUTO (EVEN) o AUTO (KEY). Si la tabla usa distribución automática, el estilo de distribución podría inicialmente mostrar AUTO (ALL), para a continuación mostrar AUTO (EVEN) o AUTO (KEY) cuando la tabla crezca.\n",
    "\n",
    "En la siguiente tabla, se proporciona el estilo de distribución para cada valor de la tabla RELEFFECTIVEDISTSTYLE:\n",
    "\n",
    "|RELEFFECTIVEDISTSTYLE|\tEstilo de distribución actual|\n",
    "|:---|:---|\n",
    "|0|\tEVEN|\n",
    "|1|\tKEY|\n",
    "|8|\tALL|\n",
    "|10|\tAUTO (ALL)|\n",
    "|11|\tAUTO (EVEN)|\n",
    "|12|\tAUTO (KEY)|\n",
    "\n",
    "La columna DISTSTYLE en SVV_TABLE_INFO indica el estilo de distribución actual de la tabla. Si la tabla usa una distribución automática, DISTSTYLE es AUTO (ALL), AUTO (EVEN) o AUTO (KEY).\n",
    "\n",
    "En el siguiente ejemplo, se crean cuatro tablas usando los tres estilos de distribución y distribución automática, luego, se consulta la tabla SVV_TABLE_INFO para ver los estilos de distribución.\n",
    "\n",
    "```sql\n",
    "create table public.dist_key (col1 int)\n",
    "diststyle key distkey (col1);\n",
    "\n",
    "insert into public.dist_key values (1);\n",
    "\n",
    "create table public.dist_even (col1 int)\n",
    "diststyle even;\n",
    "\n",
    "insert into public.dist_even values (1);\n",
    "\n",
    "create table public.dist_all (col1 int)\n",
    "diststyle all;\n",
    "\n",
    "insert into public.dist_all values (1);\n",
    "\n",
    "create table public.dist_auto (col1 int);\n",
    "\n",
    "insert into public.dist_auto values (1);\n",
    "\n",
    "select \"schema\", \"table\", diststyle from SVV_TABLE_INFO\n",
    "where \"table\" like 'dist%';\n",
    "```\n",
    "\n",
    "        schema   |    table        | diststyle\n",
    "     ------------+-----------------+------------\n",
    "      public     | dist_key        | KEY(col1)\n",
    "      public     | dist_even       | EVEN\n",
    "      public     | dist_all        | ALL\n",
    "      public     | dist_auto       | AUTO(ALL)\n",
    "      \n",
    "      \n",
    "### <a name=\"mark_12.2\"></a> Utilizamos \"STV_TBL_PERM\" pero para una tabla en especial \"encoding_venue\":\n",
    "### [Index](#index)\n",
    "\n",
    "Como podemos ver, tabla \"encoding_venue\" se encuentra distribuida en 4 slices, si tenemos 2 nodos (2 slices por nodo), conteniendo 9,721,099 filas por cada uno de los slices, copio el id de mi tabla (sin la coma) \"101395\"\n",
    "\n",
    "![](img_60.png)\n",
    "\n",
    "### <a name=\"mark_12.3\"></a>Continuamos con la sentencia \"STV_BLOCKLIST\" --> cantidad de bloques de 1 MB.\n",
    "### [Index](#index)\n",
    "\n",
    "![](img_61.png)\n",
    "\n",
    "Realizamos la misma búsquda pero para el id de la tabla que habíamos copiado \"101395\".\n",
    "\n",
    "![](img_62.png)\n",
    "\n",
    "Recordemos los tipos de codificación que utilizamos al crear la tabla:\n",
    "\n",
    "```sql\n",
    "CREATE table encoding_venue(\n",
    "nameraw varchar(100) encode raw, --0\n",
    "namebytedict varchar(100) encode bytedict, --1\n",
    "namelzo varchar(100) encode lzo, --2\n",
    "namerunlength varchar(100) encode runlength,--3\n",
    "nametext255 varchar(100) encode text255,--4\n",
    "nametext32k varchar(100) encode text32k,--5\n",
    "namezstd varchar(100) encode zstd,--6\n",
    "\n",
    ");\n",
    "```\n",
    "\n",
    "O se que para la codificación \"0\" (raw: sin coprimir) estoy usando 203 bloques de datos por cada segmento/sector/slice en nuestros nodos, \n",
    "\n",
    "La columna \"1\" y \"6\" (compresión bytedict, zstd respectivamente) solo ocupa 10 bloques de datos, las mejores, las otras no son tan buenas y \"2\" (lzo) que incrementó el valor de bloques.\n",
    "\n",
    "### <a name=\"mark_12.4\"></a>Comparando codificación Raw vs. ZSTD.\n",
    "### [Index](#index)\n",
    "\n",
    "Col = 0 --> Raw, tenemos 47,759 valores por cada bloque.\n",
    "\n",
    "![](img_63.png)\n",
    "\n",
    "Col = 6 --> ZSTD, tenemos 982,383 valores por cada bloque.\n",
    "\n",
    "![](img_64.png)\n",
    "\n",
    "\n",
    "STV_BLOCKLIST tiene la cantidad de bloques de 1 MB de disco que utiliza cada sector, tabla o columna en una base de datos.\n",
    "\n",
    "Utilice consultas de agregación con STV_BLOCKLIST, como se muestra en los siguientes ejemplos, para determinar la cantidad de bloques de 1 MB de disco asignados para cada base de datos, tabla, sector o columna. También puede utilizar [STV_PARTITIONS](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STV_PARTITIONS.html) para obtener información resumida acerca de la utilización del disco.\n",
    "\n",
    "Solo los superusuarios pueden ver STV_BLOCKLIST. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "#### Columnas de la tabla:\n",
    "\n",
    "|Nombre de la columna|\tTipo de datos|\tDescripción|\n",
    "|:---|:---:|:---|\n",
    "|slice|\tentero|\tSector del nodo.|\n",
    "|col|\tentero|\tÍndice con base cero para la columna. Cada tabla que cree tiene tres columnas ocultas anexadas: INSERT_XID, DELETE_XID y ROW_ID (OID). Una tabla con 3 columnas definidas por el usuario tiene 6 columnas reales, y las columnas definidas por el usuario se enumeran internamente como 0, 1 y 2. Las columnas INSERT_XID, DELETE_XID y ROW_ID se enumeran 3, 4 y 5, respectivamente, en este ejemplo.|\n",
    "|tbl|\tentero|\tID de la tabla para la tabla de la base de datos.|\n",
    "|blocknum|\tentero|\tID para el bloque de datos.|\n",
    "|num_values|\tentero|\tCantidad de valores contenidos en el bloque.|\n",
    "|extended_limits|\tentero|\tPara uso interno.|\n",
    "|minvalue|\tbigint|\tValor mínimo de datos del bloque. Almacena los primeros ocho caracteres como un entero de 64 bits para datos no numéricos. Se utiliza para explorar el disco.|\n",
    "|maxvalue|\tbigint\t|Valor máximo de dato del bloque. Almacena los primeros ocho caracteres como un entero de 64 bits para datos no numéricos. Se utiliza para explorar el disco.|\n",
    "|sb_pos|\tentero|\tIdentificador interno de Amazon Redshift para la posición del superbloque en el disco.|\n",
    "|pinned|\tentero|\tIndica si el bloque se conectó o no a la memoria como parte de la carga previa: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|on_disk|\tentero|\tIndica si el bloque se almacenó automáticamente o no en el disco: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|modified|\tentero|\tIndica si el bloque se modificó o no: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "hdr_modified|\tentero|\tIndica si el encabezado del bloque se modificó o no: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|unsorted|\tentero|\tIndica si el bloque está desordenado o no: 0 = false; 1 = true. El valor predeterminado es verdadero.|\n",
    "|tombstone|\tentero|\tPara uso interno.|\n",
    "|preferred_diskno|\tentero\t|Cantidad de discos en que debe estar el bloque, excepto que el disco tenga una falla. Una vez arreglado el disco, el bloque volverá a ese disco.|\n",
    "|temporary|\tentero\t|Indica si el bloque tiene o no datos temporales, como una tabla temporal o resultados intermedios de consulta: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|newblock|\tentero\t|Indica si un bloque es o no nuevo (true) o si nunca se guardó en el disco (false): 0 = false; 1 = true.|\n",
    "|num_readers|\tentero\t|Cantidad de referencias en cada bloque.|\n",
    "|flags|\tentero\t|Indicadores internos de Amazon Redshift para el encabezado del bloque.|\n",
    "\n",
    "### Consultas de ejemplo:\n",
    "\n",
    "STV_BLOCKLIST tiene una fila por cada bloque del disco designado, por lo que una consulta que selecciona todas las filas posiblemente devuelva una gran cantidad de filas. Le recomendamos usar solo las consultas de agregación con STV_BLOCKLIST.\n",
    "\n",
    "La vista [SVV_DISKUSAGE](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_DISKUSAGE.html) proporciona información similar en un formato más sencillo de usar. Sin embargo, en el siguiente ejemplo se demuestra un uso de la tabla STV_BLOCKLIST.\n",
    "\n",
    "Para determinar la cantidad de bloques de 1 MB que utiliza cada columna de la tabla VENUE, escriba la siguiente consulta:\n",
    "\n",
    "```sql\n",
    "\n",
    "select col, count(*)\n",
    "from stv_blocklist, stv_tbl_perm\n",
    "where stv_blocklist.tbl = stv_tbl_perm.id\n",
    "and stv_blocklist.slice = stv_tbl_perm.slice\n",
    "and stv_tbl_perm.name = 'venue'\n",
    "group by col\n",
    "order by col;\n",
    "\n",
    "```\n",
    "\n",
    "Esta consulta devuelve la cantidad de bloques de 1 MB asignados a cada columna de la tabla VENUE, tal como se muestra en los siguientes datos de ejemplo:\n",
    "\n",
    " |col | count|\n",
    "|:---:|:---:|\n",
    "  | 0 |  4|\n",
    "  | 1 |  4|\n",
    "  | 2 |  4|\n",
    "  | 3 |  4|\n",
    "  | 4 |  4|\n",
    "  | 5 |  4|\n",
    "  | 7 |  4|\n",
    "  | 8 |  4|\n",
    "   \n",
    "(8 rows)\n",
    "\n",
    "En la siguiente consulta, se muestra si una tabla está distribuida en todos los sectores:\n",
    "\n",
    "```sql\n",
    "\n",
    "select trim(name) as table, stv_blocklist.slice, stv_tbl_perm.rows\n",
    "from stv_blocklist,stv_tbl_perm\n",
    "where stv_blocklist.tbl=stv_tbl_perm.id\n",
    "and stv_tbl_perm.slice=stv_blocklist.slice\n",
    "and stv_blocklist.id > 10000 and name not like '%#m%'\n",
    "and name not like 'systable%'\n",
    "group by name, stv_blocklist.slice, stv_tbl_perm.rows\n",
    "order by 3 desc;\n",
    "\n",
    "```\n",
    "\n",
    "Esta consulta produce el siguiente ejemplo de salida, que muestra una distribución uniforme de los datos en la tabla que tiene la mayor cantidad de filas:\n",
    "\n",
    "|table   | slice | rows|\n",
    "|:---|:---:|:---:|\n",
    "|listing  |    13 | 10527|\n",
    "|listing  |    14 | 10526|\n",
    "|listing  |     8 | 10526|\n",
    "|listing  |     9 | 10526|\n",
    "|listing  |     7 | 10525|\n",
    "|listing  |     4 | 10525|\n",
    "|listing  |    17 | 10525|\n",
    "|listing  |    11 | 10525|\n",
    "|listing  |     5 | 10525|\n",
    "|listing  |    18 | 10525|\n",
    "|listing  |    12 | 10525|\n",
    "|listing  |     3 | 10525|\n",
    "|listing  |    10 | 10525|\n",
    "|listing  |     2 | 10524|\n",
    "|listing  |    15 | 10524|\n",
    "|listing  |    16 | 10524|\n",
    "|listing  |     6 | 10524|\n",
    "|listing  |    19 | 10524|\n",
    "|listing  |     1 | 10523|\n",
    "|listing  |     0 | 10521|\n",
    "...\n",
    "(180 rows)\n",
    "\n",
    "La siguiente consulta determina si hay algún bloque del tipo tombstone confirmado en el disco:\n",
    "\n",
    "```sql\n",
    "select slice, col, tbl, blocknum, newblock\n",
    "from stv_blocklist\n",
    "where  tombstone > 0;\n",
    "```\n",
    "\n",
    "\n",
    "|slice | col |   tbl  | blocknum | newblock|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|4     |  0  | 101285 |    0     |   1|\n",
    "|4     |  2  | 101285 |    0     |   1|\n",
    "|4     |  4  | 101285 |    1     |   1|\n",
    "|5     |  2  | 101285 |    0     |   1|\n",
    "|5     |  0  | 101285 |    0     |   1|\n",
    "|5     |  1  | 101285 |    0     |   1|\n",
    "|5     |  4  | 101285 |    1     |   1|\n",
    "...\n",
    "(24 rows)\n",
    "\n",
    "### <a name=\"mark_12.5\"></a>ANALYZE COMPRESSION, recomienda que tipo de compresión usar.\n",
    "### [Index](#index)\n",
    "\n",
    "Realiza un análisis de compresión y produce un informe con la codificación de compresión sugerida para las tablas analizadas. Para cada columna, el informe incluye un cálculo de la reducción potencial de espacio en disco en comparación con la codificación actual.\n",
    "\n",
    "![](img_65.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Lectura recomendada:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_intro_STV_tables.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd7e4c",
   "metadata": {},
   "source": [
    "### <a name=\"mark_13\"></a>Estilos de distribución con Redshift\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "![](img_66.png)\n",
    "\n",
    "![](img_67.png)\n",
    "\n",
    "### Las tres tipos de distribuciones disponibles en Redshift:\n",
    "\n",
    "También podemos ver [estilos de distribuciones](#mark_12.1)\n",
    "\n",
    "![](img_68.png)\n",
    "\n",
    "Cuando crea una tabla, puede designar uno de los cuatro estilos de distribución: **AUTO, EVEN, KEY o ALL**.\n",
    "\n",
    "Si no se especifica un estilo de distribución, Amazon Redshift usa la **distribución AUTO**.\n",
    "\n",
    "### <a name=\"mark_13.0\"></a>Distribución AUTO \n",
    "[Index](#index_01)\n",
    "\n",
    "Con la distribución AUTO, Amazon Redshift asigna un estilo de distribución óptimo basado en el tamaño de los datos de la tabla. Por ejemplo, si se especifica el estilo de distribución AUTO, Amazon Redshift asigna inicialmente el estilo de distribución ALL a una tabla pequeña. Cuando la tabla crezca, Amazon Redshift podría cambiar el estilo de distribución a KEY y elegir la clave principal (o una columna de la clave primaria compuesta) como la clave de distribución. Si la tabla crece y ninguna de las columnas es adecuada para ser la clave de distribución, Amazon Redshift cambia el estilo de distribución a EVEN. El cambio en el estilo de distribución se produce en segundo plano y tiene un impacto mínimo en las consultas de los usuarios.\n",
    "\n",
    "Si desea consultar las acciones que Amazon Redshift realizó de forma automática para modificar la clave de distribución de una tabla, consulte [SVL_AUTO_WORKER_ACTION](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVL_AUTO_WORKER_ACTION.html). Si desea conocer las recomendaciones actuales relativas a la modificación de la clave de distribución de una tabla, consulte [SVV_ALTER_TABLE_RECOMMENDATIONS](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_ALTER_TABLE_RECOMMENDATIONS.html).\n",
    "\n",
    "Para ver el estilo de distribución aplicado a una tabla, consulte la vista de catálogo del sistema PG_CLASS_INFO. Para obtener más información, consulte [Visualización de los estilos de distribución](https://docs.aws.amazon.com/es_es/redshift/latest/dg/viewing-distribution-styles.html). Si no se especifica un estilo de distribución con la instrucción CREATE TABLE, Amazon Redshift aplica la distribución AUTO.\n",
    "\n",
    "### <a name=\"mark_13.1\"></a>Distribución EVEN\n",
    "[Index](#index_01)\n",
    "\n",
    "![](img_71.png)\n",
    "\n",
    "El nodo principal distribuye las filas entre los sectores con un método de turnos rotativos, independientemente de los valores de cualquier columna en particular. La distribución EVEN resulta adecuada cuando una tabla no participa en las uniones. Además, resulta adecuada en los casos en que no hay una opción clara entre la distribución KEY y la distribución ALL.\n",
    "\n",
    "Algoritmo utilizado: round-robin\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "El algoritmo de planificación Round-Robin (RR) es un algoritmo de planificación de procesos simple y justo, que se utiliza en sistemas operativos multitarea. El algoritmo RR asigna a cada proceso una cantidad de tiempo de CPU denominada **quantum**. Cuando un proceso se ejecuta, se le asigna el control de la CPU durante el quantum. Al final del quantum, el proceso se suspende y el siguiente proceso en la cola de \"listos\" se ejecuta. El proceso que se suspendió se coloca de nuevo en la cola de \"listos\" y se ejecutará de nuevo cuando le llegue su turno.\n",
    "\n",
    "El algoritmo RR es justo porque cada proceso tiene la oportunidad de ejecutarse durante un quantum, independientemente de su prioridad. Sin embargo, el algoritmo RR puede no ser eficiente si los procesos tienen diferentes necesidades de CPU. Por ejemplo, un proceso que necesita más tiempo de CPU que el quantum podría terminar ejecutándose de manera ineficiente.\n",
    "\n",
    "El algoritmo RR funciona de la siguiente manera:\n",
    "\n",
    "1. Se crea una cola de listos que contiene todos los procesos listos para ejecutarse.\n",
    "2. El proceso en la parte superior de la cola de listos se ejecuta durante el quantum.\n",
    "3. Al final del quantum, el proceso se suspende y el siguiente proceso en la cola de listos se ejecuta.\n",
    "4. El proceso que se suspendió se coloca de nuevo en la cola de listos.\n",
    "5. Los pasos 2-4 se repiten hasta que todos los procesos hayan terminado de ejecutarse.\n",
    "\n",
    "El algoritmo RR se puede implementar utilizando una cola circular. La cola circular es una estructura de datos que se utiliza para almacenar un conjunto de datos en el que el primer elemento y el último elemento están conectados. Esto permite que el algoritmo RR acceda al primer elemento de la cola sin tener que desplazarse por la cola completa.\n",
    "\n",
    "El algoritmo RR es un algoritmo simple y eficiente que se utiliza en una amplia variedad de sistemas operativos. Es un buen algoritmo para sistemas operativos multitarea en los que todos los procesos tienen las mismas necesidades de CPU.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### <a name=\"mark_13.2\"></a>Distribución KEY\n",
    "[Index](#index_01)\n",
    "\n",
    "![](img_69.png)\n",
    "\n",
    "Las filas se distribuyen según los valores de una columna (valores clave). El nodo principal ubica juntos los valores que coinciden en el mismo sector del nodo. Si distribuye un par de tablas en las claves de unión, el nodo principal ubica las filas en los sectores según los valores de las columnas de unión. De este modo, los valores que coinciden de las columnas que tienen en común se almacenan juntos físicamente.\n",
    "\n",
    "### <a name=\"mark_13.3\"></a>Distribución ALL\n",
    "[Index](#index_01)\n",
    "\n",
    "![](img_70.png)\n",
    "\n",
    "Se distribuye una copia de toda la tabla a cada nodo. Mientras que la distribución EVEN o la distribución KEY colocan solo una parte de las filas de la tabla en cada nodo, la distribución ALL garantiza que se coloque cada fila para cada combinación en la que participa la tabla.\n",
    "\n",
    "La distribución ALL multiplica el almacenamiento requerido por la cantidad de nodos del clúster, por lo que demanda más tiempo para cargar, actualizar o insertar datos en distintas tablas. La distribución ALL es adecuada solo para tablas con movimientos relativamente lentos, es decir tablas que no se actualizan con frecuencia ni de forma generalizada. Dado que el costo de redistribuir tablas pequeñas durante una consulta es bajo, no hay un beneficio significativo para definir tablas de dimensiones pequeñas como DISTSTYLE ALL.\n",
    "\n",
    "#### nota:\n",
    "\n",
    "Una vez que haya especificado un estilo de distribución para una columna, Amazon Redshift se encarga de la distribución de datos en el nivel del clúster. Amazon Redshift no requiere ni admite el concepto de partición de datos dentro de los objetos de la base de datos. No es necesario crear espacios de tablas ni definir esquemas de partición para las tablas.\n",
    "\n",
    "En ciertas situaciones, puede cambiar el estilo de distribución de una tabla después de crearla. Para obtener más información, consulte [ALTER TABLE](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_ALTER_TABLE.html). En aquellas situaciones en las que no puede cambiar el estilo de distribución de una tabla después de crearla, puede volver a crear la tabla y rellenar la nueva tabla con una copia profunda. Para obtener más información, consulte [Realización de una copia profunda](https://docs.aws.amazon.com/es_es/redshift/latest/dg/performing-a-deep-copy.html).\n",
    "\n",
    "\n",
    "### Lectura Recomendada:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_choosing_dist_sort.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e9871a",
   "metadata": {},
   "source": [
    "### <a name=\"mark_14\"></a>Evaluando los estilos de distribución\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "## Importante!\n",
    "\n",
    "**Recordar que los estilos de distribución y la compresión son establecidos al momento de la creación de la tabla.**\n",
    "\n",
    "Como ejemplo crearemos un tablas simples con 1 sola columna y cada uno de los estilos de distribución\n",
    "\n",
    "```sql\n",
    "CREATE TABLE dist_key (columna int)\n",
    "diststyle key distkey (columna); -- Necesito definir la columna clave\n",
    "INSERT INTO dist_key VALUES (10);\n",
    "\n",
    "CREATE TABLE dist_even (columna int)\n",
    "diststyle even; -- No necesito definir la columna clave, ya que usa el algorito de Round-Robin.\n",
    "INSERT INTO dist_even VALUES (10);\n",
    "\n",
    "CREATE TABLE dist_all (columna int)\n",
    "diststyle all; -- No necesito definir la columna clave, ya que distribuye todas las tablas en cada nodo.\n",
    "INSERT INTO dist_all VALUES (10);\n",
    "\n",
    "CREATE TABLE dist_auto (columna int);-- Evaluando la distribución automática\n",
    "INSERT INTO dist_auto VALUES (10)\n",
    "```\n",
    "\n",
    "### <a name=\"mark_14.0\"></a>SVV_TABLE_INFO, información resumen de tablas en base de datos.\n",
    "[Index](#index_01)\n",
    "\n",
    "Muestra información de resumen acerca de las tablas en la base de datos. La vista filtra las tablas de sistema y muestra solamente las tablas definidas por el usuario.\n",
    "\n",
    "Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna|\tTipo de datos|\tDescripción|\n",
    "|:---|:---:|:---|\n",
    "|database\t|texto\t|Nombre de la base de datos.|\n",
    "|schema\t|texto\t|Nombre del esquema.|\n",
    "|table_id\t|oid\t|ID de la tabla.|\n",
    "|table|\ttexto\t|Nombre de la tabla.|\n",
    "|encoded\t|texto\t|Valor que indica si la columna tiene definida alguna codificación de compresión.|\n",
    "|diststyle\t|texto\t|Estilo de distribución o columna de clave de distribución, siempre que se haya definido una clave de distribución. Entre los valores posibles se incluyen EVEN, KEY(column), ALL, AUTO(ALL), AUTO(EVEN) y AUTO(KEY(column)).|\n",
    "|sortkey1\t|texto\t|Primera columna en la clave de ordenación, siempre que se haya definido una clave de ordenación. Entre los valores posibles se incluyen: column, AUTO(SORTKEY) y AUTO(SORTKEY(column)).|\n",
    "|max_varchar\t|entero\t|Tamaño de la columna más grande que utiliza un tipo de datos VARCHAR.|\n",
    "|sortkey1_enc|\tcharacter (32)\t|Codificación de compresión de la primera columna en la clave de ordenación, siempre que se haya definido una clave de ordenación.|\n",
    "|sortkey_num\t|entero\t|Cantidad de columnas definidas como claves de ordenación.|\n",
    "|size|\tbigint|\tTamaño de la tabla, en bloques de datos de 1 MB.|\n",
    "|pct_used\t|numeric (10,4)|\tPorcentaje de espacio disponible que usa la tabla.|\n",
    "|empty\t|bigint\t|Para uso interno. Esta columna ya no se utiliza y se eliminará en una futura versión.|\n",
    "|unsorted\t|numeric (5,2)\t|Porcentaje de filas desordenadas en la tabla.|\n",
    "|stats_off\t|numeric (5,2)|\tNúmero que indica cuán obsoletas son las estadísticas de la tabla; 0 es actual y 100 es desactualizada.|\n",
    "|tbl_rows|\tnumeric (38,0)\t|Cantidad total de filas en la tabla. Este valor incluye las filas marcadas para ser eliminadas, pero que aún no se han limpiado.|\n",
    "|skew_sortkey1\t|numeric (19,2)|\tProporción del tamaño de la columna más grande que no sea una clave de ordenación con el tamaño de la primera columna de la clave de ordenación, siempre que se haya definido una clave de ordenación. Utilice este valor para evaluar la eficacia de la clave de ordenación.|\n",
    "|skew_rows\t|numeric (19,2)\t|Proporción de la cantidad de filas en el sector con la mayor cantidad de filas con la cantidad de filas en el sector con la menor cantidad de filas.|\n",
    "|estimated_visible_rows\t|numeric (38,0)\t|Las filas estimadas en la tabla. Este valor no incluye las filas marcadas para ser eliminadas.|\n",
    "|risk_event|\ttexto|\tInformación de riesgo sobre una tabla. El campo está separado en partes: `risk_typex/id/timestamp` risk_type, donde 1 indica que se ejecutó COPY command with the EXPLICIT_IDS option. Amazon Redshift ya no verifica la unicidad de las columnas IDENTITY en la tabla. Para obtener más información, consulte EXPLICIT_IDS. El ID de la transacción, xid, que introduce el riesgo. El timestamp cuando se ejecutó el comando COPY. El siguiente ejemplo muestra el valor en el campo. `1/1107/2019-06-22 07:16:11.292952`|\n",
    "|vacuum_sort_benefit\t|numeric(12,2)|\tEl porcentaje máximo de mejora estimado del rendimiento de la consulta del escaneo cuando ejecuta una ordenación vacuum.\n",
    "|create_time\t|TIMESTAMP sin zona horaria\t|Marca de tiempo UNIX de cuando se creó la tabla.|\n",
    "\n",
    "```sql\n",
    "-- Checkeamos la estructura con \"svv_table_info\"\n",
    "SELECT *\n",
    "FROM pg_catalog.svv_table_info\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "![](img_72.png)\n",
    "\n",
    "Filtramos por las tablas creadas anteriormente.\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM pg_catalog.svv_table_info\n",
    "WHERE \"table\" LIKE '%dist%'\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "![](img_73.png)\n",
    "\n",
    "### <a name=\"mark_14.0.0\"></a>SVV_TABLE\n",
    "[Index](#index_01)\n",
    "\n",
    "Utilice SVV_TABLES para ver las tablas en catálogos locales y externos (es un resumen de SVV_TABLE_INFO).\n",
    "\n",
    "SVV_TABLES es visible para todos los usuarios. Los superusuarios pueden ver todas las filas; los usuarios normales solo pueden ver los metadatos a los que tienen acceso.\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna\t|Tipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|table_catalog|\ttexto\t|Nombre del catálogo donde se encuentra la tabla.|\n",
    "|table_schema|\ttexto\t|Nombre del esquema para la tabla.|\n",
    "|table_name|\ttexto\t|El nombre de la tabla.|\n",
    "|table_type|\ttexto\t|El tipo de tabla. Los valores posibles son vistas, tablas externas y tablas base.|\n",
    "|remarks\t|texto\t|Observaciones.|\n",
    "\n",
    "### <a name=\"mark_14.1\"></a>PG_TABLE_DEF, Almacena información acerca de las columnas de la tabla.\n",
    "[Index](#index_01)\n",
    "\n",
    "PG_TABLE_DEF solo devuelve información acerca de las tablas que son visibles para el usuario. Si PG_TABLE_DEF no devuelve los resultados esperados, verifique que el parámetro search_path se estableció correctamente para incluir los esquemas relevantes.\n",
    "\n",
    "Puede utilizar [SVV_TABLE_INFO](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_TABLE_INFO.html) para consultar información más exhaustiva acerca de una tabla, incluidos el sesgo de distribución de datos, el sesgo de distribución de claves, el tamaño de tabla y las estadísticas.\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna\t|Tipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|schemaname\t|nombre\t|Nombre del esquema.|\n",
    "|tablename\t|nombre\t|Nombre de la tabla.|\n",
    "|column\t|nombre|\tNombre de la columna.|\n",
    "|type\t|texto\t|Tipo de datos de la columna.|\n",
    "|encoding\t|character (32)\t|Codificación de la columna.|\n",
    "|distkey\t|booleano\t|True si esta columna es la clave de distribución para la tabla.|\n",
    "|sortkey\t|entero|\tOrden de la columna en la clave de ordenación. Si la tabla utiliza una clave de ordenación compuesta, todas las columnas que forman parte de la clave de ordenación tienen un valor positivo que indica la posición de la columna en la clave de ordenación. Si la tabla utiliza una clave de ordenación intercalada, cada una de las columnas que formen parte de la clave de ordenación tienen un valor que puede ser positivo o negativo, donde el valor absoluto indica la posición de la columna en la clave de ordenación. Si es 0, la columna no forma parte de la clave de ordenación.|\n",
    "|notnull|\tbooleano|\tTrue si la columna tiene una restricción NOT NULL.|\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM pg_table_def\n",
    "WHERE tablename = 'users';\n",
    "```\n",
    "![](img_74.png)\n",
    "\n",
    "Por ejemplo para la tabla \"users\" podemos identificar la columna utilizada para la distribution key.\n",
    "\n",
    "![](img_75.png)\n",
    "\n",
    "Me gustaría saber cómo están distribuidos mis datos.\n",
    "\n",
    "### <a name=\"mark_14.2\"></a>SVV_DISKUSAGE, combinación de las tablas STV_TBL_PERM y STV_BLOCKLIST\n",
    "[Index](#index_01)\n",
    "\n",
    "Amazon Redshift crea la vista de sistema SVV_DISKUSAGE mediante la combinación de las tablas STV_TBL_PERM y STV_BLOCKLIST. La vista SVV_DISKUSAGE tiene información relacionada con la asignación de datos para las tablas en una base de datos.\n",
    "\n",
    "Utilice consultas de agregación con SVV_DISKUSAGE, ver [ejemplos](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_DISKUSAGE.html), para determinar la cantidad de bloques de disco asignados a cada base de datos, tabla, sector o columna. Cada bloque de datos utiliza 1 MB. También puede utilizar [STV_PARTITIONS](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STV_PARTITIONS.html) para obtener información resumida acerca de la utilización del disco.\n",
    "\n",
    "Solo los superusuarios pueden ver SVV_DISKUSAGE. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "#### nota:\n",
    "Esta vista solo está disponible cuando se consultan clústeres aprovisionados.\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna|\tTipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|db_id\t|entero\t|ID de la base de datos.|\n",
    "|nombre|\tcharacter (72)|\tNombre de la tabla.|\n",
    "|slice\t|entero|\tSector de datos asignado a la tabla.|\n",
    "|col\t|entero\t|Índice con base cero para la columna. Cada tabla que cree tiene tres columnas ocultas anexadas: INSERT_XID, DELETE_XID y ROW_ID (OID). Una tabla con 3 columnas definidas por el usuario tiene 6 columnas reales, y las columnas definidas por el usuario se enumeran internamente como 0, 1 y 2. Las columnas INSERT_XID, DELETE_XID y ROW_ID se enumeran 3, 4 y 5, respectivamente, en este ejemplo.\n",
    "|tbl|\tentero|\tID de la tabla.|\n",
    "|blocknum|\tentero\t|ID para el bloque de datos.|\n",
    "|num_values|\tentero|\tCantidad de valores contenidos en el bloque.|\n",
    "|minvalue|\tbigint\t|Valor mínimo contenido en el bloque.|\n",
    "|maxvalue|\tbigint|\tValor máximo contenido en el bloque.|\n",
    "|sb_pos\t|entero\t|Identificador interno para la posición del super bloque en el disco.|\n",
    "|pinned\t|entero\t|Indica si el bloque se conectó o no a la memoria como parte de la carga previa: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|on_disk\t|entero|\tIndica si el bloque se almacenó automáticamente o no en el disco: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|modified\t|entero\t|Indica si el bloque se modificó o no: 0 = false; 1 = true. La opción predeterminada es falso.\n",
    "|hdr_modified|\tentero\t|Indica si el encabezado del bloque se modificó o no: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|unsorted\t|entero\t|Indica si el bloque está desordenado o no: 0 = false; 1 = true. El valor predeterminado es verdadero.|\n",
    "|tombstone\t|entero\t|Para uso interno.|\n",
    "|preferred_diskno|\tentero|\tCantidad de discos en que debe estar el bloque, excepto que el disco tenga una falla. Una vez arreglado el disco, el bloque volverá a ese disco.|\n",
    "|temporary\t|entero\t|Indica si el bloque tiene o no datos temporales, como una tabla temporal o resultados intermedios de consulta: 0 = false; 1 = true. La opción predeterminada es falso.|\n",
    "|newblock\t|entero\t|Indica si un bloque es o no nuevo (true) o si nunca se guardó en el disco (false): 0 = false; 1 = true.|\n",
    "```sql\n",
    "SELECT *\n",
    "FROM pg_catalog.svv_diskusage \n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "![](img_76.png)\n",
    "\n",
    "Buscamos la tabla \"users\" y la columna que sabemos que es \"distribution key\".\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM pg_catalog.svv_diskusage \n",
    "WHERE \"name\" = 'users'\n",
    "AND col = 0;\n",
    "```\n",
    "\n",
    "![](img_77.png)\n",
    "\n",
    "Veamos el comportamiento de los 4 slices y la distribución de los datos.\n",
    "\n",
    "```sql\n",
    "SELECT distinct slice, col, num_values, \"minvalue\", \"maxvalue\"\n",
    "FROM pg_catalog.svv_diskusage \n",
    "WHERE \"name\" = 'users'\n",
    "AND col = 0\n",
    "AND num_values > 0\n",
    "ORDER BY slice, col;\n",
    "```\n",
    "\n",
    "Como podemos ver los datos están muy bien distribuidos.\n",
    "\n",
    "![](img_78.png)\n",
    "\n",
    "La siguiente sentencia crea una nueva tabla \"user_key_state\", en la cual ahora su distribution key será la columna \"state\" usando/copiando la tabla \"users\".\n",
    "```sql\n",
    "CREATE TABLE user_key_state distkey(state) AS (SELECT * FROM users)\n",
    "```\n",
    "\n",
    "A continuación realizamos el mismo análisis pero con la nueva tabla \"user_key_state\"\n",
    "\n",
    "```sql\n",
    "SELECT distinct slice, col, num_values, \"minvalue\", \"maxvalue\"\n",
    "FROM pg_catalog.svv_diskusage \n",
    "WHERE \"name\" = 'user_key_state'\n",
    "AND col = 0\n",
    "AND num_values > 0\n",
    "ORDER BY slice, col;\n",
    "```\n",
    "\n",
    "A continuación podemos ver que la distribución por la columna \"state\" no fue muy performante.\n",
    "\n",
    "![](img_79.png)\n",
    "\n",
    "Repetimos el ejercicio para **\"diststyle even\"**:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE user_even diststyle even AS (SELECT * FROM users)\n",
    "\n",
    "SELECT distinct slice, col, num_values, \"minvalue\", \"maxvalue\"\n",
    "FROM pg_catalog.svv_diskusage \n",
    "WHERE \"name\" = 'user_even'\n",
    "AND col = 0\n",
    "AND num_values > 0\n",
    "ORDER BY slice, col;\n",
    "```\n",
    "\n",
    "Como vemos las columnas/datos están equitativamente distribuidos, pero recordar que están distribuidos al azar por el algoritomo RR, esto quiere decir que a pesar de que se encuentre perfectamente distribuido no será una distribución optima para realizar analítica.\n",
    "\n",
    "![](img_80.png)\n",
    "\n",
    "Probando con **\"diststyle all\"**\n",
    "\n",
    "```sql\n",
    "CREATE TABLE user_all diststyle all AS (SELECT * FROM users)\n",
    "\n",
    "SELECT distinct slice, col, num_values, \"minvalue\", \"maxvalue\"\n",
    "FROM pg_catalog.svv_diskusage \n",
    "WHERE \"name\" = 'user_all'\n",
    "AND col = 0\n",
    "AND num_values > 0\n",
    "ORDER BY slice, col;\n",
    "\n",
    "```\n",
    "\n",
    "Como vemos está replicando cada tabla en el primer slice de cada nodo.\n",
    "\n",
    "![](img_81.png)\n",
    "\n",
    "### Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_TABLE_INFO.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_TABLES.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_PG_TABLE_DEF.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_CREATE_TABLE_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c3c34",
   "metadata": {},
   "source": [
    "### <a name=\"mark_15\"></a> sortkey, Llaves de ordenamiento para optimizar nuestras consultas\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Ordenar las columnas por cierto criterio para que sean guardadas en nuestros nodos, cuando evaluamos las tablas y columnas podemos ver una columna de valores mínimos y valores máximos, esto quiere decir que cada bloque de datos sabe en que rango de filas está trabajando, por ejemplo rango numerico, alfabetico, o de fechas, si utilizamos un \"where date between ...\" esta sentencia descartará automaticamente las fechas que no están en ese rango y solo buscará los bloques para los cuales el mínimo y máximo valor son útiles.\n",
    "\n",
    "![](img_82.png)\n",
    "\n",
    "#### nota:\n",
    "\n",
    "Se recomienda que cree sus tablas con SORTKEY AUTO. En este caso, Amazon Redshift utiliza la optimización automática de tablas para elegir la clave de ordenación. Para obtener más información, consulte Uso de la optimización automática de tablas. En el resto de esta sección, se proporcionan detalles sobre el orden.\n",
    "\n",
    "Cuando se crea una tabla, se pueden definir, como alternativa, una o varias de sus columnas como claves de ordenación. Cuando carga por primera vez los datos en una tabla vacía, las filas se almacenan en el disco de forma ordenada. Se transmite la información relacionada con las columnas con clave de ordenación al planificador de consultas, que utiliza esta información para construir planes que aprovechen la forma en la que se almacenan los datos. Para obtener más información, consulte [CREATE TABLE](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_CREATE_TABLE_NEW.html). Para obtener información sobre las prácticas recomendadas a la hora de crear una clave de clasificación, consulte [Elección de claves de ordenación recomendadas](https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_best-practices-sort-key.html).\n",
    "\n",
    "La ordenación permite encargarse eficazmente de predicados restringidos por rangos. Amazon Redshift almacena los datos de columna en bloques de disco de 1 MB. Los valores mínimo y máximo de cada bloque se almacenan como parte de los metadatos. Si una consulta usa un predicado de rango restringido, el procesador de consultas puede usar los valores mínimos y máximos para omitir rápidamente grandes cantidades de bloques durante los análisis de las tablas. Por ejemplo, supongamos que una tabla almacena cinco años de datos ordenados por fecha y una consulta especifica un rango de fechas correspondiente a un mes. En este caso, se puede eliminar hasta el 98 % de los bloques de disco del análisis. Si los datos no están ordenados, se deben examinar más bloques del disco (posiblemente todos).\n",
    "\n",
    "Puede especificar una clave de ordenación compuesta o intercalada. Una clave de ordenación compuesta es más eficaz cuando los predicados de la consulta usan un prefijo, que es un subconjunto ordenado de columnas de clave de ordenación. Una clave de ordenación intercalada le otorga el mismo peso a cada columna de la clave de ordenación, por lo que los predicados de la consulta pueden usar cualquier subconjunto de columnas que conforman la clave de ordenación, en cualquier orden.\n",
    "\n",
    "Para comprender el impacto de la clave de ordenación seleccionada en el rendimiento de las consultas, utilice el comando [EXPLAIN](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_EXPLAIN.html). Para obtener más información, consulte [Flujo de trabajo de planificación y ejecución de consultas](https://docs.aws.amazon.com/es_es/redshift/latest/dg/c-query-planning.html).\n",
    "\n",
    "Para definir un tipo de ordenación, utilice la palabra clave INTERLEAVED o COMPOUND en su instrucción CREATE TABLE o CREATE TABLE AS. La opción predeterminada es COMPOUND. Se recomienda el COMPOUND cuando actualice las tablas a menudo con operaciones INSERT, UPDATE o DELETE. Una clave de ordenación INTERLEAVED puede usar como máximo ocho columnas. En función de los datos y del tamaño del clúster, VACUUM REINDEX necesita mucho más tiempo que VACUUM FULL ya que realiza una ejecución adicional para analizar las claves de ordenación intercaladas. La operación de clasificación y fusión tarda más tiempo para tablas intercaladas porque es posible que la clasificación intercalada tenga que reorganizar más filas que una clasificación compuesta.\n",
    "\n",
    "Para ver las claves de ordenación de una tabla, consulte la vista de sistema [SVV_TABLE_INFO](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_TABLE_INFO.html).\n",
    "\n",
    "![](img_83.png)\n",
    "\n",
    "La \"Simple\" --> Es una llave COMPUOND pero por una sola columna.\n",
    "\n",
    "#### Temas\n",
    "\n",
    "Clave de ordenación compuesta (COMPOUND)\n",
    "Clave de ordenación intercalada (INTERLEAVED)\n",
    "\n",
    "### <a name=\"mark_15.0\"></a>Clave de ordenación compuesta (COMPOUND)\n",
    "[Index](#index_01)\n",
    "\n",
    "![](img_84.png)\n",
    "\n",
    "- Llave primaria --> \"region\".\n",
    "\n",
    "- Llave secundaria --> \"ciudad\", \"segmento\", \"fecha_creacion\"\n",
    "\n",
    "- Funciona muy bien si vamos a filtrar por todas las columnas compuestas.\n",
    "\n",
    "- Muy mal si solo filtro por ciertas columnas del arreglo.\n",
    "\n",
    "- Peor aún peor si filtro solo por columnas secundarias (ejemplo por una sola \"ciudad\").\n",
    "\n",
    "Una clave compuesta está formada por todas las columnas presentes en la definición de la clave de ordenación, en el orden que aparecen. **Una clave de ordenación compuesta es más útil cuando el filtro de una consulta aplica condiciones, como filtros y combinaciones, que usan un prefijo de clave de ordenación. Los beneficios del rendimiento de la ordenación compuesta se reducen cuando las consultas dependen solo de las columnas de ordenación secundarias, sin hacer referencia a las columnas principales. COMPOUND es el tipo de ordenación predeterminado.**\n",
    "\n",
    "Las claves de ordenación compuesta pueden acelerar las combinaciones, las operaciones GROUP BY y ORDER BY y las funciones de ventana que usan PARTITION BY y ORDER BY. Por ejemplo, una combinación de fusión, que suele ser más rápida que una combinación hash, es factible cuando los datos se distribuyen y se ordenan previamente en las columnas de combinación. Las claves de ordenación compuesta también ayudan a mejorar la compresión.\n",
    "\n",
    "A medida que agrega filas a una tabla ordenada que ya tiene datos, la región desordenada aumenta y esto tiene un efecto significativo en el rendimiento. El efecto es mayor cuando la tabla usa una ordenación intercalada, en especial cuando las columnas de ordenación incluyen datos que aumentan de forma monótona, como las columnas de fecha o de marca temporal. Ejecute una operación VACUUM de manera periódica, sobre todo después de grandes cargas de datos, para volver a clasificar y a analizar los datos. Para obtener más información, consulte [Administración del tamaño de la región no ordenada](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_vacuum_diskspacereqs.html). Después hacer la limpieza para volver a ordenar los datos, se recomienda ejecutar un comando ANALYZE para actualizar los metadatos estadísticos para el planificador de consultas. Para obtener más información, consulte [Análisis de tablas](https://docs.aws.amazon.com/es_es/redshift/latest/dg/t_Analyzing_tables.html).\n",
    "\n",
    "### <a name=\"mark_15.1\"></a>Clave de ordenación intercalada (INTERLEAVED)\n",
    "[Index](#index_01)\n",
    "\n",
    "![](img_85.png)\n",
    "\n",
    "![](img_86.png)\n",
    "\n",
    "- Para este caso, la llave secundaria \"ciudad\" tiene la misma prioridad que la llave primaria \"region\"\n",
    "\n",
    "- Si queremos optimizar cargas, no utilizar llaves intercaladas (INTERLEAVED)\n",
    "\n",
    "Una clave de ordenación intercalada le otorga el mismo peso a cada columna o subconjunto de columnas en la clave de ordenación. Si hay distintas consultas que usan diferentes columnas para filtros, puede, por lo general, mejorar el rendimiento de esas consultas utilizando un estilo de ordenación intercalada. Cuando una consulta usa predicados restrictivos en las columnas de ordenación secundarias, la ordenación intercalada mejora considerablemente el rendimiento de la consulta, si se la compara con la ordenación compuesta.\n",
    "\n",
    "#### Importante\n",
    "No utilice una clave de ordenación intercalada con atributos monótonamente crecientes, como columnas de identidad, fechas o marcas temporales.\n",
    "\n",
    "Las mejoras de rendimiento que obtiene al implementar una clave de ordenación intercalada se deben compensar con el aumento de los tiempos de carga y de limpieza.\n",
    "\n",
    "Las ordenaciones intercaladas son más eficaces con las consultas sumamente selectivas que filtran una o más columnas con clave de ordenación en la cláusula WHERE; por ejemplo, la consulta select c_name from customer where c_region = 'ASIA'. Los beneficios de la ordenación intercalada aumentan con la cantidad de columnas ordenadas que están limitadas.\n",
    "\n",
    "La ordenación intercalada es más eficaz con las tablas grandes. La ordenación se aplica a cada sector. Por lo tanto, una ordenación intercalada resulta más eficaz si una tabla es lo suficientemente grande como para requerir varios bloques de 1 MB por sector. En este caso, el procesador de consultas puede omitir una proporción significativa de los bloques mediante el uso de predicados restrictivos. Para ver la cantidad de bloques que utiliza una tabla, consulte la vista de sistema [STV_BLOCKLIST](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STV_BLOCKLIST.html).\n",
    "\n",
    "Cuando se ordena una sola columna, una ordenación intercalada puede tener un mejor rendimiento que una ordenación compuesta si los valores de la columna tienen un prefijo largo en común. Por ejemplo, los URL, por lo general, comienzan con \"http://www\". Las claves de ordenación compuesta usan una cantidad limitada de caracteres del prefijo, lo que genera muchas duplicaciones de claves. Las ordenaciones intercaladas utilizan un esquema de compresión interno para los valores de mapas de zonas que les permite discriminar mejor entre valores de columnas que tienen un prefijo largo en común.\n",
    "\n",
    "Al migrar clústeres aprovisionados de Amazon Redshift a Amazon Redshift sin servidor, Redshift convierte tablas con claves de clasificación intercaladas y DISTSTYLE KEY en claves de clasificación compuestas. El DISTSTYLE no cambia. Para obtener más información sobre los estilos de distribución, consulte [Uso de estilos de distribución de datos](https://docs.aws.amazon.com/es_es/redshift/latest/dg/t_Distributing_data.html).\n",
    "\n",
    "### VACUUM REINDEX\n",
    "\n",
    "A medida que agrega filas a una tabla ordenada que ya tiene datos, el rendimiento puede deteriorarse con el tiempo. Este deterioro ocurre tanto en las ordenaciones compuestas como en las intercaladas, pero tiene mayor efecto en las tablas intercaladas. El comando VACUUM restaura el orden, pero la operación puede tomar más tiempo para las tablas intercaladas porque fusionar datos nuevos intercalados puede implicar modificar cada bloque de datos.\n",
    "\n",
    "Cuando las tablas se cargan por primera vez, Amazon Redshift analiza la distribución de los valores de las columnas con clave de ordenación y usa la información para intercalar de manera óptima las columnas con clave de ordenación. A medida que una tabla aumenta, la distribución de los valores de las columnas con clave de ordenación puede variar o generar un sesgo, en especial con las columnas de fecha o de marca temporal. Si el sesgo es demasiado grande, puede afectar el rendimiento. Para volver a analizar las claves de ordenación y restaurar el rendimiento, ejecute el comando VACUUM con la palabra clave REINDEX. Como el comando VACUUM REINDEX debe hacer un análisis adicional sobre los datos, puede tardar más que un comando VACUUM estándar para las tablas intercaladas. Para ver información acerca del sesgo de distribución de claves y del tiempo de la última reindexación, consulte la vista de sistema [SVV_INTERLEAVED_COLUMNS](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVV_INTERLEAVED_COLUMNS.html).\n",
    "\n",
    "Para obtener más información acerca de cómo determinar con qué frecuencia ejecutar el comando VACUUM y cuándo ejecutar el comando VACUUM REINDEX, consulte[Decidir si reindexar](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_vacuum-decide-whether-to-reindex.html).\n",
    "\n",
    "### Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/t_Sorting_data.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/t_Creating_tables.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_CREATE_TABLE_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6f7fe",
   "metadata": {},
   "source": [
    "### <a name=\"mark_16\"></a>Evaluando algoritmos de ordenamiento\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Para realizar este análisis se realizaron previamente algunas cuestiones, para generar el contexto de la prueba:\n",
    "\n",
    "- Creación de una tabla llamada \"cust_sales_date\" vacía, sin ningún sort, diststyle, o compresión.\n",
    "- Migración/populación utilizando \"copy\" y el archivo comprimido \"cust_sales_date.bz2\" del [repo](https://github.com/alarcon7a/redshift_course/tree/master/Claves_de_ordenamiento), observar que solo basta cargar el archivo comprimido en S3 y realizar un copy desde el bucket correspondiente.\n",
    "    3. \n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE public.cust_sales_date (\n",
    "\tc_custkey int4 NULL,\n",
    "\tc_nation varchar(15) NULL,\n",
    "\tc_region varchar(12) NULL,\n",
    "\tc_mktsegment varchar(10) NULL,\n",
    "\td_date date NULL,\n",
    "\tlo_revenue int4 NULL\n",
    ");\n",
    "\n",
    "copy cust_sales_date from 's3://mybucketredshift/cust_sales_date.bz2' \n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXX:role/MiRoleRedshift' \n",
    "BZIP2\n",
    "region 'us-east-2';\n",
    "```\n",
    "Checkeamos la cantidad de registros insertados \"10 millones de registros\".\n",
    "\n",
    "```sql\n",
    "SELECT COUNT(0) FROM cust_sales_date; \n",
    "SELECT * FROM  cust_sales_date LIMIT 10;\n",
    "```\n",
    "\n",
    "![](img_87.png)\n",
    "\n",
    "![](img_88.png)\n",
    "\n",
    "\n",
    "- Creación de tabla \"auxiliar\" que tiene 1 columna y 5 filas, el objetivo de esta tabla es solo para hacer un \"producto cartesiano\" con la tabla \"cust_sales_date\" y llevar los registros de 10 millones a 50 millones.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE auxiliar (col int);\n",
    "INSERT INTO auxiliar VALUES (1), (2), (3), (4), (5);\n",
    "SELECT * FROM auxiliar;\n",
    "\n",
    "```\n",
    "![](img_89.png)\n",
    "\n",
    "\n",
    "- Ahora crearemos y poblaremos (cada tabla con una misma sentencia SELECT) las 3 tablas con sus diferentes sortkey (simple/automatico, compound, y interleaved) para el análisis de su comportamiento.\n",
    "\n",
    "```sql\n",
    "\n",
    "-- SIMPLE\n",
    "CREATE TABLE cust_sales_simple\n",
    "sortkey (c_custkey)\n",
    "AS (\n",
    "\tSELECT c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue\n",
    "\tFROM cust_sales_date, auxiliar\n",
    "\t);\n",
    "\n",
    "-- COMPOUND\n",
    "CREATE TABLE cust_sales_compuesto\n",
    "compound sortkey (c_custkey, c_region, c_mktsegment, d_date)\n",
    "AS (\n",
    "\tSELECT c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue\n",
    "\tFROM cust_sales_date, auxiliar\n",
    "\t);\n",
    "\n",
    "-- INTERLEAVED\n",
    "CREATE TABLE cust_sales_intercalado\n",
    "interleaved sortkey (c_custkey, c_region, c_mktsegment, d_date)\n",
    "AS (\n",
    "\tSELECT c_custkey, c_nation, c_region, c_mktsegment, d_date, lo_revenue\n",
    "\tFROM cust_sales_date, auxiliar\n",
    "\t);\n",
    "    \n",
    "SELECT count(0) FROM cust_sales_simple;\t\n",
    "SELECT count(0) FROM cust_sales_compuesto;\n",
    "SELECT count(0) FROM cust_sales_intercalado;\n",
    "\n",
    "```\n",
    "\n",
    "En este punto cada tabla con exactamente los mismos datos se encuentra con sus respectivas llaves de ordenamiento, procedemos a realizar la misma consulta para cada tabla y evaluar el tiempo de ejecución, ingresando por su llave primaria \"c_custkey\".\n",
    "\n",
    "### <a name=\"test_01\"></a>`*********************************PRIMER PRUEBA********************************************`\n",
    "[Index](#index_01)\n",
    "\n",
    "### Simple primer análisis prueba 01:\n",
    "\n",
    "```sql\n",
    "\n",
    "SELECT max(lo_revenue), min(lo_revenue) \n",
    "FROM cust_sales_simple\n",
    "WHERE c_custkey < 100000;\n",
    "```\n",
    "\n",
    "Tiempo de ejecución --> **4.226s**\n",
    "\n",
    "![](img_90.png)\n",
    "\n",
    "### Compound primer análisis prueba 01:\n",
    "\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue) \n",
    "FROM cust_sales_compuesto\n",
    "WHERE c_custkey < 100000;\n",
    "```\n",
    "\n",
    "Tiempo de ejecución --> **118ms**\n",
    "\n",
    "![](img_91.png)\n",
    "\n",
    "### Interleaved primer análisis prueba 01:\n",
    "\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue) \n",
    "FROM cust_sales_intercalado\n",
    "WHERE c_custkey < 100000;\n",
    "\n",
    "```\n",
    "\n",
    "Tiempo de ejecución --> **3.690s**\n",
    "\n",
    "![](img_92.png)\n",
    "\n",
    "Hasta el momento siendo estas las primeras ejecuciones que tiene Redshift sobre las tablas la mejor performance la tiene \"Compound\", para evitar el sesgo que el guardado en cache nos ocacionarían para repetidas consultar de una misma tabla, realizamos el apagado de cache y medimos nuevamente las 3 consultas.\n",
    "\n",
    "```sql\n",
    "\n",
    "set enable_result_cache_for_session to off;\n",
    "\n",
    "```\n",
    "\n",
    "### Simple primer análisis prueba 02:\n",
    "\n",
    "![](img_93.png)\n",
    "\n",
    "Tiempo de ejecución --> **189ms**\n",
    "\n",
    "### Compound primer análisis prueba 02:\n",
    "\n",
    "![](img_94.png)\n",
    "\n",
    "Tiempo de ejecución --> **123ms**\n",
    "\n",
    "### Interleaved primer análisis prueba 02:\n",
    "\n",
    "![](img_95.png)\n",
    "\n",
    "Tiempo de ejecución --> **178ms**\n",
    "\n",
    "#### Tabla de comparación\n",
    "\n",
    "|simple|compound|interleaved|comments|\n",
    "|:---:|:---:|:---:|:---|\n",
    "|4.226s|118ms|3.690s|prueba 01|\n",
    "|189ms|123ms|178ms|prueba 02|\n",
    "\n",
    "Como podemos ver \"compound\", sigue siendo, en este contexto, la mejor, provemos con diferentes consultas.\n",
    "\n",
    "### <a name=\"test_02\"></a>`*********************************SEGUNDA PRUEBA********************************************`\n",
    "[Index](#index_01)\n",
    "\n",
    "### Simple segundo análisis prueba 01\n",
    "```sql\n",
    "\n",
    "SELECT max(lo_revenue), min(lo_revenue) \n",
    "FROM cust_sales_simple\n",
    "WHERE c_region = 'ASIA'\n",
    "AND c_mktsegment = 'FURNITURE'; \n",
    "```\n",
    "![](img_96.png)\n",
    "\n",
    "Tiempo de ejecución --> **4.374s**\n",
    "\n",
    "\n",
    "### Compound segundo análisis prueba 01\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue) \n",
    "FROM cust_sales_compuesto\n",
    "WHERE c_region = 'ASIA'\n",
    "AND c_mktsegment = 'FURNITURE';\n",
    "```\n",
    "\n",
    "![](img_97.png)\n",
    "\n",
    "Tiempo de ejecución --> **4.450s**\n",
    "\n",
    "### Interleaved segundo análisis prueba 01\n",
    "\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue) \n",
    "FROM cust_sales_intercalado\n",
    "WHERE c_region = 'ASIA'\n",
    "AND c_mktsegment = 'FURNITURE';\n",
    "\n",
    "```\n",
    "\n",
    "![](img_98.png)\n",
    "\n",
    "Tiempo de ejecución --> **3.856s**\n",
    "\n",
    "Apagamos nuevamente el cache y repetimos.\n",
    "\n",
    "```sql\n",
    "\n",
    "set enable_result_cache_for_session to off;\n",
    "\n",
    "```\n",
    "\n",
    "### Simple segundo análisis prueba 02:\n",
    "\n",
    "![](img_99.png)\n",
    "\n",
    "Tiempo de ejecución --> **612ms**\n",
    "\n",
    "### Compound segundo análisis prueba 02:\n",
    "\n",
    "![](img_100.png)\n",
    "\n",
    "Tiempo de ejecución --> **361ms**\n",
    "\n",
    "### Interleaved segundo análisis prueba 02:\n",
    "\n",
    "![](img_101.png)\n",
    "\n",
    "Tiempo de ejecución --> **164ms**\n",
    "\n",
    "#### Tabla de comparación\n",
    "\n",
    "|simple|compound|interleaved|comments|\n",
    "|:---:|:---:|:---:|:---|\n",
    "|4.374s|4.450s|3.856s|prueba 01|\n",
    "|612ms|361ms|164ms|prueba 02|\n",
    "\n",
    "El ordenamiento intercalado, para este caso, al tener la misma prioridad para cada llave es más eficiente en la búsqueda.\n",
    "\n",
    "### <a name=\"test_03\"></a>`*********************************TERCER PRUEBA*********************************`\n",
    "[Index](#index_01)\n",
    "\n",
    "### Simple tercer análisis prueba 01\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue)\n",
    "FROM cust_sales_simple\n",
    "WHERE d_date BETWEEN '01/01/1996' AND '01/14/1996'\n",
    "AND c_mktsegment = 'FURNITURE'\n",
    "AND c_region = 'ASIA'; \n",
    "```\n",
    "![](img_102.png)\n",
    "\n",
    "Tiempo de ejecución --> **5.44s**\n",
    "\n",
    "\n",
    "### Compound tercer análisis prueba 01\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue)\n",
    "FROM cust_sales_compuesto\n",
    "WHERE d_date BETWEEN '01/01/1996' AND '01/14/1996'\n",
    "AND c_mktsegment = 'FURNITURE'\n",
    "AND c_region = 'ASIA';\n",
    "```\n",
    "\n",
    "![](img_103.png)\n",
    "\n",
    "Tiempo de ejecución --> **5.395s**\n",
    "\n",
    "### Interleaved tercer análisis prueba 01\n",
    "\n",
    "```sql\n",
    "SELECT max(lo_revenue), min(lo_revenue)\n",
    "FROM cust_sales_intercalado\n",
    "WHERE d_date BETWEEN '01/01/1996' AND '01/14/1996'\n",
    "AND c_mktsegment = 'FURNITURE'\n",
    "AND c_region = 'ASIA';\n",
    "```\n",
    "\n",
    "![](img_104.png)\n",
    "\n",
    "Tiempo de ejecución --> **5.499s**\n",
    "\n",
    "Apagamos nuevamente el cache y repetimos.\n",
    "\n",
    "```sql\n",
    "\n",
    "set enable_result_cache_for_session to off;\n",
    "\n",
    "```\n",
    "\n",
    "### Simple tercer análisis prueba 02:\n",
    "\n",
    "![](img_105.png)\n",
    "\n",
    "Tiempo de ejecución --> **680ms**\n",
    "\n",
    "### Compound tercer análisis prueba 02:\n",
    "\n",
    "![](img_106.png)\n",
    "\n",
    "Tiempo de ejecución --> **432ms**\n",
    "\n",
    "### Interleaved tercer análisis prueba 02:\n",
    "\n",
    "![](img_107.png)\n",
    "\n",
    "Tiempo de ejecución --> **143ms**\n",
    "\n",
    "#### Tabla de comparación\n",
    "\n",
    "|simple|compound|interleaved|comments|\n",
    "|:---:|:---:|:---:|:---|\n",
    "|5.44s|5.395s|5.499s|Prueba 01|\n",
    "|680ms|432ms|143ms|Prueba 02|\n",
    "\n",
    "El ordenamiento intercalado, para este caso, al tener la misma prioridad para cada llave es más eficiente en la búsqueda.\n",
    "\n",
    "### <a name=\"conclusion_01\"></a>Conclusiones:\n",
    "\n",
    "### [Primer prueba](#test_01)\n",
    "\n",
    "#### Utilizando la llave primaria para las consultas \"COMPOUND\" fue más eficiente.\n",
    "\n",
    "|simple|compound|interleaved|comments|\n",
    "|:---:|:---:|:---:|:---|\n",
    "|4.226s|118ms|3.690s|prueba 01|\n",
    "|189ms|123ms|178ms|prueba 02|\n",
    "\n",
    "### [Segunda prueba](#test_02)\n",
    "\n",
    "#### Utilizando distintans llaves secundarias, \"INTERLEAVED\" fue más eficiente.\n",
    "\n",
    "|simple|compound|interleaved|comments|\n",
    "|:---:|:---:|:---:|:---|\n",
    "|4.374s|4.450s|3.856s|prueba 01|\n",
    "|612ms|361ms|164ms|prueba 02|\n",
    "\n",
    "### [Tercer prueba](#test_03)\n",
    "\n",
    "#### Utilizando distintans llaves secundarias + un contexto limitado de fechas, \"INTERLEAVED\" fue más eficiente.\n",
    "\n",
    "|simple|compound|interleaved|comments|\n",
    "|:---:|:---:|:---:|:---|\n",
    "|5.44s|5.395s|5.499s|Prueba 01|\n",
    "|680ms|432ms|143ms|Prueba 02|\n",
    "\n",
    "Si bien una tabla con ordenamiento intercalado tarda más en ser populada y su mantenimiento es un extra administrativo que debemos tener en cuenta, al momento de las consultas es mucho más eficiente, en 2 de las 3 consultas, como lo demuestran los análisis en cada contexto.\n",
    "\n",
    "### Observación:\n",
    "\n",
    "Explicando el porque de las diferencias temporales entre la Prueba_01 y la Prueba_02.\n",
    "\n",
    "En la primer ejecución de cada prueba, el cache no estaba apagado, entonces Redshift analiza primero si tiene ya ese resultado y lo intenta mostrar, por lo cual es más lento. Pero en cada segunda prueba este cache se apago entonces simplemente ejecuta la centencia directamente, por eso para el ejercicio es más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea740f5",
   "metadata": {},
   "source": [
    "### <a name=\"mark_17\"></a> Buenas prácticas para diseñar tablas en Redshift\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "Si bien los constrains no son parte esencias de una db (datalake/datawarehouse/datalakehouse) para analítica, en Redshift si podemos crear esta integridad será mucho mejor, ya que la velocidad de consulta será mayor.\n",
    "\n",
    "![](img_108.png)\n",
    "\n",
    "- Siempre utiliza o transforma las fechas a un tipo de formato.\n",
    "- En lo posible limitar la longitud de los campos (varchar) al número más exactamente necesario, esto mejora los algoritmos de compresión en Redshift.\n",
    "- Crear comentarios explicativos sobre lo que sea necesario.\n",
    "\n",
    "![](img_109.png)\n",
    "\n",
    "La siguiente query es una query que se encontraba en la Documentación (actualmente no es posible encontrarla como \"análisis del diseño de tablas\"). Que muestra las siguientes columnas:\n",
    "\n",
    "_ schemaname\n",
    "\n",
    "_ tablename\n",
    "\n",
    "_ tableid\n",
    "\n",
    "_ size_in_mb\n",
    "\n",
    "_ has_disk_key --> tiene o no una llave de distribución\n",
    "\n",
    "_ has_sort_key --> tiene o no una llave de ordenamiento\n",
    "\n",
    "_ has_col_encoding --> tiene o no columnas comprimidas\n",
    "\n",
    "_ ratio_skew_across_slices --> indica si hay un sesgo en la distribución de los datos (un valor bajo significa algo bueno)\n",
    "\n",
    "_ pct_slices_populated --> muestra en % cuán lleno de datos están los slices en una tabla (un valor del 100% es bueno)\n",
    "```sql\n",
    "\n",
    "select\n",
    "\tschema schemaname,\n",
    "\t\"table\" tablename,\n",
    "\ttable_id tableid,\n",
    "\tsize size_in_mb,\n",
    "\tcase\n",
    "\t\twhen diststyle not in ('EVEN', 'ALL') then 1\n",
    "\t\telse 0\n",
    "\tend has_dist_key,\n",
    "\tcase\n",
    "\t\twhen sortkey1 is not null then 1\n",
    "\t\telse 0\n",
    "\tend has_sort_key,\n",
    "\tcase\n",
    "\t\twhen encoded = 'Y' then 1\n",
    "\t\telse 0\n",
    "\tend has_col_encoding,\n",
    "\tcast(max_blocks_per_slice - min_blocks_per_slice as FLOAT) / greatest(NVL (min_blocks_per_slice, 0)::int, 1) ratio_skew_across_slices,\n",
    "\tcast(100 * dist_slice as FLOAT) /(\n",
    "\tselect\n",
    "\t\tCOUNT(distinct slice)\n",
    "\tfrom\n",
    "\t\tstv_slices) pct_slices_populated\n",
    "from\n",
    "\tsvv_table_info ti\n",
    "join (\n",
    "\tselect\n",
    "\t\ttbl, MIN(c) min_blocks_per_slice, MAX(c) max_blocks_per_slice, COUNT(distinct slice) dist_slice\n",
    "\tfrom\n",
    "\t\t(\n",
    "\t\tselect\n",
    "\t\t\tb.tbl, b.slice, COUNT(*) as c\n",
    "\t\tfrom\n",
    "\t\t\tSTV_BLOCKLIST b\n",
    "\t\tgroup by\n",
    "\t\t\tb.tbl, b.slice)\n",
    "\twhere\n",
    "\t\ttbl in (\n",
    "\t\tselectselect\n",
    "\tschema schemaname,\n",
    "\t\"table\" tablename,\n",
    "\ttable_id tableid,\n",
    "\tsize size_in_mb,\n",
    "\tcase\n",
    "\t\twhen diststyle not in ('EVEN', 'ALL') then 1\n",
    "\t\telse 0\n",
    "\tend has_dist_key,\n",
    "\tcase\n",
    "\t\twhen sortkey1 is not null then 1\n",
    "\t\telse 0\n",
    "\tend has_sort_key,\n",
    "\tcase\n",
    "\t\twhen encoded = 'Y' then 1\n",
    "\t\telse 0\n",
    "\tend has_col_encoding,\n",
    "\tcast(max_blocks_per_slice - min_blocks_per_slice as FLOAT) / greatest(NVL (min_blocks_per_slice, 0)::int, 1) ratio_skew_across_slices,\n",
    "\tcast(100 * dist_slice as FLOAT) /(\n",
    "\tselect\n",
    "\t\tCOUNT(distinct slice)\n",
    "\tfrom\n",
    "\t\tstv_slices) pct_slices_populated\n",
    "from\n",
    "\tsvv_table_info ti\n",
    "join (\n",
    "\tselect\n",
    "\t\ttbl, MIN(c) min_blocks_per_slice, MAX(c) max_blocks_per_slice, COUNT(distinct slice) dist_slice\n",
    "\tfrom\n",
    "\t\t(\n",
    "\t\tselect\n",
    "\t\t\tb.tbl, b.slice, COUNT(*) as c\n",
    "\t\tfrom\n",
    "\t\t\tSTV_BLOCKLIST b\n",
    "\t\tgroup by\n",
    "\t\t\tb.tbl, b.slice)\n",
    "\twhere\n",
    "\t\ttbl in (\n",
    "\t\tselect\n",
    "\t\t\ttable_id\n",
    "\t\tfrom\n",
    "\t\t\tsvv_table_info)\n",
    "\tgroup by\n",
    "\t\ttbl) iq on\n",
    "\tiq.tbl = ti.table_id;\n",
    "\t\t\ttable_id\n",
    "\t\tfrom\n",
    "\t\t\tsvv_table_info)\n",
    "\tgroup by\n",
    "\t\ttbl) iq on\n",
    "\tiq.tbl = ti.table_id;\n",
    "\n",
    "```\n",
    "\n",
    "### Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_analyzing-table-design.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db006dd",
   "metadata": {},
   "source": [
    "### <a name=\"mark_18\"></a>Tipos de datos en AWS Redshift\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "En esta clase conoceremos los tipos de datos que maneja Redshift, y como ya lo sabes, Redshift está basado en PostgreSQL, de manera que si estás familiarizado con los tipos de datos de PostgreSQL esta clase se te facilitará.\n",
    "\n",
    "Como otras bases de datos, Redshift maneja datos de tipo numérico, texto, de lógica booleana y estos datos tienen ciertas particularidades.\n",
    "\n",
    "### Tipos de datos para texto:\n",
    "\n",
    "En estos tipos de datos almacenaremos caracteres y cadenas de texto como pueden ser nombres, direcciones, descripciones, etc.\n",
    "\n",
    "Lo más importante que debes saber al momento de manejar estos tipos de datos es que el parámetro que le indiques a cada dato no es la longitud, es el número de bytes que trabajará, de manera que un dato VARCHAR(2) no indica que reciba 2 caracteres de longitud, sino que el contenido es de 2 bytes.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE  TABLE  test_tipo_dato (\n",
    "dato  varchar(2)\n",
    ")\n",
    "\n",
    "insert  into  test_tipo_dato  values ('AA');\n",
    "insert  into  test_tipo_dato  values ('€');\n",
    "\n",
    "```\n",
    "\n",
    "Si ejecutas las anteriores sentencias notarás que el primer insert se ejecuta sin ningún problema, “A” es un carácter que solo ocupa 1 byte. De modo que “AA” ocupa un total de 2 bytes. Pero, la segunda sentencia de insert falló y esta solo contiene un carácter “€”, esto ocurre por que este carácter tiene una longitud de 3 bytes por sí solo, de manera que rompe la restricción de tipo de dato VARCHAR(2).\n",
    "\n",
    "![](img_110.png)\n",
    "\n",
    "Como ves los tipos de dato BPCHAR son convertidos por Redshift a CHAR y los tipos de dato TEXT a VARCHAR; también es importante tener en cuenta que un dato CHAR de 10, siempre ocupara 10 bytes, y dado el caso que el dato guardado no complete los 10 bytes Redshift lo completa con espacios en blanco, esto no ocurre en el tipo de dato VARCHAR.\n",
    "\n",
    "Compruébalo tú mismo:\n",
    "\n",
    "```sql\n",
    "\n",
    "create  table  test_char (\n",
    "dato  char(10)\n",
    ");\n",
    "\n",
    "insert  into  test_char  values ('dato');\n",
    "select * from  test_char;\n",
    "\n",
    "```\n",
    "\n",
    "### Tipos de datos numéricos:\n",
    "\n",
    "Estos tipos de datos se deben usar para el manejo de números, en casos como cantidades, medidas, llaves únicas etc, los datos numéricos pueden ser enteros, decimales y de coma flotante.\n",
    "\n",
    "### Enteros:\n",
    "\n",
    "![](img_111.png)\n",
    "\n",
    "### Decimales:\n",
    "\n",
    "![](img_112.png)\n",
    "\n",
    "Para el manejo de datos decimales, decimal(precisión, escala), debes tener en cuenta que se manejan dos parámetros, la precisión y la escala.\n",
    "\n",
    "Para explicarte como funciona usemos un ejemplo:\n",
    "\n",
    "```sql\n",
    "\n",
    "create  table  test_decimal (\n",
    "dato  decimal(4,2)\n",
    ");\n",
    "\n",
    "insert  into  test_decimal  values (4581.54);\n",
    "insert  into  test_decimal  values (121.7);\n",
    "insert  into  test_decimal  values (12.21);\n",
    "insert  into  test_decimal  values (12.2);\n",
    "insert  into  test_decimal  values (12.21222);\n",
    "\n",
    "```\n",
    "\n",
    "Como puedes observar los dos primeros “insert” fallaron, y quizá te estés preguntando, ¿por qué ocurrió esto? si en la sentencia dice 4 enteros y 2 decimales… pero no, el valor de precisión (4 para este ejemplo) indica el número total de dígitos que puede recibir, y escala indica el número de decimales que se trabajarán, de modo que en realidad este dato puede máximo guardar un número de 2 enteros y 2 decimales (4,2), pero ¿y la última sentencia insert que tenía 2 enteros y 5 decimales? ¿por qué funcioné?, bueno Redshift nota que el número de enteros es válido y solo redondea las cifras decimales a 2 dígitos para este caso.\n",
    "\n",
    "### Otros tipos de dato:\n",
    "\n",
    "Los valores de fecha como date, timestamp, timestampz se manejan como cualquier otra base de datos, también los tipos de dato bool, boolean, los cuales solo pueden recibir un byte de longitud.\n",
    "\n",
    "A continuación te comparto la tabla que usa Redshift para el manejo de tipos de datos.\n",
    "\n",
    "![](img_113.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580be27a",
   "metadata": {},
   "source": [
    "### <a name=\"mark_19\"></a>Reto: mejora el desempeño de tu base de datos\n",
    "\n",
    "### [Index](#index_01)\n",
    "\n",
    "En linea con el diseño \"estrella\", Creamos las tablas...\n",
    "\n",
    "_ part (dimensión)\n",
    "\n",
    "_ supplier (dimensión)\n",
    "\n",
    "_ customer (dimensión)\n",
    "\n",
    "_ dwdate (dimensión)\n",
    "\n",
    "_ lineorder(tabla de hechos)\n",
    "\n",
    "Sin ningún tipo de compresión, ordenamiento, distribución, ni constrains.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE part \n",
    "(\n",
    "  p_partkey     INTEGER NOT NULL,\n",
    "  p_name        VARCHAR(22) NOT NULL,\n",
    "  p_mfgr        VARCHAR(6) NOT NULL,\n",
    "  p_category    VARCHAR(7) NOT NULL,\n",
    "  p_brand1      VARCHAR(9) NOT NULL,\n",
    "  p_color       VARCHAR(11) NOT NULL,\n",
    "  p_type        VARCHAR(25) NOT NULL,\n",
    "  p_size        INTEGER NOT NULL,\n",
    "  p_container   VARCHAR(10) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE supplier \n",
    "(\n",
    "  s_suppkey   INTEGER NOT NULL,\n",
    "  s_name      VARCHAR(25) NOT NULL,\n",
    "  s_address   VARCHAR(25) NOT NULL,\n",
    "  s_city      VARCHAR(10) NOT NULL,\n",
    "  s_nation    VARCHAR(15) NOT NULL,\n",
    "  s_region    VARCHAR(12) NOT NULL,\n",
    "  s_phone     VARCHAR(15) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE customer \n",
    "(\n",
    "  c_custkey      INTEGER NOT NULL,\n",
    "  c_name         VARCHAR(25) NOT NULL,\n",
    "  c_address      VARCHAR(25) NOT NULL,\n",
    "  c_city         VARCHAR(10) NOT NULL,\n",
    "  c_nation       VARCHAR(15) NOT NULL,\n",
    "  c_region       VARCHAR(12) NOT NULL,\n",
    "  c_phone        VARCHAR(15) NOT NULL,\n",
    "  c_mktsegment   VARCHAR(10) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dwdate \n",
    "(\n",
    "  d_datekey            INTEGER NOT NULL,\n",
    "  d_date               VARCHAR(19) NOT NULL,\n",
    "  d_dayofweek          VARCHAR(10) NOT NULL,\n",
    "  d_month              VARCHAR(10) NOT NULL,\n",
    "  d_year               INTEGER NOT NULL,\n",
    "  d_yearmonthnum       INTEGER NOT NULL,\n",
    "  d_yearmonth          VARCHAR(8) NOT NULL,\n",
    "  d_daynuminweek       INTEGER NOT NULL,\n",
    "  d_daynuminmonth      INTEGER NOT NULL,\n",
    "  d_daynuminyear       INTEGER NOT NULL,\n",
    "  d_monthnuminyear     INTEGER NOT NULL,\n",
    "  d_weeknuminyear      INTEGER NOT NULL,\n",
    "  d_sellingseason      VARCHAR(13) NOT NULL,\n",
    "  d_lastdayinweekfl    VARCHAR(1) NOT NULL,\n",
    "  d_lastdayinmonthfl   VARCHAR(1) NOT NULL,\n",
    "  d_holidayfl          VARCHAR(1) NOT NULL,\n",
    "  d_weekdayfl          VARCHAR(1) NOT NULL\n",
    ");\n",
    "CREATE TABLE lineorder \n",
    "(\n",
    "  lo_orderkey          INTEGER NOT NULL,\n",
    "  lo_linenumber        INTEGER NOT NULL,\n",
    "  lo_custkey           INTEGER NOT NULL,\n",
    "  lo_partkey           INTEGER NOT NULL,\n",
    "  lo_suppkey           INTEGER NOT NULL,\n",
    "  lo_orderdate         INTEGER NOT NULL,\n",
    "  lo_orderpriority     VARCHAR(15) NOT NULL,\n",
    "  lo_shippriority      VARCHAR(1) NOT NULL,\n",
    "  lo_quantity          INTEGER NOT NULL,\n",
    "  lo_extendedprice     INTEGER NOT NULL,\n",
    "  lo_ordertotalprice   INTEGER NOT NULL,\n",
    "  lo_discount          INTEGER NOT NULL,\n",
    "  lo_revenue           INTEGER NOT NULL,\n",
    "  lo_supplycost        INTEGER NOT NULL,\n",
    "  lo_tax               INTEGER NOT NULL,\n",
    "  lo_commitdate        INTEGER NOT NULL,\n",
    "  lo_shipmode          VARCHAR(10) NOT NULL\n",
    ");\n",
    "\n",
    "```\n",
    "\n",
    "Populando de datos las tablas:\n",
    "\n",
    "- \"awssampledbuswest2\" es una carpeta oculta, y por defecto, en AWS que contiene estos ejemplos\n",
    "\n",
    "- Reemplezar ARN del Role y la región correspondiente.\n",
    "\n",
    "- \"gzip compupdate off\" No quiero comprimir los datos luego de la ingesta.\n",
    "\n",
    "```sql\n",
    "\n",
    "copy customer from 's3://awssampledbuswest2/ssbgz/customer' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>'  \n",
    "gzip compupdate off region 'us-west-2';\n",
    "\n",
    "copy dwdate from 's3://awssampledbuswest2/ssbgz/dwdate' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>'  \n",
    "gzip compupdate off region 'us-west-2';\n",
    "\n",
    "copy lineorder from 's3://awssampledbuswest2/ssbgz/lineorder' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>'  \n",
    "gzip compupdate off region 'us-west-2';\n",
    "\n",
    "copy part from 's3://awssampledbuswest2/ssbgz/part' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>'  \n",
    "gzip compupdate off region 'us-west-2';\n",
    "\n",
    "copy supplier from 's3://awssampledbuswest2/ssbgz/supplier' \n",
    "credentials 'aws_iam_role=<Reemplazar_con_tu_iam_role_arn>'  \n",
    "gzip compupdate off region 'us-west-2';\n",
    "\n",
    "```\n",
    "\n",
    "Chequemos la creación de la tabla \"customer\"\n",
    "\n",
    "![](img_114.png)\n",
    "\n",
    "Analizamos la recomendación de compresión/encoding utilizando `analyze compression nombre_tabla`\n",
    "\n",
    "![](img_115.png)\n",
    "\n",
    "Aplicamos la recomendación creando una nueva tabla \"customer_pro\", notar que colocando la distribución \"distkey\" en la línea de la columna la declara automaticamente.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS  public.customer_pro \n",
    "(\n",
    "  c_custkey      INTEGER NOT NULL ENCODE az64 distkey,\n",
    "  c_name         VARCHAR(25) NOT NULL ENCODE zstd,\n",
    "  c_address      VARCHAR(25) NOT NULL ENCODE zstd,\n",
    "  c_city         VARCHAR(10) NOT NULL ENCODE bytedict,\n",
    "  c_nation       VARCHAR(15) NOT NULL ENCODE bytedict,\n",
    "  c_region       VARCHAR(12) NOT NULL ENCODE bytedict,\n",
    "  c_phone        VARCHAR(15) NOT NULL ENCODE zstd,\n",
    "  c_mktsegment   VARCHAR(10) NOT NULL ENCODE bytedict\n",
    ");\n",
    "\n",
    "```\n",
    "\n",
    "Poblemos la tabla ...\n",
    "\n",
    "```sql\n",
    "\n",
    "insert into customer_pro (select * from customer);\n",
    "\n",
    "```\n",
    "\n",
    "## Realizar el mismo procedimiento de optimización con las demás tablas y probar la eficacia con las siguientes queries.\n",
    "\n",
    "```sql\n",
    "\n",
    "--Utiliza la siguente sentencia para desactivar la cache de las consultas con esto buscamos no inteferir en la medicion de tiempos de respuesta\n",
    "set enable_result_cache_for_session to off;\n",
    "\n",
    "-- Query 1\n",
    "select sum(lo_extendedprice*lo_discount) as revenue\n",
    "from lineorder, dwdate\n",
    "where lo_orderdate = d_datekey\n",
    "and d_year = 1997\n",
    "and lo_discount between 1 and 3\n",
    "and lo_quantity < 24;\n",
    "\n",
    "-- Query 2\n",
    "select sum(lo_revenue), d_year, p_brand1\n",
    "from lineorder, dwdate, part, supplier\n",
    "where lo_orderdate = d_datekey\n",
    "and lo_partkey = p_partkey\n",
    "and lo_suppkey = s_suppkey\n",
    "and p_category = 'MFGR#12'\n",
    "and s_region = 'AMERICA'\n",
    "group by d_year, p_brand1\n",
    "order by d_year, p_brand1;\n",
    "\n",
    "-- Query 3\n",
    "select c_city, s_city, d_year, sum(lo_revenue) as revenue\n",
    "from customer, lineorder, supplier, dwdate\n",
    "where lo_custkey = c_custkey\n",
    "and lo_suppkey = s_suppkey\n",
    "and lo_orderdate = d_datekey\n",
    "and (c_city='UNITED KI1' or\n",
    "c_city='UNITED KI5')\n",
    "and (s_city='UNITED KI1' or\n",
    "s_city='UNITED KI5')\n",
    "and d_yearmonth = 'Dec1997'\n",
    "group by c_city, s_city, d_year\n",
    "order by d_year asc, revenue desc;  \n",
    "\n",
    "-- Query 4\n",
    "select sum(lo_revenue) , c.c_nation from lineorder l \n",
    "inner join customer c \n",
    "on l.lo_custkey = c.c_custkey \n",
    "where c_region  = 'EUROPE'\n",
    "group by c.c_nation\n",
    "order by 1 desc\n",
    "\n",
    "```\n",
    "\n",
    "### Lectura Recomendada:\n",
    "\n",
    "https://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables-create-test-data.html\n",
    "\n",
    "https://github.com/alarcon7a/redshift_course/blob/master/Reto_Dise%C3%B1o_Tablas/Sentencias.sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda094c2",
   "metadata": {},
   "source": [
    "### <a name=\"mark_20\"></a>Olvídate de los insert, el copy llego para quedarse\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "- Es mejor realizar la carga de multiples archivos en una sola ejecución que realizarlos uno por uno.\n",
    "\n",
    "- Se pueden cargar archivos comprimidos.\n",
    "\n",
    "![](img_116.png)\n",
    "\n",
    "\n",
    "### Que debo tener en cuenta al momento de hacer un COPY.\n",
    "\n",
    "- Otorgar permisos, lo hacemos con el rol de IAM.\n",
    "\n",
    "- Recordar que la codificación original debe estar en UTF-8\n",
    "\n",
    "- Particionar los datos para mejorar la performance de carga.\n",
    "\n",
    "![](img_117.png)\n",
    "\n",
    "![](img_118.png)\n",
    "\n",
    "\n",
    "### Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_COPY.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f205c",
   "metadata": {},
   "source": [
    "### <a name=\"mark_21\"></a>Cargando archivos tipo JSON\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "Ahora veremos cómo llevar nuestros datos a Redshift dado el caso que nuestro origen de datos se encuentre en un formato json.\n",
    "\n",
    "Lo primero que debemos saber es que existen dos formas de cargar los datos en formato json en Redshift, una es la forma automática, en donde cada “key” del archivo json debe hacer match con cada columna que queramos llenar de una tabla en Redshift.\n",
    "\n",
    "Es importante saber que las columnas en Redshift se crean en minúsculas, de manera que las “key” del archivo en json deben estar en minúscula también. La segunda forma es especificando la estructura del archivo para que este pueda ser cargado a la tabla, para ello utilizaremos un archivo extra denominado jsonpaths.\n",
    "\n",
    "### Carga automática:\n",
    "\n",
    "Usando la estructura de tabla que hemos manejado en este curso:\n",
    "\n",
    "```sql\n",
    "--Create table\n",
    "create  table  estudiante\n",
    "( id  int2,\n",
    "nombre  varchar(20),\n",
    "apellido  varchar(20),\n",
    "edad  int2,\n",
    "fecha_ingreso  date );\n",
    "```\n",
    "\n",
    "Crearemos una estructura en Redshift que pueda satisfacer las necesidades de esta tabla, de modo que puede quedar algo así (recuerda que los “key” del archivo se debe llamar igual que en la tabla):\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "\t\"id\": 4544,\n",
    "\t\"nombre\": \"test_1\",\n",
    "\t\"apellido\": \"json_1\",\n",
    "\t\"edad\": 33,\n",
    "\t\"fecha_ingreso\": \"2020-08-01\"\n",
    "}\n",
    "{\n",
    "\t\"id\": 23232,\n",
    "\t\"nombre\": \"test_2\",\n",
    "\t\"apellido\": \"json_2\",\n",
    "\t\"edad\": 22,\n",
    "\t\"fecha_ingreso\": \"2020-08-03\"\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Ahora lo subimos a S3 con el nombre que queramos y procedemos a cargarlo de la siguiente manera:\n",
    "\n",
    "```sql\n",
    "\n",
    "copy  public.estudiante  from  's3://[tu_propia_ruta_del_archivo_json]'  credentials  'aws_iam_role=[tu_iam_role]'\n",
    "format  as  json  'auto'  region  '[tu_region]';\n",
    "\n",
    "```\n",
    "\n",
    "### Carga con jsonpaths:\n",
    "\n",
    "Esta carga consiste en determinar una estructura basada en el archivo json que pueda ser insertada en la tabla, como sabemos en un archivo json podemos tener datos no estructurados, pero Redshift requiere de data estructurada; de manera que lo primero será crear un pequeño ejemplo de un archivo json con datos no estructurada.\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "\t\"id\": 4544,\n",
    "\t\"nombre\": \"test_json3\",\n",
    "\t\"apellido\": \"test_json3\",\n",
    "\t\"edad\": [\n",
    "\t\t24332,\n",
    "\t\t33,\n",
    "\t\t443,\n",
    "\t\t323232,\n",
    "\t\t43434\n",
    "\t],\n",
    "\t\"fechas\": [\n",
    "\t\t{\n",
    "\t\t\t\"id\": 0,\n",
    "\t\t\t\"fecha_ingreso\": \"2015-05-01\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"fecha_ingreso\": \"2016-05-30\"\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Vamos a suponer que de la lista de edades son la segunda posición o sea la posición [1] es la que yo requiero, y para las fechas, la fecha que en verdad me interesa es la contenida en el segundo bloque, para especificar estas reglas crearemos un nuevo archivo con la siguiente estructura y lo nombraremos “jsonpaths”:\n",
    "\n",
    "```json\n",
    "\n",
    "{\n",
    "\t\"jsonpaths\": [\n",
    "\t\t\"$['id']\",\n",
    "\t\t\"$['nombre']\",\n",
    "\t\t\"$['apellido']\",\n",
    "\t\t\"$['edad'][1]\",\n",
    "\t\t\"$['fechas'][1]['fecha_ingreso']\"\n",
    "\t]\n",
    "}\n",
    "\n",
    "```\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "El símbolo $ se utiliza para acceder a los valores de un objeto JSON. En este caso, el objeto JSON se llama \"jsonpaths\" y tiene cinco propiedades: \"id\", \"nombre\", \"apellido\", \"edad\" y \"fechas\".\n",
    "\n",
    "Las cadenas que comienzan con $ representan rutas JSON. Una ruta JSON es una secuencia de nombres de propiedades que se utilizan para acceder a un valor específico en un objeto JSON.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Ahora subimos nuestros dos archivos a S3 y ejecutamos la siguiente sentencia en nuestro editor SQL.\n",
    "\n",
    "```sql\n",
    "\n",
    "copy  public.estudiante  from  's3://[tu_propia_ruta_del_archivo_json]'  credentials  'aws_iam_role=[tu_iam_role]'\n",
    "format  as  json  's3://[tu_propia_ruta_del_archivo_jsonpaths]'  region  'tu_propia_region';\n",
    "\n",
    "```\n",
    "\n",
    "Y como vemos, el registro fue cargado exitosamente.\n",
    "\n",
    "![](img_119.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b5030",
   "metadata": {},
   "source": [
    "### <a name=\"mark_22\"></a>El comando copy a fondo\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "Para este ejemplo utilizaremos 3 archivos csv con diferentes datos.\n",
    "\n",
    "### primer_cargue.csv\n",
    "\n",
    "```csv\n",
    "\n",
    "Id;Nombre;Apellido;Edad;Fecha_registro\n",
    "1;Carlos;Alarcon;27;2020-06-15\n",
    "\n",
    "2;Andres;Perez;45;2019-01-01\n",
    "3;Felipe;Lopez;20;2020-07-03\n",
    "4;  ;Garcia;35;2020-01-03\n",
    "5;;Torres;14;2020-03-07\n",
    "\n",
    "```\n",
    "\n",
    "### segundo_cargue.csv\n",
    "\n",
    "```csv\n",
    "\n",
    "Id;Nombre;Apellido;Edad;Fecha_registro\n",
    "1;Carlos;Alarcon;27;06-05-2020\n",
    "2;Andres;Perez;45;06-05-2020\n",
    "3;Felipe;Lopez;20;06-05-2020\n",
    "\n",
    "```\n",
    "\n",
    "### tercer_cargue.csv\n",
    "\n",
    "```csv\n",
    "\n",
    "1Carlos   Alarcon  2706-05-2020\n",
    "2Andres   Perez    4506-05-2020\n",
    "3Felipe   Lopez    2006-05-2020\n",
    "\n",
    "```\n",
    "\n",
    "Comenzamos a crear las tablas para popular los datos.\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE estudiante (\n",
    "id int2,\n",
    "nombre varchar(20),\n",
    "apellido varchar(20),\n",
    "edad int2,\n",
    "fecha_ingreso date\n",
    ");\n",
    "\n",
    "```\n",
    "Realizamos el primer intento de carga.\n",
    "\n",
    "![](img_120.png)\n",
    "\n",
    "Como vemos la carga falló, y el warning nos indica verificar la tabla \"stl_load_errors\", podemos ver que no hubo delimitador predefinido.\n",
    "\n",
    "![](img_121.png)\n",
    "\n",
    "Realizamos el segundo intento de carga agregando el delimiter ';'.\n",
    "\n",
    "Y como \"Id\" está en el encabezado y no es de tipo numérico surge el error, con lo cual como no necesitamos el encabezado debemos indicarle que realice la carga sin el header.\n",
    "\n",
    "![](img_122.png)\n",
    "\n",
    "Realizamos el tercer intento de carga quitando el header, agregamos \"ignoreheader 1\" --> ignora la linea 1.\n",
    "\n",
    "Como vemos hay una tercer falla que indica la existencia de filas en blanco, con lo cual habrá que indicarle que las ignore también.\n",
    "\n",
    "![](img_123.png)\n",
    "\n",
    "Realizamos el caurto intento de carga agregando indicando \"ignoreblanklines\" y finalmente se carga con exito.\n",
    "\n",
    "```sql\n",
    "\n",
    "copy estudiante FROM 's3://mibucketredshift/primer_cargue.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "region 'us-east-2'\n",
    "delimiter ';'\n",
    "ignoreheader 1\n",
    "ignoreblanklines;\n",
    "```\n",
    "Pero si observamos los datos faltantes no figuran como valores NULL sino que figuran como valores faltantes o blank (en blanco), esto no es conveniente, con lo cual indicaremos que rellene los valores blank con NULL.\n",
    "\n",
    "![](img_124.png)\n",
    "\n",
    "Primero limpiamos la tabla conservando el schema realizando un `TRUNCATE TABLE nombre_tabla` y agregamos el parámetro `blanksasnull`.\n",
    "\n",
    "```sql \n",
    "TRUNCATE TABLE estudiante;\n",
    "\n",
    "copy estudiante FROM 's3://mibucketredshift/primer_cargue.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "region 'us-east-2'\n",
    "delimiter ';'\n",
    "ignoreheader 1\n",
    "ignoreblanklines\n",
    "blanksasnull;\n",
    "```\n",
    "Como podemos ver los valores faltanes se encuentran como NULL.\n",
    "\n",
    "![](img_125.png)\n",
    "\n",
    "Ahora con la misma sintaxis intentemos cargar el segundo archivo \"sugundo_cargue.csv\", el cual contiene un formato de fecha diferente, y como vemos a continuación la carga falla por este motivo.\n",
    "\n",
    "![](img_126.png)\n",
    "\n",
    "Procedemos a indicarle cual es el formato de fecha utilizado en el archivo de origen (segundo_cargue.csv), agregamos `dateformat = 'mm-dd-yyyy'`, con está instrucción, como vemos a continuación el toma el formato original y lo transforma a yyyy-mm-dd que es el tipo de formato \"date\" con el cual creamos la tabla.\n",
    "\n",
    "```sql\n",
    "copy estudiante FROM 's3://mibucketredshift/segundo_cargue.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "region 'us-east-2'\n",
    "delimiter ';'\n",
    "ignoreheader 1\n",
    "ignoreblanklines\n",
    "blanksasnull\n",
    "dateformat 'mm-dd-yyyy';\n",
    "```\n",
    "\n",
    "![](img_127.png)\n",
    "\n",
    "### <a name=\"mark_22.0\"></a> STL_LOAD_ERRORS\n",
    "[Index](#index_02)\n",
    "\n",
    "Muestra los registros de todos los errores de carga de Amazon Redshift.\n",
    "\n",
    "STL_LOAD_ERRORS presenta un historial de todos los errores de carga de Amazon Redshift. Para obtener una lista completa de posibles errores y explicaciones de cargas, consulte [Referencia de error de carga](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_Load_Error_Reference.html).\n",
    "\n",
    "Consulte [STL_LOADERROR_DETAIL](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STL_LOADERROR_DETAIL.html) para obtener detalles adicionales, como la fila y la columna de datos exactas en las que se produjo un error de análisis, después de consultar STL_LOAD_ERRORS para encontrar información general sobre el error.\n",
    "\n",
    "Todos los usuarios pueden acceder a esta vista. Los superusuarios pueden ver todas las filas; los usuarios normales solo pueden ver sus datos. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "Algunas o todas las columnas de esta tabla también están definidas en la vista de monitoreo de SYS [SYS_LOAD_ERROR_DETAIL](https://docs.aws.amazon.com/es_es/redshift/latest/dg/SYS_LOAD_ERROR_DETAIL.html).\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna\t|Tipo de datos|\tDescripción|\n",
    "|:---|:---:|:---|\n",
    "|userid\t|entero\t|ID del usuario que generó la entrada.|\n",
    "|slice|\tentero\t|Sector en el que se produjo el error.|\n",
    "|tbl\t|entero\t|ID de la tabla.|\n",
    "|starttime\t|timestamp\t|La hora de inicio de la carga en UTC.|\n",
    "|session\t|entero|\tID de la sesión que realiza la carga.|\n",
    "|query\t|entero\t|ID de la consulta. La columna de consulta puede usarse para combinar otras vistas y tablas del sistema.|\n",
    "|filename|\tcharacter (256)\t|Ruta completa hacia el archivo de entrada para la carga.\n",
    "|line_number|\tbigint\t|Número de línea en el archivo de carga que tiene el error. Para el comando COPY de JSON, el número de línea de la última línea del objeto JSON que tiene el error.|\n",
    "|colname\t|character (127)\t|Campo que tiene el error.|\n",
    "|type\t|character (10)\t|Tipo de datos del campo.|\n",
    "|col_length\t|character (10)\t|Longitud de la columna, si corresponde. Este campo se rellena cuando el tipo de datos tiene una longitud limitada. Por ejemplo, para una columna con un tipo de datos \"character (3)\", esta columna tendrá el valor \"3\".|\n",
    "|position\t|entero\t|Posición del error en el campo.|\n",
    "|raw_line|\tcharacter (1024)\t|Datos de carga sin formato que tienen el error. Caracteres multibyte en la carga de datos que se reemplazan con un punto.|\n",
    "|raw_field_value\t|char (1024)\t|El valor antes del análisis del campo \"colname\" que induce el error de análisis.|\n",
    "|err_code\t|entero\t|Código de error.|\n",
    "|err_reason|\tcharacter (100)\t|Explicación del error.|\n",
    "|is_partial|\tentero\t|Valor que, si es true (1), indica que el archivo de entrada se divide en rangos durante una operación COPY. Si este valor es false (0), el archivo de entrada no se divide.\n",
    "|start_offset|\tbigint\t|Valor que, si el archivo de entrada se divide durante una operación COPY, indica el valor de desplazamiento de la división (en bytes). Si se desconoce el número de línea del archivo, el número de línea es -1. Si el archivo no se divide, este valor es 0.|\n",
    "|copy_job_id|\tbigint\t|Identificador del trabajo de copia. Un 0 indica que no hay ningún identificador de trabajo.|\n",
    "\n",
    "### <a name=\"mark_22.1\"></a> STL_LOAD_COMMITS\n",
    "[Index](#index_02)\n",
    "\n",
    "Devuelve información para realizar un seguimiento o solucionar problemas en la carga de datos.\n",
    "\n",
    "En esta vista, se registra el progreso de cada archivo de datos mientras se carga en una tabla de base de datos (registra las cargas exitosas). \n",
    "\n",
    "Todos los usuarios pueden acceder a esta vista. Los superusuarios pueden ver todas las filas; los usuarios normales solo pueden ver sus datos. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "Algunas o todas las columnas de esta tabla también están definidas en las vistas de monitoreo de SYS [SYS_LOAD_DETAIL](https://docs.aws.amazon.com/es_es/redshift/latest/dg/SYS_LOAD_DETAIL.html) y [SYS_LOAD_HISTORY](https://docs.aws.amazon.com/es_es/redshift/latest/dg/SYS_LOAD_HISTORY.html).\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna\t|Tipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|userid\t|entero\t|ID del usuario que generó la entrada.|\n",
    "|query\t|entero\t|ID de la consulta. La columna de consulta puede usarse para combinar otras vistas y tablas del sistema.|\n",
    "|slice\t|entero\t|Sector cargado para esta entrada.|\n",
    "|nombre|\tcharacter (256)\t|Valor definido por el sistema.|\n",
    "|filename\t|character (256)|\tNombre del archivo que del que se hace un seguimiento.|\n",
    "|byte_offset\t|entero\t|Esta información es solo para uso interno.\n",
    "|lines_scanned\t|entero\t|Cantidad de líneas examinadas del archivo de carga. Este número puede no coincidir con el número de filas que se cargan efectivamente. Por ejemplo, la carga puede examinar y, al mismo tiempo, tolerar una serie de registros incorrectos, en función de la opción MAXERROR en el comando COPY.|\n",
    "|errors\t|entero|\tEsta información es solo para uso interno.|\n",
    "|curtime\t|timestamp\t|Hora en que se actualizó por última vez esta entrada.|\n",
    "|status\t|entero\t|Esta información es solo para uso interno.|\n",
    "|file_format\t|character(16)\t|Formato del archivo que se va a cargar. Los valores posibles son los siguientes: Avro, JSON, ORC, Parquet, Texto|\n",
    "|is_partial|\tentero\t|Valor que, si es true (1), indica que el archivo de entrada se divide en rangos durante una operación COPY. Si este valor es false (0), el archivo de entrada no se divide.|\n",
    "|start_offset\t|bigint|\tValor que, si el archivo de entrada se divide durante una operación COPY, indica el valor de desplazamiento de la división (en bytes). Cada división de archivos se registra como un registro independiente con el valor start_offset correspondiente. Si el archivo no se divide, este valor es 0.|\n",
    "|copy_job_id\t|bigint\t|Identificador del trabajo de copia. Un 0 indica que no hay ningún identificador de trabajo.|\n",
    "\n",
    "### Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/copy-parameters-data-conversion.html#copy-escape\n",
    "\n",
    "https://github.com/alarcon7a/redshift_course/tree/master/Copy\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STL_LOAD_ERRORS.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STL_LOAD_COMMITS.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c71134",
   "metadata": {},
   "source": [
    "### <a name=\"mark_23\"></a>Manifiestos y uso de COMPUPDATE para carga con compresión automática\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "Ahora trabajaremos sobre el \"tercer_cargue.csv\" el cual no contiene delimitadores.\n",
    "\n",
    "### tercer_cargue.csv\n",
    "\n",
    "```csv\n",
    "1Carlos   Alarcon  2706-05-2020\n",
    "2Andres   Perez    4506-05-2020\n",
    "3Felipe   Lopez    2006-05-2020\n",
    "```\n",
    "Para indicarle cómo tiene que separar la data se utiliza el parámetro `fixedwith` seguido de `columna:cantidad_de_caracteres`.\n",
    "\n",
    "`fixedwith '0:1,1:9,2:9,3:2,4:10'`, este sería:\n",
    "\n",
    "    - col 0, 1 caracter.\n",
    "    \n",
    "    - col 1, 9 caracteres (también incluye los espacios)\n",
    "    \n",
    "    - col 2, 9 caracteres (también incluye los espacios)\n",
    "    \n",
    "y así sucesivamente.\n",
    "\n",
    "```sql\n",
    "copy estudiante FROM 's3://mibucketredshift/tercer_cargue.csv'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "region 'us-east-2'\n",
    "fixedwidth '0:1,1:9,2:9,3:2,4:10'\n",
    "dateformat 'mm-dd-yyyy';\n",
    "```\n",
    "\n",
    "![](img_128.png)\n",
    "\n",
    "### <a name=\"mark_23.0\"></a>Carga con archivo de manifiesto:\n",
    "[Index](#index_02)\n",
    "\n",
    "Para esto necesitamos crear un archivo con la extensión `.manifest`\n",
    "\n",
    "Contenido en JSON del archivo \"tests.manifest\", como se observa pueden ser multiples fuentes/URLs, mandatory signfica que debe ser mandatorio que se realice la ingesta. Este archivo debe estar en alguno de mis buckets S3.\n",
    "\n",
    "```sql\n",
    "{\n",
    "    \"entries\": [\n",
    "        {\"url\":\"s3://mibucketredshift/cargue_1.csv\", \"mandatory\":true},\n",
    "        {\"url\":\"s3://mibucketredshift/cargue_2.csv\", \"mandatory\":true}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Observación: \"cargue_1.csv\" y \"cargue_2.csv\"  son diferentes a los cargados anteriormente, observar que no figuran los parámetros --> ignoreblanklines, blanksasnull, dateformat 'mm-dd-yyyy'.\n",
    "\n",
    "Sentencia SQL para realizar la ingesta:\n",
    "\n",
    "```sql\n",
    "copy estudiante FROM 's3://mibucketredshift/test.manifest'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "delimiter ';'\n",
    "ignoreheader 1\n",
    "manifest\n",
    "region 'us-east-2';\n",
    "```\n",
    "\n",
    "![](img_129.png)\n",
    "\n",
    "\n",
    "### <a name=\"mark_23.1\"></a>Comando de compresión COMPUPDATE\n",
    "[Index](#index_02)\n",
    "\n",
    "Para esta pruebas se creand 2 tablas diferentes sin compresión (ENCODE raw), \"sales_compression_on\" y \"sales_compression_off\"\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.sales_compression_on\n",
    "(\n",
    "\tsalesid INTEGER NOT NULL  ENCODE raw\n",
    "\t,listid INTEGER NOT NULL  ENCODE raw\n",
    "\t,sellerid INTEGER NOT NULL  ENCODE raw\n",
    "\t,buyerid INTEGER NOT NULL  ENCODE raw\n",
    "\t,eventid INTEGER NOT NULL  ENCODE raw\n",
    "\t,dateid SMALLINT NOT NULL  ENCODE RAW\n",
    "\t,qtysold SMALLINT NOT NULL  ENCODE raw\n",
    "\t,pricepaid NUMERIC(8,2)   ENCODE raw\n",
    "\t,commission NUMERIC(8,2)   ENCODE raw\n",
    "\t,saletime TIMESTAMP WITHOUT TIME ZONE   ENCODE raw\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.sales_compression_off\n",
    "(\n",
    "\tsalesid INTEGER NOT NULL  ENCODE raw\n",
    "\t,listid INTEGER NOT NULL  ENCODE raw\n",
    "\t,sellerid INTEGER NOT NULL  ENCODE raw\n",
    "\t,buyerid INTEGER NOT NULL  ENCODE raw\n",
    "\t,eventid INTEGER NOT NULL  ENCODE raw\n",
    "\t,dateid SMALLINT NOT NULL  ENCODE RAW\n",
    "\t,qtysold SMALLINT NOT NULL  ENCODE raw\n",
    "\t,pricepaid NUMERIC(8,2)   ENCODE raw\n",
    "\t,commission NUMERIC(8,2)   ENCODE raw\n",
    "\t,saletime TIMESTAMP WITHOUT TIME ZONE   ENCODE raw\n",
    ");\n",
    "\n",
    "```\n",
    "\n",
    "Al momento de la ingesta de datos utilizo el comando \"COMPUPDATE ON\" que realiza la siguiente tarea, el comando copy carga parcialmente los datos, luego COMPUPDATE ON analiza cual es la mejor compresión para ese tipo de datos, luego borra la carga parcial y vuelve a crear la tabla con el mejor encodign.\n",
    "\n",
    "```sql\n",
    "copy sales_compression_on FROM 's3://mibucketredshift/sales_tab.txt'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "delimiter '\\t' timeformat 'MM/DD/YYYY HH:MI:SS' COMPUPDATE ON region 'us-east-2';\n",
    "\n",
    "copy sales_compression_off FROM 's3://mibucketredshift/sales_tab.txt'\n",
    "credentials 'aws_iam_role=arn:aws:iam::XXXXXX:role/MiRolRedshift'\n",
    "delimiter '\\t' timeformat 'MM/DD/YYYY HH:MI:SS' COMPUPDATE OFF region 'us-east-2';\n",
    "\n",
    "SELECT * \n",
    "FROM pg_table_def\n",
    "WHERE tablename = 'sales_compression_on';\n",
    "\n",
    "SELECT * \n",
    "FROM pg_table_def\n",
    "WHERE tablename = 'sales_compression_off';\n",
    "\n",
    "```\n",
    "\n",
    "Validación:\n",
    "\n",
    "![](img_130.png)\n",
    "\n",
    "![](img_131.png)\n",
    "\n",
    "### Lecturas Recomendadas\n",
    "\n",
    "https://github.com/alarcon7a/redshift_course/tree/master/Copy/Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d595beb",
   "metadata": {},
   "source": [
    "### <a name=\"mark_24\"></a>Métodos de carga alternativos al comando copy\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "### <a name=\"mark_24.0\"></a>INSERT por lotes.\n",
    "[Index](#index_02)\n",
    "\n",
    "Caso de uso --> No tengo archivos planos, No tengo acceso al bucket, cómo puedo insertar? --> \"INSERT\" con algunas restricciones.\n",
    "\n",
    "Cargar data por lotes, cuanto más mejor (NO UN INSERT POR CADA FILA!!!).\n",
    "\n",
    "![](img_132.png)\n",
    "\n",
    "### <a name=\"mark_24.1\"></a>bulk insert, datos por lotes\n",
    "[Index](#index_02)\n",
    "\n",
    "Esto es a travez de un select poblar otra tabla al momento de su creación.\n",
    "\n",
    "```sql\n",
    "CREATE TABLE total_price_by_event AS (\n",
    "SELECT e.eventname, e.starttime, sum(pricepaid) pricepaid, sum(commission) commission \n",
    "FROM sales s \n",
    "INNER JOIN event e\n",
    "ON s.eventid = e.eventid \n",
    "GROUP BY e.eventname , e.starttime\n",
    ");\n",
    "```\n",
    "\n",
    "### <a name=\"mark_24.2\"></a>deep copy\n",
    "[Index](#index_02)\n",
    "\n",
    "```sql\n",
    "CREATE TABLE likesales (like sales);\n",
    "\n",
    "INSERT INTO likesales (SELECT * FROM sales);\n",
    "\n",
    "DROP TABLE sales;\n",
    "\n",
    "ALTER TABLE likesales RENAME TO sales;\n",
    "```\n",
    "\n",
    "### Lecturas Recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/redshift/latest/dg/performing-a-deep-copy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c21e9",
   "metadata": {},
   "source": [
    "### <a name=\"mark_25\"></a>¿Cómo ejecutar sentencias UPDATE y DELETE?\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "Caso de uso: Tengo que actualizar datos erroneos, entonces hay que actualizar o elimnar en filas y columnas especificas para dejar mi db como corresponde, debido a que Redshift no es una RDB esto no es lo más sencillo, a continuación veremos como realizarlo.\n",
    "\n",
    "Para este ejemplo tenemos la tabla \"sales\" y la tabla \"event\", en los cuales la suposición es que, las filas de las columnas \"pricepaid\" y \"commission\" pertenecientes a la tabla \"sales\" para un determinado evento \"eventid\" tiene un problema y debe ser actualizadas.\n",
    "\n",
    "Columnas de la tabla \"sales\":\n",
    "\n",
    "![](img_133.png)\n",
    "\n",
    "Primero, supongamos que el problema se presenta en un determinado evento llamado \"BECK\", con lo cual para obtener el \"eventid\" conociendo el nombre del evento primero debe consultar la talba \"event\" de la siguiente forma.\n",
    "\n",
    "```sql\n",
    "\n",
    "SELECT * FROM event\n",
    "\n",
    "WHERE eventname = 'BECK'\n",
    "\n",
    "```\n",
    "Como se ve a continuación tenemos mutiples \"eventid\"\n",
    "\n",
    "![](img_134.png)\n",
    "\n",
    "Utilizaremos estos \"eventid\" como sub consulta para buscar todas las ventas que se encuentran en esa lista de eventos.\n",
    "\n",
    "```sql\n",
    "SELECT * FROM sales s \n",
    "WHERE eventid in (SELECT eventid FROM event e\n",
    "WHERE eventname = 'BECK');\n",
    "\n",
    "```\n",
    "Ya tengo las filas que quiero actualizar y recordemos las columnas que necesito, \"pricepaid\" y \"commission\".\n",
    "\n",
    "![](img_135png)\n",
    "\n",
    "A continuación creamos una tabla auxiliar \"sales_auxiliar\" con solo las columnas necesarias, en donde hacemos una transformación multiplicando por 2 el \"pricepaid\" y la \"commission\".\n",
    "\n",
    "```sql\n",
    "CREATE TABLE sales_auxiliar AS (\n",
    "SELECT salesid, pricepaid*2 as pricepaid, commission*2 as commission FROM sales s \n",
    "WHERE eventid in (SELECT eventid FROM event e\n",
    "WHERE eventname = 'BECK'));\n",
    "\n",
    "```\n",
    "Entonces sabemos que el UPDATE fila a fila/transacción a transacción, no es bueno para las db columnares. \n",
    "\n",
    "Nuestro primer UPDATE sera, actualizar la columna \"pricepaid\" de la tabla \"sales\" con los valores de \"pricepaid\" de la tabla \"sales_auxiliar\" solo cuando los \"salesid\" coincidan:\n",
    "\n",
    "```sql\n",
    "UPDATE sales\n",
    "SET pricepaid = sa.pricepaid \n",
    "FROM sales_auxiliar sa\n",
    "WHERE sa.salesid = sales.salesid\n",
    "```\n",
    "\n",
    "También podemos, primero borrar los datos especificos que necesitamos modificar antes de la carga modificada.\n",
    "\n",
    "```sql\n",
    "DELETE FROM sales\n",
    "USING sales_auxiliar\n",
    "WHERE sales_auxiliar.salesid = sales.salesid\n",
    "```\n",
    "\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/redshift/latest/dg/t_updating-inserting-using-staging-tables-.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b73913",
   "metadata": {},
   "source": [
    "### <a name=\"mark_26\"></a>¿Cómo mantener el desempeño de tu base de datos?\n",
    "\n",
    "### [Index](#index_02)\n",
    "\n",
    "### <a name=\"mark_26.0\"></a>Analyze\n",
    "[Index](#index_02)\n",
    "\n",
    "Analyze, actualiza las estadísticas y los metadatos de nuestra base de datos, y debido a que Redshift busca la mejor forma de resolver una consulta compleja, y esto lo hace basandose en los metadatos que existen de las tablas, pequeños segmentos de datos que me dicen como se encuentra distribuida en que slice existen, cual es la cantidad de datos que tiene, que columnas se utilizan para hacer filtros o combinaciones, y todo esto para que al momento de ejecutar la consulta sepa cual es el mejor camino para resolverla. Si esos metadatos y estadísticas no se encuentran actualizados el performance de Redshift puede bajar bastante.\n",
    "\n",
    "![](img_136.png)\n",
    "\n",
    "Como el comando Analyze consume ciertos recurso, este solo se ejecuta automaticamente cuando la cantidad de datos insertados supera un cierto porcentaje. Pero también lo podemos hacer manualmente.\n",
    "\n",
    "Acontinuación puedo indicar que se correrá Analyze cuando el porcentaje de cambio sea del 10%.\n",
    "\n",
    "![](img_137.png)\n",
    "\n",
    "### <a name=\"mark_26.1\"></a>Vacuum (Limpieza)\n",
    "[Index](#index_02)\n",
    "\n",
    "En Redshift cuando eliminamos datos de una tabla, en background estos datos siguen almacenados por un tiempo hasta que se corre la operación de Vacuum.\n",
    "\n",
    "- Full: Si borre muchos registros de una tabla, los eliminará liberando espacio.\n",
    "\n",
    "- Sort Only: Supongamos que tengo una tabla, la cual recibirá 10 COPYs (inserto 10 lotes de datos), si esa tabla está ordenada, esos lotes de datos, no se insertaran de manera ordenada al instante, en definitiva, luego de esos 10 COPYs mi tabla se encuentra desordenada en mis Nodos y en mis Slices, SORT ONLY reordena y redistribuye los datos para optimizar el consumo.\n",
    "\n",
    "![](img_140.png)\n",
    "\n",
    "![](img_141.png)\n",
    "\n",
    "Como se ve a continuación la data termina ordenada según el criterio de una llave de ordenamiento, o según el criterio que los Nodos indiquen deba ser ordenada, para finalmente hacer un merge a la tabla original con el ordenamiento necesario.\n",
    "\n",
    "![](img_142.png)\n",
    "\n",
    "### Observación: \n",
    "\n",
    "Tener en cuenta que Vacuum Sort es una operación costosa en terminos de tiempo, que también indispone la tabla durante el proceso.\n",
    "\n",
    "- Delete Only: Es la tarea de limpieza del delete.\n",
    "\n",
    "- Reindex: Es unica y especificamente para tipos de ordenamientos intercalados, reordena en cada Nodo y Slice para ese tipo de tablas con ordenamiento intercalado.\n",
    "\n",
    "![](img_138.png)\n",
    "\n",
    "Vacuum también maneja un porcentaje de \"desorden\", por lo cual si encuetro que la tabla está en cierto porcentaje desordenada se correrá la tarea.\n",
    "\n",
    "![](img_139.png)\n",
    "\n",
    "### Lecturas recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_ANALYZE.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_VACUUM_command.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a1d9d",
   "metadata": {},
   "source": [
    "### <a name=\"mark_27\"></a>Estadísticas y limpieza de las tablas\n",
    "### [Index](#index_02)\n",
    "\n",
    "### <a name=\"mark_27.0\"></a>PG_STATISTIC_INDICATOR\n",
    "[Index](#index_02)\n",
    "\n",
    "Almacena información acerca del número de filas insertadas o eliminadas desde la última vez que se ejecutó ANALYZE. La tabla PG_STATISTIC_INDICATOR se actualiza frecuentemente siguiendo las operaciones DML, por lo que las estadísticas son aproximadas.\n",
    "\n",
    "Solo los superusuarios pueden ver PG_STATISTIC_INDICATOR. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna|\tTipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|stairelid\t|oid\t|ID de la tabla|\n",
    "|stairows|\tfloat\t|Cantidad total de filas en la tabla.|\n",
    "|staiins\t|float\t|Número de filas insertadas desde el último ANALYZE.|\n",
    "|staidels|\tfloat\t|Número de filas eliminadas o actualizadas desde el último ANALYZE.|\n",
    "\n",
    "#### Observación: (OID) Identificadores Únicos de Objetos, Según definición de la Unión Internacional de Telecomunicaciones (ITU) un OID es: “un valor único global asociado con un objeto que lo identifica sin ambigüedades”.\n",
    "\n",
    "![](img_143.png)\n",
    "\n",
    "![](img_144.png)\n",
    "\n",
    "Puedo hacer el Analyze de diferentes maneras, lo puedo hacer por \"columnas\", por \"columnas de predicado\" que son las columnas que usualmente Redshift reconoce que se les hace un \"where\", o puedo hacer un análisis de estadística total.\n",
    "\n",
    "Supongamos que sabemos sobre la inserción reciente de datos en las columnas \"saleid\", y \"pricepaid\" de la tabla \"sales\", puedo correr el comando Analyze para esas columnas y tabla, actualizando las estadísticas de \"sales\" por las columnas indicadas.\n",
    "\n",
    "```sql\n",
    "analyze sales(saleid, pricepaid);\n",
    "```\n",
    "\n",
    "![](img_145.png)\n",
    "\n",
    "Realizamos el analyze pero por \"columnas de predicado\", que son las columnas identificadas por Redshift como columnas que siempre se utilizan para filtrar datos.\n",
    "\n",
    "```sql\n",
    "analyze sale predicate columns;\n",
    "```\n",
    "![](img_146.png)\n",
    "\n",
    "Por último le puedo indicar que actualize las estadísticas de toda la tabla \"sales\".\n",
    "\n",
    "```sql\n",
    "analyze sales;\n",
    "```\n",
    "![](img_147.png)\n",
    "\n",
    "### <a name=\"mark_27.1\"></a>STL_ANALYZE\n",
    "[Index](#index_02)\n",
    "\n",
    "Log de ejecución de análisis manual o automatico realizado por Redshift.\n",
    "\n",
    "Registra detalles para las operaciones [ANALYZE](https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_ANALYZE.html).\n",
    "\n",
    "Solo los superusuarios pueden acceder a esta vista. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "Algunas o todas las columnas de esta tabla también están definidas en la vista de monitoreo de SYS [SYS_ANALYZE_HISTORY](https://docs.aws.amazon.com/es_es/redshift/latest/dg/SYS_ANALYZE_HISTORY.html).\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna|\tTipo de datos|\tDescripción|\n",
    "|:---|:---:|:---|\n",
    "|userid|\tentero\t|ID del usuario que generó la entrada.|\n",
    "|xid|\tlong\t|El ID de la transacción.|\n",
    "|base de datos|\tchar (30)\t|El nombre de la base de datos.|\n",
    "|table_id\t|entero\t|El ID de la tabla.|\n",
    "|status|\tchar (15)\t|El resultado del comando analyze. Los posibles valores son Full, Skipped y PredicateColumn.|\n",
    "|rows|\tdouble|\tLa cantidad total de filas en la tabla.|\n",
    "|modified_rows|\tdouble\t|La cantidad total de filas que se modificaron desde la última operación ANALYZE.|\n",
    "|threshold_percent\t|entero\t|El valor del parámetro analyze_threshold_percent.|\n",
    "|is_auto\t|char(1)\t|El valor es true (t) si la operación incluía una operación analyze de Amazon Redshift de manera predeterminada. El valor es false (f) si el comando ANALYZE se ejecutaba explícitamente.|\n",
    "|starttime\t|timestamp\t|La hora en UTC en que comenzó a ejecutarse la operación analyze.|\n",
    "|endtime|\ttimestamp\t|La hora en UTC en que terminó de ejecutarse la operación analyze.|\n",
    "|prevtime|\ttimestamp\t|La hora en UTC en que se analizó previamente la tabla.|\n",
    "|num_predicate_cols|\tentero\t|La cantidad actual de columnas de predicados en la tabla.|\n",
    "|num_new_predicate_cols|\tentero\t|La cantidad de columnas nuevas de predicado desde la operación analyze anterior.|\n",
    "|is_background\t|character (1)\t|El valor es true (t) si una operación analyze automática ejecutaba el análisis. De lo contrario, el valor es false (f).|\n",
    "|auto_analyze_phase\t|character (100)\t|Se reserva para uso interno.|\n",
    "|schema_name\t|char(128)\t|Nombre del esquema para la tabla.|\n",
    "|table_name|\tchar (136)\t|El nombre de la tabla.|\n",
    "\n",
    "### <a name=\"mark_27.2\"></a>Vacuum:\n",
    "[Index](#index_02)\n",
    "\n",
    "A continuación podemos ver como utilizar la función \"svv_table_info\" para extraer información de las las columnas \"table\", unsorted, vacuum_sort_benefit.\n",
    "\n",
    "En donde unsorted me indica que porcentaje está desordenado, y \"vacuum_sort_benefit\" cual será el beneficio de realizar la operación de limpieza vacuum sort.\n",
    "\n",
    "```sql\n",
    "SELECT \"table\", unsorted, vacuum_sort_benefit\n",
    "FROM svv_table_info;\n",
    "\n",
    "vacuum sales;\n",
    "vacuum sort only sales to 75 percent;\n",
    "vacuum delete only sales to 75 percent;\n",
    "vacuum reindex sales; --Solo funciona se el ordenamiento es intercalado/interleaved\n",
    "```\n",
    "\n",
    "\n",
    "### Lectura recomendada:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_PG_STATISTIC_INDICATOR.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STL_ANALYZE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f8179",
   "metadata": {},
   "source": [
    "### <a name=\"mark_28\"></a>Agrupamiento, ordenamiento y subqueries\n",
    "### [Index](#index_03)\n",
    "\n",
    "Pensar todo lo siguiente en un contexto de mejora de performance y no en lógica.\n",
    "\n",
    "![](img_148.png)\n",
    "\n",
    "Usar join por llaves --> usar left join, right join, etc si solo utilizar join, esto pasara a un plano cartesiano.\n",
    "\n",
    "Las funciones funcionan filas por filas, muy costoso.\n",
    "\n",
    "![](img_149.png)\n",
    "\n",
    "Si la subconsulta retorna más de 200 registros, mejor utilizar un join.\n",
    "\n",
    "![](img_150.png)\n",
    "\n",
    "![](img_151.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a337c26",
   "metadata": {},
   "source": [
    "### <a name=\"mark_29\"></a>¿Qué es y cómo interpretar un explain plan?\n",
    "### [Index](#index_03)\n",
    "\n",
    "Cómo mido el performance? --> EXPLAIN\n",
    "\n",
    "Ejemplo: De las tablas \"event\" y \"venue\" quiero las la siguiente consulta, pero ejecuto la consulta con la palabra \"explain\" delante.\n",
    "\n",
    "```sql\n",
    "explain \n",
    "select eventid, eventname, event.venueid, venuename\n",
    "from event, venue;\n",
    "```\n",
    "\n",
    "- Como vimos anteriormente el `from vent, venue` ejecutado de esa forma produce un producto cartesiano \"Nested Loop\", lo cual es muy malo para el performance.\n",
    "\n",
    "- Luego de ejecutar EXPLAIN se muestran diferentes parámetros, por ejemplo que tipo de query estamos ejecutando, \"Nested Loop\", seguido a ésta veremos una función, para este caso particular es \"DS_BCAST_INNER\", las funciones que aparecen en esa posición, nos indican cuan buenas o malas son las queries, a continuación una lista de ellas.\n",
    "\n",
    "![](img_152.png)\n",
    "\n",
    "- Luego vemos el costo de la query \"cost=0.00..56595631.90 rows=17777196 widht=43\", siendo 0.00 cuanto me cuesta traer la primer fila, siendo 56595631.90 cuanto me cuesta traer todas las filas, seguido de rows=17777196 la cantidad de filas que retornará, y finalmente widht=43 cuantas pesa en bytes.\n",
    "\n",
    "- cost=0.00..56595631.90, es una estimación de cuanto es el costo de aplicar un operador, para este caso Nested Loop, son valores relativos y no tienen una traducción directa, sin embargo pueden ser utilizados para comparar el costo entre diferentes operaciones.\n",
    "\n",
    "De igual manera puedo interpretar cada una de las ejecuciones.\n",
    "\n",
    "![](img_153.png)\n",
    "\n",
    "Cómo cambia el performance si a la misma query le agregamos un filtro where, en donde hacemos coincidir los \"venueid\"\n",
    "\n",
    "```sql\n",
    "explain\n",
    "select eventid, eventname, event.venueid, venuename\n",
    "from event, venue\n",
    "where event.venueid = venue.venueid;\n",
    "```\n",
    "\n",
    "Como vemos a continuación ya no tenemos un Nested Loop, sino un Hash Join con un menor costo y menor cantidad de filas.\n",
    "\n",
    "![](img_154.png)\n",
    "\n",
    "Finalmente veremos un ejemplo el cual es el cruce óptimo para las tablas de Redshift, para ello primero exploramos las tablas \"event\" y \"venue\" con \"pg_table_def\".\n",
    "\n",
    "```sql\n",
    "select * from pg_table_def\n",
    "where tablename in ('event', 'venue');\n",
    "```\n",
    "Podemos ver que la tabla \"venue\" sí tiene una llave de distribución por \"venueid\", pero la tabla event no la tiene, con lo cual crearemos una nueva tabla \"event_2\" para que tenga su llave de distribución y de ordenamiento también por \"venueid\".\n",
    "\n",
    "![](img_155.png)\n",
    "\n",
    "A continuación creo una nueva tabla \"event_2\" con una llave de distribución y ordenamiento en \"venueid\".\n",
    "\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS public.event_2\n",
    "(\n",
    "\teventid INTEGER NOT NULL  ENCODE az64\n",
    "\t,venueid SMALLINT NOT NULL  ENCODE az64 distkey sortkey\n",
    "\t,catid SMALLINT NOT NULL  ENCODE az64\n",
    "\t,dateid SMALLINT NOT NULL  ENCODE RAW\n",
    "\t,eventname VARCHAR(200)   ENCODE lzo\n",
    "\t,starttime TIMESTAMP WITHOUT TIME ZONE   ENCODE az64\n",
    ");\n",
    "```\n",
    "Poblamos \"event_2\" con los mismos datos de \"event\"\n",
    "\n",
    "```sql\n",
    "insert into event_2 (select * from event);\n",
    "```\n",
    "\n",
    "Ahora utilicemos nuestra nueva tabla \"event_2\" con la misma query.\n",
    "\n",
    "```sql\n",
    "explain\n",
    "select eventid, eventname, event_2.venueid, venuename\n",
    "from event_2, venue\n",
    "where event_2.venueid = venue.venueid;\n",
    "```\n",
    "Vemos que el costo se ha reducido muchisimo, pero con un tamaño en bites considerablemente mayor, pero como es una tabla recien creada aún no tiene las estadísticas actualizadas, tengamos en cuenta que EXPLAIN utiliza la metada y las estadísticas para evaluar la mejor ejecución, con lo cual hay que correr un analyze.\n",
    "\n",
    "![](img_156.png)\n",
    "\n",
    "Corremos el analyze.\n",
    "\n",
    "```sql\n",
    "analyze event_2\n",
    "```\n",
    "Corremos la query nuevamente.\n",
    "\n",
    "![](img_157.png)\n",
    "\n",
    "De mejor a peor.\n",
    "\n",
    "Merge Join --> Hash Join --> Nested Loop\n",
    "\n",
    "### Ejemplos utilizando funciones de agregación.\n",
    "\n",
    "Ejecutamos la siguiente query.\n",
    "\n",
    "```sql\n",
    "explain\n",
    "select e.eventname, sum(pricepaid)\n",
    "from sales s\n",
    "inner join event e\n",
    "where e.eventid = s.eventid\n",
    "group by e.eventname;\n",
    "```\n",
    "\n",
    "![](img_158.png)\n",
    "\n",
    "![](img_159.png)\n",
    "\n",
    "A continuación sobre la tabla event probaremos algunas queries.\n",
    "\n",
    "![](img_160.png)\n",
    "\n",
    "La misma query pero con un \"order by\" por una columna que no tiene ordenamiento.\n",
    "\n",
    "![](img_161.png)\n",
    "\n",
    "Como vemos a continuación \"dateid\" es la columna que sí tiene llave de ordenamiento.\n",
    "\n",
    "![](img_162.png)\n",
    "\n",
    "La performance mejora muchísimo.\n",
    "\n",
    "![](img_163.png)\n",
    "\n",
    "### <a name=\"mark_29.0\"></a>Alerta sobre el performance de las queries con stl_alert_event_log\n",
    "[Index](#index_03)\n",
    "\n",
    "![](img_164.png)\n",
    "\n",
    "### <a name=\"mark_29.1\"></a>STL_QUERY\n",
    "[Index](#index_03)\n",
    "\n",
    "Devuelve información acerca de la ejecución de la consulta de una base de datos.\n",
    "\n",
    "![](img_165.png)\n",
    "\n",
    "### <a name=\"mark_29.2\"></a>SVL_QLOG\n",
    "[Index](#index_03)\n",
    "\n",
    "La vista SVL_QLOG tiene un registro de todas las consultas que se ejecutan para la base de datos.\n",
    "\n",
    "Amazon Redshift crea la vista SVL_QLOG como un subconjunto legible de información de la tabla STL_QUERY. Utilice esta tabla para encontrar el ID de consulta correspondiente a una consulta recientemente ejecutada o para ver cuánto demora en completarse una consulta.\n",
    "\n",
    "SVL_QLOG es visible para todos los usuarios. Los superusuarios pueden ver todas las filas; los usuarios normales solo pueden ver sus datos. Para obtener más información, consulte [Visibilidad de datos en las tablas y vistas de sistema](https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html#c_visibility-of-data).\n",
    "\n",
    "Algunas o todas las columnas de esta tabla también están definidas en la vista de monitoreo de SYS [SYS_QUERY_HISTORY](https://docs.aws.amazon.com/es_es/redshift/latest/dg/SYS_QUERY_HISTORY.html).\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna\t|Tipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|userid|\tentero\t|ID del usuario que generó la entrada.|\n",
    "|query\t|entero|\tID de la consulta. Puede usar este ID para combinar distintas vistas y tablas del sistema.|\n",
    "|xid\t|bigint\t|ID de transacción.|\n",
    "|pid\t|entero\t|ID del proceso asociado a la consulta.|\n",
    "|starttime\t|timestamp\t|Hora exacta en que la instrucción comenzó a ejecutarse, con seis dígitos de precisión para las fracciones de segundos; por ejemplo: 2009-06-12 11:29:19.131358|\n",
    "|endtime\t|timestamp\t|Hora exacta en que la instrucción terminó de ejecutarse, con seis dígitos de precisión para las fracciones de segundos; por ejemplo: 2009-06-12 11:29:19.193640|\n",
    "|elapsed\t|bigint\t|Tiempo que demoró la consulta en ejecutarse (en microsegundos).\n",
    "|aborted\t|entero\t|Si el sistema detuvo la consulta o el usuario la canceló, esta columna tendrá el valor 1. Si la consulta se ejecutó hasta su finalización, esta columna tendrá el valor 0. Las consultas que se cancelaron por motivos de administración de cargas de trabajo y que se reiniciaron después también tienen un valor 1 en esta columna.|\n",
    "|label\t|character(320)\t|Ya sea el nombre del archivo utilizado para ejecutar la consulta o una etiqueta definida con un comando SET QUERY_GROUP. Si la consulta no se basa en archivos o si no se establece el parámetro QUERY_GROUP, el valor del campo es default.|\n",
    "|subcadena\t|character (60)\t|Texto de consulta truncado.|\n",
    "|source_query\t|entero\t|Si la consulta utilizó el almacenamiento en caché de los resultados, es el ID de la consulta de origen de los resultados almacenados en caché. Si no se utilizó el almacenamiento en caché, el valor de este campo es NULL.|\n",
    "|concurrency_scaling_status_txt|\ttexto\t|Una descripción que indica si la consulta se ejecutó en el clúster principal o en un clúster de escalado de simultaneidad.|\n",
    "|from_sp_call\t|entero\t|Si la consulta se llamó desde un procedimiento almacenado, el ID de la consulta de la llamada del procedimiento. Si la consulta no se ejecutó como parte de un procedimiento almacenado, este campo es NULL.|\n",
    "\n",
    "### EXPLAIN\n",
    "\n",
    "Muestra el plan de ejecución de una instrucción de consulta sin ejecutar la consulta. Para obtener información sobre el flujo de trabajo del análisis de consultas, consulte [Flujo de trabajo de análisis de consultas](https://docs.aws.amazon.com/es_es/redshift/latest/dg/c-query-analysis-process.html).\n",
    "\n",
    "#### Sintaxis\n",
    "\n",
    "\n",
    "`EXPLAIN [ VERBOSE ] query`\n",
    "                  \n",
    "#### Parámetros\n",
    "\n",
    "_ VERBOSE\n",
    "\n",
    "Muestra el plan de consulta completo en lugar de solo un resumen.\n",
    "\n",
    "_ Query\n",
    "\n",
    "Instrucción de consulta que se explicará. La consulta puede ser una instrucción SELECT, INSERT, CREATE TABLE AS, UPDATE o DELETE.\n",
    "\n",
    "#### Notas de uso\n",
    "\n",
    "El rendimiento de EXPLAIN a menudo se ve afectado por el tiempo que lleva crear tablas temporales. Por ejemplo, una consulta que usa la optimización de subexpresión común requiere que se creen y analicen tablas temporales para devolver el resultado de EXPLAIN. El plan de consulta depende del esquema y las estadísticas de las tablas temporales. Por lo tanto, el comando EXPLAIN para este tipo de consulta puede tardar más tiempo en ejecutarse que lo esperado.\n",
    "\n",
    "#### Puede usar EXPLAIN solo para los siguientes comandos:\n",
    "\n",
    "SELECT\n",
    "\n",
    "SELECT INTO\n",
    "\n",
    "CREATE TABLE AS\n",
    "\n",
    "INSERT\n",
    "\n",
    "UPDATE\n",
    "\n",
    "DELETE\n",
    "\n",
    "El comando EXPLAIN producirá un error si lo utiliza para otros comandos SQL, como operaciones de base de datos o lenguaje de definición de datos (DDL).\n",
    "\n",
    "Amazon Redshift utiliza los costos unitarios relativos de los resultados de EXPLAIN para elegir un plan de consultas. Amazon Redshift compara los tamaños de varias estimaciones de recursos para determinar el plan.\n",
    "\n",
    "#### Pasos de planificación y ejecución de consultas\n",
    "\n",
    "El plan de ejecución para una instrucción de consulta de Amazon Redshift específica desglosa la ejecución y el cálculo de una consulta en una secuencia discreta de operaciones de tabla y pasos que eventualmente producen un conjunto de resultados finales para la consulta. Para obtener información acerca de la planificación de consultas, consulte [Procesamiento de consultas](https://docs.aws.amazon.com/es_es/redshift/latest/dg/c-query-processing.html).\n",
    "\n",
    "La siguiente tabla proporciona un resumen de los pasos que Amazon Redshift puede utilizar con objeto de desarrollar un plan de ejecución para cualquier consulta que un usuario envíe para su ejecución.\n",
    "\n",
    "|Operadores EXPLAIN|\tPasos de ejecución de la consulta|\tDescripción|\n",
    "|:---|:---:|:---|\n",
    "|SCAN:|\n",
    "|Sequential Scan|\tscan|\tEs el paso o el operador de análisis de tabla, o el análisis de la relación de Amazon Redshift. Analiza la tabla completa de manera secuencial de principio a fin y evalúa las restricciones de la consulta para cada una de las filas (Filtro) si se especifica con la cláusula WHERE. También se utiliza para ejecutar las instrucciones INSERT, UPDATE y DELETE.|\n",
    "|JOINS: Amazon Redshift utiliza diferentes operadores de combinación en función del diseño físico de las tablas que se combinarán, de la ubicación de los datos que se necesitan para la combinación y de los atributos específicos de la propia consulta. Análisis de subconsulta -- El anexo y análisis de subconsultas se utilizan para ejecutar consultas UNION.|\n",
    "|Nested Loop|\tnloop\t|Combinación menos óptima; se utiliza principalmente para las combinaciones cruzadas (productos cartesianos; sin una condición de combinación) y algunas combinaciones de desigualdades.|\n",
    "|Hash Join|\thjoin|\tTambién se utiliza para combinaciones internas y combinaciones externas de izquierda y derecha; por lo general, es más rápida que una combinación de bucle anidado. La combinación hash lee la tabla hash externa, aplica la función hash a la columna de combinación y encuentra coincidencias en la tabla hash interna. El paso puede verterse en el disco. (La entrada interna de hjoin es un paso hash que puede estar basado en el disco).|\n",
    "|Merge Join\t|mjoin\t|También se utiliza para combinaciones internas y externas (para tablas de combinación que están distribuidas y ordenadas en las columnas de combinación). Por lo general, es el algoritmo de combinación de Amazon Redshift más rápido, sin incluir otras consideraciones en torno al costo.|\n",
    "|AGGREGATION: operadores y pasos que se utilizan en las consultas que implican funciones de agregación y operaciones GROUP BY.|\n",
    "|Aggregate\t|aggr|\tOperador/paso para las funciones de agregación escalar.|\n",
    "|HashAggregate\t|aggr|\tOperador/paso para las funciones de agregación agrupadas. Puede funcionar desde el disco en virtud de la tabla hash que se vierte en el disco.|\n",
    "|GroupAggregate\t|aggr\t|En ocasiones, es el operador elegido para consultas de agrupación en conjunto si la configuración de Amazon Redshift del ajuste force_hash_grouping está desactivada.|\n",
    "|SORT: operadores y pasos que se utilizan cuando las consultas tienen que ordenar o fusionar conjuntos de resultados.|\n",
    "|Sort\t|sort\t|Sort realiza la ordenación especificada por la cláusula ORDER BY y, también, otras operaciones como UNION y combinaciones. Puede funcionar desde el disco.|\n",
    "|Merge|\tmerge|\tProduce los resultados ordenados finales de una consulta en función de los resultados ordenados intermedios que se derivan de las operaciones realizadas en paralelo.|\n",
    "|Operaciones EXCEPT, INTERSECT y UNION:|\n",
    "|SetOp Except [Distinct]|\thjoin\t|Se usa para consultas EXCEPT. Puede funcionar desde el disco dado que el hash de entrada puede estar basado en el disco.|\n",
    "|Hash Intersect [Distinct]|\thjoin\t|Se usa para consultas INTERSECT. Puede funcionar desde el disco dado que el hash de entrada puede estar basado en el disco.|\n",
    "|Append [All Distinct]|\tsave\t|Anexo utilizado con el análisis de subconsulta para implementar las consultas UNION y UNION ALL. Puede funcionar desde el disco en virtud del comando \"save\".|\n",
    "|Otros:|\n",
    "|Hash|\thash|\tSe utiliza para combinaciones internas y combinaciones externas de izquierda y derecha (proporciona una entrada a la combinación hash). El operador Hash crea la tabla hash para la tabla interna de una combinación. (La tabla interna es la tabla que se revisa para encontrar coincidencias y, en una combinación de dos tablas, es por lo general la más pequeña de las dos).|\n",
    "|Límite|\tlimit|\tEvalúa la cláusula LIMIT.|\n",
    "|Materialize|\tsave|\tMaterializa las filas para ingresarlas en combinaciones de bucle anidado y en algunas combinaciones de fusión. Puede funcionar desde el disco.|\n",
    "|--\t|parse\t|Se usa para analizar datos de entrada de texto durante una carga.|\n",
    "|--\t|project\t|Se usa para reordenar columnas y expresiones de computación, es decir, datos del proyecto.|\n",
    "|Resultado\t|--\t|Ejecuta funciones escalares que no implican el acceso a ninguna tabla.|\n",
    "|--|\treturn\t|Devuelve filas al nodo principal o cliente.|\n",
    "|Subplan\t|--\t|Se utiliza para algunas subconsultas.|\n",
    "|Unique\t|unique|\tElimina los duplicados de las consultas SELECT DISTINCT y UNION.|\n",
    "|Window\t|window|\tCalcula la adición y clasificación de funciones de ventana. Puede funcionar desde el disco.|\n",
    "|Operaciones de red:|\n",
    "|Network (Broadcast)\t|bcast\t|Broadcast también es una atributo de los operadores y pasos de Join Explain.|\n",
    "|Network (Distribute)\t|dist\t|Distribuye filas en nodos de computación para el procesamiento paralelo del clúster del data warehouse.|\n",
    "|Network (Send to Leader)|\treturn|\tEnvía los resultados de regreso al nodo principal para seguir trabajando con ellos.|\n",
    "|Operaciones DML (operadores que modifican datos):|\n",
    "|Insert (using Result)|\tinsert|\tInserta datos.|\n",
    "|Delete (Scan + Filter)\t|eliminar\t|Elimina datos. Puede funcionar desde el disco.|\n",
    "|Update (Scan + Filter)\t|delete, insert\t|Implementado como eliminar e insertar.|\n",
    "\n",
    "#### Uso de EXPLAIN para RLS\n",
    "\n",
    "Si una consulta contiene una tabla que está sujeta a políticas de seguridad de la fila (RLS), EXPLAIN muestra un nodo especial SecureScan de RLS. Amazon Redshift también registra el mismo tipo de nodo en la tabla del sistema STL_EXPLAIN. EXPLAIN no revela el predicado de RLS que se aplica a dim_tbl. El tipo de nodo SecureScan de RLS sirve como indicador de que el plan de ejecución contiene operaciones adicionales que son invisibles para el usuario actual.\n",
    "\n",
    "En el siguiente ejemplo, se ilustra un nodo SecureScan de RLS.\n",
    "\n",
    "\n",
    "EXPLAIN\n",
    "SELECT D.cint\n",
    "FROM fact_tbl F INNER JOIN dim_tbl D ON F.k_dim = D.k\n",
    "WHERE F.k_dim / 10 > 0;\n",
    "                               QUERY PLAN\n",
    "------------------------------------------------------------------------\n",
    " XN Hash Join DS_DIST_ALL_NONE  (cost=0.08..0.25 rows=1 width=4)\n",
    "   Hash Cond: (\"outer\".k_dim = \"inner\".\"k\")\n",
    "   ->  *XN* *RLS SecureScan f  (cost=0.00..0.14 rows=2 width=4)*\n",
    "         Filter: ((k_dim / 10) > 0)\n",
    "   ->  XN Hash  (cost=0.07..0.07 rows=2 width=8)\n",
    "         ->  XN Seq Scan on dim_tbl d  (cost=0.00..0.07 rows=2 width=8)\n",
    "               Filter: ((\"k\" / 10) > 0)\n",
    "Para habilitar una investigación completa de los planes de consultas que están sujetos a RLS, Amazon Redshift ofrece los permisos del sistema EXPLAIN RLS. Los usuarios a los que se les haya otorgado este permiso pueden inspeccionar planes de consulta completos que también incluyan predicados de RLS.\n",
    "\n",
    "En el siguiente ejemplo, se ilustra un análisis de secuencia adicional debajo del nodo SecureScan de RLS que también incluye el predicado de política de RLS (k_dim > 1).\n",
    "\n",
    "\n",
    "EXPLAIN SELECT D.cint\n",
    "FROM fact_tbl F INNER JOIN dim_tbl D ON F.k_dim = D.k\n",
    "WHERE F.k_dim / 10 > 0;\n",
    "                                   QUERY PLAN\n",
    "---------------------------------------------------------------------------------\n",
    " XN Hash Join DS_DIST_ALL_NONE  (cost=0.08..0.25 rows=1 width=4)\n",
    "   Hash Cond: (\"outer\".k_dim = \"inner\".\"k\")\n",
    "   *->  XN RLS SecureScan f  (cost=0.00..0.14 rows=2 width=4)\n",
    "         Filter: ((k_dim / 10) > 0)*\n",
    "         ->  *XN* *Seq Scan on fact_tbl rls_table  (cost=0.00..0.06 rows=5 width=8)\n",
    "               Filter: (k_dim > 1)*\n",
    "   ->  XN Hash  (cost=0.07..0.07 rows=2 width=8)\n",
    "         ->  XN Seq Scan on dim_tbl d  (cost=0.00..0.07 rows=2 width=8)\n",
    "               Filter: ((\"k\" / 10) > 0)\n",
    "Mientras se concede el permiso EXPLAIN RLS a un usuario, Amazon Redshift registra el plan de consulta completo, incluidos los predicados de RLS, en la tabla del sistema STL_EXPLAIN. Las consultas que se ejecutan mientras no se concede este permiso se registrarán sin los internos de RLS. La concesión o eliminación del permiso EXPLAIN RLS no cambiará lo que Amazon Redshift haya registrado en STL_EXPLAIN para consultas anteriores.\n",
    "\n",
    "Relaciones de Redshift protegidas por RLS de AWS Lake Formation\n",
    "En el siguiente ejemplo se ilustra un nodo LF SecureScan que puede utilizar para ver las relaciones entre Lake Formation y RLS.\n",
    "\n",
    "\n",
    "EXPLAIN\n",
    "SELECT *\n",
    "FROM lf_db.public.t_share\n",
    "WHERE a > 1;\n",
    "QUERY PLAN\n",
    "---------------------------------------------------------------\n",
    "XN LF SecureScan t_share  (cost=0.00..0.02 rows=2 width=11)\n",
    "(2 rows)\n",
    "Ejemplos\n",
    "\n",
    "nota\n",
    "Para estos ejemplos, la salida de ejemplo puede variar según la configuración de Amazon Redshift.\n",
    "\n",
    "El siguiente ejemplo devuelve el plan de consulta para una consulta que selecciona EVENTID, EVENTNAME, VENUEID y VENUENAME de la tablas EVENT y VENUE:\n",
    "\n",
    "\n",
    "explain\n",
    "select eventid, eventname, event.venueid, venuename\n",
    "from event, venue\n",
    "where event.venueid = venue.venueid;\n",
    "\n",
    "                                QUERY PLAN\n",
    "--------------------------------------------------------------------------\n",
    "XN Hash Join DS_DIST_OUTER  (cost=2.52..58653620.93 rows=8712 width=43)\n",
    "Hash Cond: (\"outer\".venueid = \"inner\".venueid)\n",
    "->  XN Seq Scan on event  (cost=0.00..87.98 rows=8798 width=23)\n",
    "->  XN Hash  (cost=2.02..2.02 rows=202 width=22)\n",
    "->  XN Seq Scan on venue  (cost=0.00..2.02 rows=202 width=22)\n",
    "(5 rows)\n",
    "El siguiente ejemplo devuelve el plan de consulta para la misma consulta con resultados más detallados:\n",
    "\n",
    "\n",
    "explain verbose\n",
    "select eventid, eventname, event.venueid, venuename\n",
    "from event, venue\n",
    "where event.venueid = venue.venueid;\n",
    "\n",
    "                                QUERY PLAN\n",
    "--------------------------------------------------------------------------\n",
    "{HASHJOIN\n",
    ":startup_cost 2.52\n",
    ":total_cost 58653620.93\n",
    ":plan_rows 8712\n",
    ":plan_width 43\n",
    ":best_pathkeys <>\n",
    ":dist_info DS_DIST_OUTER\n",
    ":dist_info.dist_keys (\n",
    "TARGETENTRY\n",
    "{\n",
    "VAR\n",
    ":varno 2\n",
    ":varattno 1\n",
    "...\n",
    "\n",
    "XN Hash Join DS_DIST_OUTER  (cost=2.52..58653620.93 rows=8712 width=43)\n",
    "Hash Cond: (\"outer\".venueid = \"inner\".venueid)\n",
    "->  XN Seq Scan on event  (cost=0.00..87.98 rows=8798 width=23)\n",
    "->  XN Hash  (cost=2.02..2.02 rows=202 width=22)\n",
    "->  XN Seq Scan on venue  (cost=0.00..2.02 rows=202 width=22)\n",
    "(519 rows)\n",
    "El siguiente ejemplo devuelve el plan de consulta para una instrucción CREATE TABLE AS (CTAS):\n",
    "\n",
    "\n",
    "explain create table venue_nonulls as\n",
    "select * from venue\n",
    "where venueseats is not null;\n",
    "\n",
    "QUERY PLAN\n",
    "-----------------------------------------------------------\n",
    "XN Seq Scan on venue  (cost=0.00..2.02 rows=187 width=45)\n",
    "Filter: (venueseats IS NOT NULL)\n",
    "(2 rows)\n",
    "\n",
    "\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/c_data_redistribution.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STL_ALERT_EVENT_LOG.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_STL_QUERY.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVL_QLOG.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b46c0",
   "metadata": {},
   "source": [
    "### <a name=\"mark_30\"></a>¿Cómo descargar datos eficientemente con UNLOAD?\n",
    "### [Index](#index_03)\n",
    "\n",
    "Supongamos que tenemos toda nuestra data en Redshift y alguien en nuestro equipo no tiene acceso a el, entonces cual es la mejor manera de bajar esa información?\n",
    "\n",
    "Esto se trata de pasar data de mi Redshift a por ejemplo mi S3, lo cual hacemos de la siguiente forma.\n",
    "\n",
    "```sql\n",
    "unload ('select * from nombre_tabla')\n",
    "to 's3://nombre_bucket_s3/unload/unload_test_'\n",
    "credentials 'aws_iam_role=rol_arn_iam';\n",
    "```\n",
    "A continuación en nuestro S3 podemos ver dentro de la carpeta \"unlaod\" 4 archivos con el prefijo \"unload_test_\" con el mismo tamaño cada uno.\n",
    "\n",
    "Fue dividido en 4 archivos porque tengo 2 nodos con 2 slices por nodo. \n",
    "\n",
    "![](img_166.png)\n",
    "\n",
    "Si lo queremos en un solo archivo (no superior a 6.2 GB), NO recomendado!!! agregamos el parámetro \"parallel off\"\n",
    "\n",
    "```sql\n",
    "unload ('select * from nombre_tabla')\n",
    "to 's3://nombre_bucket_s3/unload/unload_test_'\n",
    "credentials 'aws_iam_role=rol_arn_iam'\n",
    "parallel off;\n",
    "```\n",
    "\n",
    "### Mismo comando con multiples parámetros\n",
    "\n",
    "```sql\n",
    "unload ('select * from unload_test_2')\n",
    "to 's3://mybucketredshiftsantiago/unload/unload_test_4'\n",
    "credentials 'aws_iam_role=arn:aws:iam::118590468211:role/MiRoleRedshift'\n",
    "ALLOWOVERWRITE\n",
    "delimiter ';'\n",
    "header\n",
    "maxfilesize 500 mb\n",
    "ZSTD --tipo de compresión\n",
    "manifest\n",
    "partition by (c_nation) INCLUDE \n",
    ";\n",
    "```\n",
    "### Lecturas recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_UNLOAD.html#unload-usage-notes\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_UNLOAD.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776c374",
   "metadata": {},
   "source": [
    "### <a name=\"mark_31\"></a>Otras tablas útiles de Redshift para entender el comportamiento de nuestros datos\n",
    "### [Index](#index_03)\n",
    "\n",
    "### <a name=\"mark_31.0\"></a> SVL_USER_INFO\n",
    "\n",
    "Puede recuperar datos sobre los usuarios de la base de datos de Amazon Redshift con la vista SVL_USER_INFO.\n",
    "\n",
    "#### Columnas de la tabla\n",
    "\n",
    "|Nombre de la columna\t|Tipo de datos\t|Descripción|\n",
    "|:---|:---:|:---|\n",
    "|usename\t|texto\t|Nombre de usuario de la función.|\n",
    "|usesysid\t|entero\t|ID del usuario.|\n",
    "|usecreatedb\t|booleano\t|Valor que indica si el usuario tiene permisos para crear bases de datos.|\n",
    "|usesuper\t|booleano\t|Valor que indica si el usuario es un superusuario.|\n",
    "|usecatupd\t|booleano\t|Valor que indica si el usuario puede actualizar los catálogos de sistema.|\n",
    "|useconnlimit\t|texto\t|Número de conexiones que el usuario puede abrir.|\n",
    "|syslogaccess\t|texto\t|Valor que indica si el usuario tiene acceso a los registros del sistema. Los dos valores posibles son RESTRICTED y UNRESTRICTED. RESTRICTED significa que los usuarios que no son superusuarios pueden ver sus propios registros. UNRESTRICTED significa que los usuarios que no son superusuarios pueden ver todos los registros de las vistas y tablas del sistema en las que tienen privilegios SELECT.|\n",
    "|last_ddl_ts|\ttimestamp\t|Marca temporal de la instrucción de creación del lenguaje de definición de datos (DDL) ejecutada por el usuario.|\n",
    "|sessiontimeout\t|entero\t|El tiempo máximo en segundos durante el cual una sesión permanece inactiva o en reposo antes de que se cierre. El 0 indica que no se establece tiempo de espera. Para obtener información acerca de la configuración de tiempo de espera en reposo o inactivo del clúster, consulte Cuotas y límites de Amazon Redshift en la Guía de administración de Amazon Redshift.|\n",
    "|external_id\t|texto\t|Identificador único del usuario en el proveedor de identidades de terceros.|\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/cm_chap_system-tables.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/redshift/latest/dg/r_SVL_USER_INFO.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
